<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kunpeng Compute Team Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kunpengcompute.github.io/"/>
  <updated>2020-09-11T01:53:43.713Z</updated>
  <id>https://kunpengcompute.github.io/</id>
  
  <author>
    <name>鲲鹏计算开源生态团队</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PostgreSQL对外部压缩方法的诉求</title>
    <link href="https://kunpengcompute.github.io/2020/08/31/postgresql-dui-wai-bu-ya-suo-fang-fa-de-su-qiu/"/>
    <id>https://kunpengcompute.github.io/2020/08/31/postgresql-dui-wai-bu-ya-suo-fang-fa-de-su-qiu/</id>
    <published>2020-08-31T01:20:24.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html">https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html</a></p><p>Amit PSQL专家分析压缩库在PostgreSQL中的迫切诉求。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>现今的每个数据库系统都有一定程度的数据压缩方法。很明显，这是为了减少数据库的大小，特别是在当今数据呈指数增长的时代。另外的原因是为了提高查询性能; 其思想是: 更小的数据大小意味着需要扫描的数据页面更少，这意味着更少的磁盘 i/o 和更快的数据访问。因此，无论如何，数据解压缩的速度应该足够快，以免影响查询性能(如果不能提高的话)。</p><p>压缩提供了不同的层次: 页面压缩、行压缩、列压缩等。柱型数据库的优点是它的列压缩比很高，因为在一个列中存在连续数据的重复区域。另一种情况是，在面向行的数据库中，列值非常大，因此有必要压缩列的单个值。如果这些值不适合放在单个页面中，甚至可以单独保留它们，并且该行具有指向行外压缩数据的指针。在 PostgreSQL 中，这种技术被称为 TOAST (超大型属性存储技术) ，对于可以包含可变长度数据的列，数据被明显地压缩并存储在同一行中，或者如果数据仍然太大，则将数据以较小的块形式单独的存储在称为 TOAST table表的行中，这些块本身可能被压缩，也可能不被压缩。</p><p>压缩为不同的数据操作提供了可能性。它可能不会被限制只有几秒数据压缩。例如，在容灾系统中，把redo logs从主服务器到从服务器的传输可能成为一个巨大的网络瓶颈，因此许多 RDBMS 提供压缩redo logs的功能。</p><p>然后是 RDBMS 使用或提供选项可选的压缩算法。这一点尤其适用于数据压缩。由于数据是用户的数据，用户数据中的特定格式可能适合特定的压缩算法，而不同的存储格式可能适合另一种压缩算法。此外，这意味着，如果 RDBMS 提供一个选项，为特定列选择特定的压缩算法，或者从众所周知的标准压缩库列表(如 zlib、 lz4、 zstd、 snappy、 gzip 等)中选择特定的用户定义类型，那么这种方法将更加有益。或者，库算法都可以是完全定制的。</p><p>并且提供了与 CPU 内核紧密耦合的压缩、加密和 SIMD 硬件加速器，这些硬件加速器可以通过压缩或加密算法加以利用。其中一个例子是<a href="https://github.com/kunpengcompute/KAEzip">Kunpeng Zlib Acceleration Engine</a>, 它提供了一个支持硬件的基础设施，用于在“ Kunpeng 920” ARM64处理器上进行压缩。我还没有机会测试这种能力，但它听起来很有希望。</p><p>此外，压缩/加密算法在数据上执行重复的任务，这是利用 SIMD 向量化的自然选择。已经有一些独立的项目在 ARM64和 Intel 上进行，以便在 zlib、 lz4等著名的压缩库中进行这种特定于平台的增强。参看<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Intrinsics case study</a> 关于优化 zlib 的 adler-32算法的 NEON intrinsic 案例研究。</p><p>所有这些都直接表明，RDBMS 服务器迫切需要为用户提供针对特定表或特定列的本地压缩算法/库的选择。在写这篇文章的时候，PostgreSQL 使用基于 LZ<a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">它自己的内建压缩算法</a> 来压缩Toast 表。想象一下，如果有一个用于选择 zlib 的接口，而不是内置的算法。进一步，选择 zlib 压缩级别。更进一步，为用户添加一个界面来创建一个扩展，该扩展使用特定平台的自定义算法，该平台使用硬件加速。</p><p>OK，我们正在实现一个这样的特性。在 PostgreSQL 黑客社区中查看这个 <a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">讨论主题</a>。这个特性可能还有很长的路要走(截至本文撰写之时) ，但是我对这个特性充满希望，因为如上所示，用例足够强大，对这个功能没有反对意见，并且提交了work-in-progress的补丁。</p><p>我查看了这个补丁，玩了一下。粗略地说，下面是操作界面的样子。在补丁集完全具体化之后，接口可能会有所不同，但我认为它的本质或多或少会保持不变。下面是我的测试结果; 请注意，这只是为了通过例子强调这个功能是多么的酷和有用，并且使我在这个博客中解释的任何东西都有意义。</p><p>CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level ‘4’));<br>CREATE TABLE lztab(t TEXT);<br>ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </p><p>pgg:s2:pg$ time psql -c “\copy zlibtab from text.data”<br>COPY 13050                                    </p><p>real  0m1.344s<br>user  0m0.031s<br>sys   0m0.026s                                 </p><p>pgg:s2:pg$ time psql -c “\copy lztab from text.data”<br>COPY 13050                                    </p><p>real  0m2.088s<br>user  0m0.008s<br>sys   0m0.050s                                 </p><p>pgg:s2:pg$ time psql -c “select pg_table_size(‘zlibtab’::regclass), pg_table_size(‘lztab’::regclass)”<br> pg_table_size | pg_table_size<br>—————+—————<br>    1261568 |    1687552                          </p><p>pgg:s2:pg$ time psql -c “select NULL from zlibtab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.127s<br>user  0m0.000s<br>sys   0m0.002s</p><p>pgg:s2:pg$ time psql -c “select NULL from lztab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.050s<br>user  0m0.002s<br>sys   0m0.000s</p><p>注意两种不同的压缩算法在压缩大小、插入数据(压缩)和选择数据(解压)的速度上是如何不同的。</p><p>你甚至可以创建一个新的压缩访问函数，就像我们创建一个新的索引一样:</p><p>CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;<br>其中my_compression_handler 应该是一个 PostgreSQL C 函数，可以使用 PostgreSQL 扩展创建。这个函数为一组预定义的钩子分配它自己的实现函数，这些钩子定义了 PostgreSQL 核心使用压缩访问方法所需要知道的一切: </p><p>Datum<br>my_compression_handler(PG_FUNCTION_ARGS)<br>{<br>    CompressionAmRoutine *routine = makeNode(CompressionAmRoutine);</p><p>​    routine-&gt;cmcheck = my_cmcheck;<br>​    routine-&gt;cminitstate = my_cminitstate;<br>​    routine-&gt;cmcompress = my_cmcompress;<br>​    routine-&gt;cmdecompress = my_cmdecompress;<br>​    routine-&gt;cmdecompress_slice = NULL;</p><p>​    PG_RETURN_POINTER(routine);<br>}</p><p>这是 PostgreSQL 高度可扩展的方式: 允许用户使用内置方法，但也为用户提供了一种方法来定义他/她自己的方法来完成相同的工作。上面的所有函数都在一个 PostgreSQL 扩展中，可以使用:<br>CREATE EXTENSION my_compression;</p></div><div id="English" class="tab-content"><p> Every modern database system has some way to compress its data at some level. The obvious reason for this feature is to reduce the size of it’s database, especially in today’s world where the data is growing exponentially. The less obvious reason is to improve query performance; the idea is: smaller data size means less data pages to scan, which means lesser disk i/o and faster data access. So, in any case, data de-compression should be fast enough so as not to hamper the query performance, if not improve it.</p><p>Compression is offered at different levels : page compression, row compression, column compression, etc. Columnar databases have the advantage of a very high compression ratio of its column because of presence of a repetetive pattern of contiguous data in a column. Another case is when, in a row oriented database, the column values are so large that it makes sense to compress individual values of the column. Such values can even be kept separately if they do not fit in a single page. And the row has pointers to the out-of-line compressed data. In PostgreSQL, such technique is called TOAST (The Oversized-Attribute Storage Technique), where, for columns that can contain variable-length data, the data is transparently compressed and stored in the same row, or else if it is still too large, it is stored in smaller chunks as rows in a separate table called a toast table, where these chunks themselves may or may not be compressed.</p><p>Compression is offered for different purposes. It may not be restricted for only data compression. E.g. in a replication system, the transfer of redo logs from the master to slave can become a huge network bottleneck, so many RDBMS offer to compress redo logs.</p><p>And then comes the compression algorithms that the RDBMS uses or gives options to choose. This applies especially more to data compression. Since data is user’s data, a specific pattern in the user data might suit a particular compression algorithm, while a different pattern might be suitable for another compression algorithm. Moreover, this implies that it would be far more beneficial if the RDBMS gives an option to choose a specific compression algorithm for a specific column or a specific user-defined type out of a list of well-known standard compression libraries such as zlib, lz4, ztd, snappy, gzip, etc. Or, the library algorithm may very well be a completely customized one.</p><p>Secondly, there has been a lot of advancements to optimize compression algorithms for specific platforms, and provide hardware accelerators for Compression, Encryption and SIMD that are closely coupled to CPU cores, which can then be levergaed by compression or encryption algorithms. One such example is the <a href="https://github.com/kunpengcompute/KAEzip">Kunpeng Zlib Acceleration Engine</a>, which offers a hardware-enabled infrastructure for compression on a “Kunpeng 920” ARM64 processor. I haven’t got a chance to test this capability, but it does sound promising.</p><p>Furthermore, the compression/encryption algorithms inherently do repetitive tasks over the data, which is a natural fit for leveraging SIMD vectorization. There has been independent projects going on on both ARM64 and Intel to do such platform-specific enhancements in well known libraries like zlib, lz4 etc. Check out this <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Intrinsics case study</a> that optimizes zlib’s adler-32 algorithm using NEON intrinsics.</p><p>All this directly points to an urgent need for RDBMS servers to give users a choice for specific native compression algorithms/libraries for specific tables or specific columns. As of this writing, PostgreSQL uses <a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">its own built-in compression algorithm</a> based on LZ for toast table compression. Imagine if there were an interface to select zlib instead of the built-in algorithm. Further, select the zlib compression level. Still further, add an interface for users to create an extension that uses a customized algorithm native to a specific platform that uses hardware acceleration.</p><p>Well, there is exactly such a proposed feature in the making. Check out this <a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">discussion thread</a> in the PostgreSQL hackers community. It may be a long way to go (as of this writing), but I am very hopeful of this feature going in, because the use-cases are strong enough as shown above, there are no fundamental objections to this functionality, and there are work-in-progress patches submitted.</p><p>I went ahead and applied this patch, and played around it. Roughly, below is how the interface looks like. After the patch-set fully materializes, the interface might be different, but I think the essence of it would remain more or less the same. Below is the output of my tests; please note that it is just to emphasize with examples how cool and useful this feature would be, and to make sense of whatever I explained above in this blog.</p><p>CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level ‘4’));<br>CREATE TABLE lztab(t TEXT);<br>ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </p><p>pgg:s2:pg$ time psql -c “\copy zlibtab from text.data”<br>COPY 13050                                    </p><p>real  0m1.344s<br>user  0m0.031s<br>sys   0m0.026s                                 </p><p>pgg:s2:pg$ time psql -c “\copy lztab from text.data”<br>COPY 13050                                    </p><p>real  0m2.088s<br>user  0m0.008s<br>sys   0m0.050s                                 </p><p>pgg:s2:pg$ time psql -c “select pg_table_size(‘zlibtab’::regclass), pg_table_size(‘lztab’::regclass)”<br> pg_table_size | pg_table_size<br>—————+—————<br>    1261568 |    1687552                          </p><p>pgg:s2:pg$ time psql -c “select NULL from zlibtab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.127s<br>user  0m0.000s<br>sys   0m0.002s</p><p>pgg:s2:pg$ time psql -c “select NULL from lztab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.050s<br>user  0m0.002s<br>sys   0m0.000s</p><p>Notice how two different compression algorithms differ in the compressed size, and the speed of inserting data (compression) and selecting data (decompression).</p><p>You would even be able to create a new compression access method using the same way as we do for creating a new index :<br>CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;<br>where my_compression_handler should be a PostgreSQL C function that could be created using a PostgreSQL extension. This function assigns its own implementation functions for a set of pre-defined hooks that define everything that the PostgreSQL core needs to know to make use of the compression access method :</p><p>Datum<br>my_compression_handler(PG_FUNCTION_ARGS)<br>{<br>    CompressionAmRoutine *routine = makeNode(CompressionAmRoutine);</p><p>​    routine-&gt;cmcheck = my_cmcheck;<br>​    routine-&gt;cminitstate = my_cminitstate;<br>​    routine-&gt;cmcompress = my_cmcompress;<br>​    routine-&gt;cmdecompress = my_cmdecompress;<br>​    routine-&gt;cmdecompress_slice = NULL;</p><p>​    PG_RETURN_POINTER(routine);<br>}</p><p>This is PostgreSQL’s way of being highly extensible : Allow user to use built-in methods, but also provide a way for the user to define his/her own methods for doing the same job. All the above functions would be inside an PostgreSQL extension, that could be created using:<br>CREATE EXTENSION my_compression;</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amit PSQL专家分析压缩库在PostgreSQL中的迫切诉求。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Aarch64服务器应用软件开发需要添加的编译参数</title>
    <link href="https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/"/>
    <id>https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/</id>
    <published>2020-08-29T05:00:55.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>作者： <a href="https://github.com/zhaorenhai">zhaorenhai</a></p><p>   本文简单总结了一下，在aarch64服务器平台进行应用软件开发或者移植工作，编译代码时，编译器应该添加哪些选项。网上类似文章不少，但是由于arm平台涉及了移动开发，嵌入式开发，服务器开发各个领域，编译方式也有交叉编译，本地编译等，而且编译器也有gcc，armcc，armclang，clang等等多种，再加上arm平台历史版本众多，又分了32位，64位，网上这些文档一般都不明确说明文档涉及的开发平台，对应指令集版本，需要的编译器之类的情况，让人看上去比较头疼。 </p><a id="more"></a><p>本文涉及的范围：aarch64 Linux 服务器平台应用软件开发，不涉及交叉编译，只讨论本地编译情况， 只讨论可以免费使用的开源gcc编译器（clang也是可以免费可以使用的开源编译器，但是编译参数全面兼容gcc，所以本文只讨论gcc）。 另外本文也只讨论和aarch64平台强相关的参数，通用的编译参数讨论的文章比较多，本文不再赘述。<br>   最主要的参考资料来自于这篇文档：<a href="https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html。">https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html。</a> 本文主要讨论最主要的也是最容易引起混淆的-march，-mtune，-mcpu参数。其余参数一般都是在一些极特殊的场景下才用到，比如需要改变代码模型，需要改变数据模型，需要指定字节序为大端字节序等等，或者开发特殊的软件比如gcc编译器之类的时候才用到，一般服务器应用软件开发很少用到，所以一般保持默认值即可，后续如果有实际遇到的场景，会再补充到本文中。</p><p>  先看-march参数，原文描述：<br>-march=name<br>Specify the name of the target architecture and, optionally, one or more feature modifiers. This option has the form -march=arch{+[no]feature}*.<br>The table below summarizes the permissible values for arch and the features that they enable by default:</p><table><thead><tr><th>arch value</th><th>Architecture</th><th>Includes by default</th></tr></thead><tbody><tr><td>‘armv8-a’</td><td>Armv8-A</td><td>‘+fp’, ‘+simd’</td></tr><tr><td>‘armv8.1-a’</td><td>Armv8.1-A</td><td>‘armv8-a’, ‘+crc’, ‘+lse’, ‘+rdma’</td></tr><tr><td>‘armv8.2-a’</td><td>Armv8.2-A</td><td>‘armv8.1-a’</td></tr><tr><td>‘armv8.3-a’</td><td>Armv8.3-A</td><td>‘armv8.2-a’</td></tr><tr><td>‘armv8.4-a’</td><td>Armv8.4-A</td><td>‘armv8.3-a’, ‘+fp16fml’, ‘+dotprod’</td></tr><tr><td>‘armv8.5-a’</td><td>Armv8.5-A</td><td>‘armv8.4-a’, ‘+sb’, ‘+ssbs’, ‘+predres’</td></tr><tr><td>‘armv8.6-a’</td><td>Armv8.6-A</td><td>‘armv8.5-a’, ‘+bf16’, ‘+i8mm’</td></tr></tbody></table><p>The value ‘native’ is available on native AArch64 GNU/Linux and causes the compiler to pick the architecture of the host system. This option has no effect if the compiler is unable to recognize the architecture of the host system,<br>The permissible values for feature are listed in the sub-section on -march and -mcpu Feature Modifiers. Where conflicting feature modifiers are specified, the right-most feature is used.<br>GCC uses name to determine what kind of instructions it can emit when generating assembly code. If -march is specified without either of -mtune or -mcpu also being specified, the code is tuned to perform well across a range of target processors implementing the target architecture.</p><p>这个参数可以指定编译的目标架构，这里的架构指得是ARM CPU的指令集架构，那我们应该指定哪个版本的指令集架构，才能做到最好的兼容性，能兼容市场上所有的ARM CPU，或者既能兼顾兼容性，又能兼顾一部分性能，毕竟新版本的指令集架构能用上更新的具有特定功能的指令集。 这个需要看一下目前市场上所有的ARM服务器CPU的情况，网上目前已有的相关文档要么比较老，信息不全，要么就是出现把移动CPU和服务器CPU都放到一起的情况，比较混乱。所以本文重新搜集了目前所有ARM服务器CPU的信息，如下表（资料来源于互联网）：</p><table><thead><tr><th>厂商</th><th>型号</th><th>技术指标</th><th>微架构</th><th>指令集架构</th><th>发布时间</th></tr></thead><tbody><tr><td>Ampere</td><td>X-gene-1</td><td>8c   2.4GHz   40nm</td><td>Storm</td><td>ARMv8.0-A</td><td>2012</td></tr><tr><td>Ampere</td><td>X-gene-2</td><td>8c   2.4GHz   28nm</td><td>Shadowcat</td><td>ARMv8.0-A</td><td>2015</td></tr><tr><td>Ampere</td><td>eMag8180   (X-gene-3)</td><td>32c   2.8GHz-3.3GHz   16nm</td><td>Skylark</td><td>ARMv8.0-A</td><td>2018</td></tr><tr><td>Ampere</td><td>Altra</td><td>32-80c   1.7-3.3GHz</td><td>Quicksilver   (Neoverse N1)</td><td>ARMv8.2-A</td><td>2020年3月</td></tr><tr><td>Ampere</td><td>Altra Max</td><td>128c   7nm</td><td>Quicksilver   (Neoverse N1)</td><td>ARMv8.2-A</td><td>2020年6月</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX   (CN8890)</td><td>48c   1.9GHz   28nm</td><td>ThunderX1</td><td>ARMv8.1-A</td><td>2016</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX2   (CN99xx系列)</td><td>32-54c   1.6-2.5GHz   14nm 16nm</td><td>Vulcan   (从Broadcom收购)</td><td>ARMv8.1-A</td><td>2018年5月</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX3</td><td>96c   3.1GHz   7nm</td><td>Triton</td><td>ARMv8.3-A</td><td>2020</td></tr><tr><td>Qualcomm</td><td>Centriq 2400系列</td><td>40-48c   2.2-2.6GHz   10nm</td><td>Falkor</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>FT1500</td><td>16c   1.6GHz   28nm</td><td>Earth</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>FT2000+</td><td>64c   2.3GHz   16nm</td><td>Mars</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>S2500</td><td>64c   2.0-2.2GHz   16nm</td><td></td><td>ARMv8.0-A</td><td>2020</td></tr><tr><td>HiSilicon</td><td>Hi1616</td><td>32c   2.4GHz   16nm</td><td>Cortex-A72</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>HiSilicon</td><td>Kunpeng 920   (Hi1620)</td><td>48c   3.0GHz   7nm</td><td>Taishan V110</td><td>ARMv8.2-A</td><td>2019</td></tr><tr><td>Annapurna Labs   （被AWS收购）</td><td>Gravtion</td><td>16c   2.3GHz   16nm</td><td>Cortex-A72</td><td>ARMv8.0-A</td><td>2018年11月</td></tr><tr><td>Annapurna Labs   （被AWS收购）</td><td>Gravtion2</td><td>64c   2.5GHz   7nm</td><td>Neoverse N1</td><td>ARMv8.2-A</td><td>2019年12月</td></tr><tr><td>Nuvia</td><td>Phoenix</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>备注1：此表仅搜集了64位的arm cpu相关信息，32位的arm服务器cpu比较老，实际应用也不多，可以不用再考虑兼容工作。<br>备注2: 此表仅包含通用arm服务器cpu相关信息，不包含移动平台的cpu以及超算cpu，更不包含嵌入式以及实时计算cpu。</p><p>可以看出来市场上还是有不少ARMv8.0-A指令集架构的CPU，所以如果要做到最佳兼容性，-march参数的name的值就要指定为armv8-a。但是也要看到，高通实际上已经退出服务器CPU市场，Cavium，Ampere，Phytium各有一部分市场，但都不大，Nuvia是新晋厂商，还没有产品问世，市场规模最大的应该主要是海思的Kunpeng920和亚马逊的Gravition2，依托于华为云和亚马逊云，应用较广泛。而且从上图中可以看出各厂商最近两年新出的CPU，基本都已经支持了ARMv8.2-A指令集，考虑到近两年ARM服务器应用刚刚开始大规模普及，特别是华为云和亚马逊云上刚刚开始大规模上线ARM的云服务器实例，所以一些新开发的应用软件或者新迁移的应用软件，甚至可以指定为armv8.2-a，这样可以做到兼容性和性能兼顾。<br>另外-march参数还支持指定一些扩展选项，下面我们来逐个看下这些扩展选项：</p><table><thead><tr><th>扩展选项</th><th>描述</th></tr></thead><tbody><tr><td>crc</td><td>从ARMv8.1-a之后就默认包含了此扩展选项，所以当-march参数的name值为armv8-a的时候，才需要添加此选项，如果代码中涉及crc相关功能和指令，建议加上此选项</td></tr><tr><td>crypto</td><td>加密选项，这个是个额外扩展，没有哪个版本的指令集默认包含此选项，所以如果代码中涉及加密相关功能和指令，都要加上此选项。一般的应用开发涉及这个的比较少，所以这个看实际应用情况来决定要不要加</td></tr><tr><td>fp</td><td>浮点指令，从ARMv8-a就开始默认支持，可以忽略</td></tr><tr><td>simd</td><td>高级单指令多数据流指令，从ARMv8-a就开始默认支持，可以忽略</td></tr><tr><td>sve</td><td>目前已出的cpu支持此功能的较少，当前还不能加</td></tr><tr><td>lse</td><td>最新的原子指令集需要此选项，从ARMv8.1-a之后就默认包含，当-march参数的name值为armv8-a的时候，而且确定CPU支持此扩展选项的情况下可以添加。</td></tr><tr><td>rdma</td><td>乘积累加指令，在一些特殊场景会用到，使用较少。不过从ARMv8.1-a之后就默认包含。</td></tr><tr><td>fp16</td><td>半精度浮点指令，在一些图形软件中会用到，用途较少，建议视使用情况添加。</td></tr></tbody></table><p>还有一些其他的扩展选项，不过都是在更新版本的指令集中才支持，当前已经问世的CPU都不支持，这里就不再讨论。</p><p>再来看-mtune参数，原文描述：<br>-mtune=name<br>Specify the name of the target processor for which GCC should tune the performance of the code. Permissible values for this option are: ‘generic’, ‘cortex-a35’, ‘cortex-a53’, ‘cortex-a55’, ‘cortex-a57’, ‘cortex-a72’, ‘cortex-a73’, ‘cortex-a75’, ‘cortex-a76’, ‘cortex-a76ae’, ‘cortex-a77’, ‘cortex-a65’, ‘cortex-a65ae’, ‘cortex-a34’, ‘ares’, ‘exynos-m1’, ‘emag’, ‘falkor’, ‘neoverse-e1’,‘neoverse-n1’,‘qdf24xx’, ‘saphira’, ‘phecda’, ‘xgene1’, ‘vulcan’, ‘octeontx’, ‘octeontx81’, ‘octeontx83’, ‘octeontx2’, ‘octeontx2t98’, ‘octeontx2t96’ ‘octeontx2t93’, ‘octeontx2f95’, ‘octeontx2f95n’, ‘octeontx2f95mm’, ‘a64fx’, ‘thunderx’, ‘thunderxt88’, ‘thunderxt88p1’, ‘thunderxt81’, ‘tsv110’, ‘thunderxt83’, ‘thunderx2t99’, ‘thunderx3t110’, ‘zeus’, ‘cortex-a57.cortex-a53’, ‘cortex-a72.cortex-a53’, ‘cortex-a73.cortex-a35’, ‘cortex-a73.cortex-a53’, ‘cortex-a75.cortex-a55’, ‘cortex-a76.cortex-a55’ ‘native’.<br>The values ‘cortex-a57.cortex-a53’, ‘cortex-a72.cortex-a53’, ‘cortex-a73.cortex-a35’, ‘cortex-a73.cortex-a53’, ‘cortex-a75.cortex-a55’, ‘cortex-a76.cortex-a55’ specify that GCC should tune for a big.LITTLE system.<br>Additionally on native AArch64 GNU/Linux systems the value ‘native’ tunes performance to the host system. This option has no effect if the compiler is unable to recognize the processor of the host system.<br>Where none of -mtune=, -mcpu= or -march= are specified, the code is tuned to perform well across a range of target processors.<br>This option cannot be suffixed by feature modifiers.<br>这个参数比较简单，实际上就是指定目标CPU微架构，让gcc编译的时候，根据目标cpu的微架构进行特定的优化，比如指定为tsv110时，gcc就会根据鲲鹏920的微架构，进行一些指令的流水线重排，会提高一些性能。但是指定这个，同时也就意味着牺牲了兼容性，编译后的软件只能在目标CPU平台上运行。</p><p>最后看一下-mcpu参数，原文描述：<br>-mcpu=name<br>Specify the name of the target processor, optionally suffixed by one or more feature modifiers. This option has the form -mcpu=cpu{+[no]feature}*, where the permissible values for cpu are the same as those available for -mtune. The permissible values for feature are documented in the sub-section on -march and -mcpu Feature Modifiers. Where conflicting feature modifiers are specified, the right-most feature is used.<br>GCC uses name to determine what kind of instructions it can emit when generating assembly code (as if by -march) and to determine the target processor for which to tune for performance (as if by -mtune). Where this option is used in conjunction with -march or -mtune, those options take precedence over the appropriate part of this option.<br>可以看出来，这个参数实际上就是前面两个参数的综合，而且指定两个参数的优先级会高于这个参数。据说后续这个参数会被干掉。建议可以忽略此参数，只用前面两个参数即可。</p><p>总结：在aarch64服务器平台，进行应用软件开发，编译时，除了通用的编译选项之外，一般只需要添加-march=armv8-a即可，考虑到crc扩展选项应用比较多，可以加上crc扩展选项，比如-march=armv8-a+crc，这样可以做到最好的兼容性。 如果确定平台支持最新的原子指令，可以再加上+lse，其余的扩展选项建议根据实际应用情况添加。 如果既想兼顾兼容性和性能，建议直接指定-march=armv8.2-a，这样不仅包含了crc和lse扩展选项，而且最两年新出的CPU都可以做到兼容。 如果想做到最佳性能，还可以添加-mtune参数，但是这个要确保编译出来的软件只在目标平台运行。<br>另外，由于编译器的习惯， ARM平台上char的默认类型为unsigned char，这与x86正好相反（x86的char默认是有符号的），在x86上运行稳定的代码，移植过来将遇到char类型的变化，会带来不少问题。由于x86上的char类型与我们的编程习惯更一致，所以我们一般将ARM平台上的程序的char类型指定为signed char。所以还需要添加编译选项-fsigned-char。</p><p>最后，上述编译选项和扩展是在不同的gcc版本里逐步得到支持的，所以添加这些选项还要注意gcc的版本，对于-march的 name和扩展选项的支持情况，可以参考ARM官方的这个文档：<a href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support">https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support</a>     对于-mtune选项的支持建议参考不同版本的gcc文档，主要看AArch64-Options这个章节就行。</p><p>参考资料：<br><a href="https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html">https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html</a><br><a href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support">https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support</a><br><a href="https://clang.llvm.org/docs/ClangCommandLineReference.html">https://clang.llvm.org/docs/ClangCommandLineReference.html</a><br><a href="http://www.semiinsights.com/s/electronic_components/23/39697.shtml">http://www.semiinsights.com/s/electronic_components/23/39697.shtml</a><br><a href="http://news.eeworld.com.cn/qrs/2019/ic-news011452649.html">http://news.eeworld.com.cn/qrs/2019/ic-news011452649.html</a><br><a href="http://news.eeworld.com.cn/xfdz/ic497298.html">http://news.eeworld.com.cn/xfdz/ic497298.html</a><br><a href="https://www.cirmall.com/articles/31558">https://www.cirmall.com/articles/31558</a><br><a href="https://koolshare.cn/thread-147215-1-3.html">https://koolshare.cn/thread-147215-1-3.html</a><br><a href="http://www.360doc.com/content/20/0101/18/99071_883541215.shtml">http://www.360doc.com/content/20/0101/18/99071_883541215.shtml</a><br><a href="https://en.wikichip.org/wiki/">https://en.wikichip.org/wiki/</a><br><a href="https://blog.csdn.net/u014470361/article/details/85988772">https://blog.csdn.net/u014470361/article/details/85988772</a><br><a href="https://www.cnblogs.com/panda-w/p/11003389.html">https://www.cnblogs.com/panda-w/p/11003389.html</a><br><a href="https://developer.arm.com/ip-products/processors/cortex-a/cortex-a78">https://developer.arm.com/ip-products/processors/cortex-a/cortex-a78</a><br><a href="https://en.wikipedia.org/wiki/Comparison_of_ARMv8-A_cores">https://en.wikipedia.org/wiki/Comparison_of_ARMv8-A_cores</a><br><a href="https://aijishu.com/a/1060000000133361">https://aijishu.com/a/1060000000133361</a><br><a href="http://phytium.com.cn/article/5">http://phytium.com.cn/article/5</a><br><a href="https://www.sohu.com/a/361552782_163726?scm=1002.44003c.fe021c.PC_ARTICLE_REC">https://www.sohu.com/a/361552782_163726?scm=1002.44003c.fe021c.PC_ARTICLE_REC</a><br><a href="https://www.infoq.cn/article/34moxVRHI8qprbh9I1mp">https://www.infoq.cn/article/34moxVRHI8qprbh9I1mp</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者： &lt;a href=&quot;https://github.com/zhaorenhai&quot;&gt;zhaorenhai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;   本文简单总结了一下，在aarch64服务器平台进行应用软件开发或者移植工作，编译代码时，编译器应该添加哪些选项。网上类似文章不少，但是由于arm平台涉及了移动开发，嵌入式开发，服务器开发各个领域，编译方式也有交叉编译，本地编译等，而且编译器也有gcc，armcc，armclang，clang等等多种，再加上arm平台历史版本众多，又分了32位，64位，网上这些文档一般都不明确说明文档涉及的开发平台，对应指令集版本，需要的编译器之类的情况，让人看上去比较头疼。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>ARM使能Landscape</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/arm-shi-neng-landscape/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/arm-shi-neng-landscape/</id>
    <published>2020-08-04T11:32:25.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>ARM使能Landscape</p><table><thead><tr><th>Field</th><th>Project</th><th>Status</th><th>Links</th></tr></thead><tbody><tr><td>Big Data</td><td>Hadoop</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/">https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/</a></td></tr><tr><td>Big Data</td><td>Hadoop</td><td>软件包发布</td><td><a href="https://hadoop.apache.org/docs/r3.3.0/index.html">https://hadoop.apache.org/docs/r3.3.0/index.html</a></td></tr><tr><td>Big Data</td><td>Spark</td><td>ARM CI</td><td><a href="https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/">https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/</a></td></tr><tr><td>Big Data</td><td>Hive</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/">https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/</a></td></tr><tr><td>Big Data</td><td>HBase</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/">https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/</a></td></tr><tr><td>Big Data</td><td>Flink</td><td>ARM CI</td><td><a href="https://status.openlabtesting.org/builds?project=apache%2Fflink">https://status.openlabtesting.org/builds?project=apache%2Fflink</a></td></tr><tr><td>Big Data</td><td>Kudu</td><td>ARM CI</td><td><a href="http://status.openlabtesting.org/builds?project=apache%2Fkudu">http://status.openlabtesting.org/builds?project=apache%2Fkudu</a></td></tr><tr><td>Database</td><td>MariaDB</td><td>ARM CI</td><td><a href="https://buildbot.mariadb.org/#/builders">https://buildbot.mariadb.org/#/builders</a></td></tr><tr><td>Database</td><td>Greenplum</td><td>ARM CI</td><td><a href="https://github.com/greenplum-db/gpdb">https://github.com/greenplum-db/gpdb</a></td></tr><tr><td>AI</td><td>TensorFlow</td><td>ARM CI</td><td><a href="https://github.com/tensorflow/tensorflow#community-supported-builds">https://github.com/tensorflow/tensorflow#community-supported-builds</a></td></tr><tr><td>AI</td><td>Pytorch</td><td>ARM CI</td><td><a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></td></tr></tbody></table><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ARM使能Landscape&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Project&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Links&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hadoop&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/&quot;&gt;https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hadoop&lt;/td&gt;
&lt;td&gt;软件包发布&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://hadoop.apache.org/docs/r3.3.0/index.html&quot;&gt;https://hadoop.apache.org/docs/r3.3.0/index.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hive&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/&quot;&gt;https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;HBase&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/&quot;&gt;https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Flink&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://status.openlabtesting.org/builds?project=apache%2Fflink&quot;&gt;https://status.openlabtesting.org/builds?project=apache%2Fflink&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Kudu&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://status.openlabtesting.org/builds?project=apache%2Fkudu&quot;&gt;http://status.openlabtesting.org/builds?project=apache%2Fkudu&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Database&lt;/td&gt;
&lt;td&gt;MariaDB&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://buildbot.mariadb.org/#/builders&quot;&gt;https://buildbot.mariadb.org/#/builders&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Database&lt;/td&gt;
&lt;td&gt;Greenplum&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/greenplum-db/gpdb&quot;&gt;https://github.com/greenplum-db/gpdb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI&lt;/td&gt;
&lt;td&gt;TensorFlow&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow#community-supported-builds&quot;&gt;https://github.com/tensorflow/tensorflow#community-supported-builds&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI&lt;/td&gt;
&lt;td&gt;Pytorch&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;https://github.com/pytorch/pytorch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>ARM&#39;s LSE (for atomics) and MySQL</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/arm-s-lse-for-atomics-and-mysql/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/arm-s-lse-for-atomics-and-mysql/</id>
    <published>2020-08-04T06:10:00.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/ARM-LSE-and-MySQL/">https://mysqlonarm.github.io/ARM-LSE-and-MySQL/</a></p><p>来看Mysql大牛Krunal带你分析LSE在Mysql上的情况。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h1 id="ARM’s-LSE-for-atomics-and-MySQL"><a href="#ARM’s-LSE-for-atomics-and-MySQL" class="headerlink" title="ARM’s LSE (for atomics) and MySQL"></a>ARM’s LSE (for atomics) and MySQL</h1><p>ARM 在其 ARMv8.1规范中引入了 LSE (Large System Extensions)。这意味着如果你的处理器是兼容 ARMv8.1的，它将支持 LSE。LSE 的目的是优化原子指令，通过使用单个 CAS (比较并交换)或 SWP (用于交换)等替换旧式的独占负载存储… … 众所周知，上述扩展本质上会提高使用原子的应用程序的性能。</p><h2 id="理解LSE"><a href="#理解LSE" class="headerlink" title="理解LSE"></a>理解LSE</h2><p>为了更好地理解 LSE，让我们看一个工作示例，看看代码是如何生成的，以及可能的优化。</p><h3 id="LSE-turned-off"><a href="#LSE-turned-off" class="headerlink" title="LSE turned off"></a>LSE turned off</h3><p><img src="https://mysqlonarm.github.io/images/blog10/img1.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/img2.png" alt="img"></p><p>正如您所看到的，有一个用于执行 CAS 的循环。加载值，检查期望值，如果不同，然后存储值。主循环是一个5步进程与2个独占指令与各自的内存顺序。SWAP 也有一个检查存储是否成功的循环。</p><hr><p>ARM 有多种不同的加载/存储指令，因此在继续之前，让我们花一分钟理解这些变体。</p><p><img src="https://mysqlonarm.github.io/images/blog10/img3.png" alt="img"></p><p><strong>stlxrb:</strong> 与发布语义/排序互斥的存储。提供对上述cacheline的独占访问权。“b”表示字节。（其他 half-word (2), word(4), double-word (8))。</p><p><strong>stlrb:</strong> 与发布语义/排序互斥的存储。帮助建议只进行语义排序，而不进行独占访问。</p><p><strong>strb:</strong> 非原子变量的普通存储(没有排序语义)</p><p>自然有人会问，为什么 <code>seq-cst</code> 和 <code>release memory order</code> 都会产生相同的asm指令。这是因为 ARM-v8中的<strong>store-release in ARM-v8 is multi-copy atomic</strong>，也就是说，如果一个agent看到了存储-释放，那么所有agent 都看到了存储-释放。没有要求普通存储为多拷贝原子存储。[类似于 x86_64中的 mov+fence 或 xchg ]。</p><hr><h3 id="LSE-开启"><a href="#LSE-开启" class="headerlink" title="LSE 开启"></a>LSE 开启</h3><p>所以现在让我们看看如果我们现在打开 lse 会发生什么。LSE 支持是在 ARM-v8.1规范中添加的，因此如果默认编译已经完成，gcc 将尝试使二进制文件与更广泛的 aarch64处理器兼容，并且可能无法启用特定的功能。为了支持 lse 用户需要指定额外的编译标志:</p><p>有多种方式打开lse:</p><ul><li>使用 gcc-6+ 编译，指定 lse 标志为-march = armv8-a+lse</li><li>通过指定 ARMv8.1(或更高版本)使用 gcc-6+ 编译(这将自动启用所有 ARMv8.1功能)。-march = ARMv8.1-a</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/img4.png" alt="img"></p><p>不再有 while 循环。单条指令(CASALB)执行比较和交换(负责加载和存储) ，与 SWAPLB 执行交换的方法相同。看起来是进行了优化。更多关于性能的信息请见下文。</p><p>但是有一个问题，如果二进制文件是用 +lse 支持编译的，但是目标计算机不支持 lse，因为它只与 arm-v8兼容。通过引入 <code>-moutline-atomics</code> ，gcc-9.4+ 解决了这个问题(使用<code>-mno-outline-atomics</code>禁用 gcc-10.1启用的默认值)。GCC 自动匹配带有动态检查变量(lse 和 non-lse)的代码。运行时作出决定，并执行相应的变量。</p><p>让我们看看如果使用 gcc-10(使用 <code>-moutline-atomic</code>使其在所有 aarch64机器上兼容)编译会发生什么)</p><table><thead><tr><th align="left">code</th><th align="left">asm (perf output)</th></tr></thead><tbody><tr><td align="left">bool expected = true; flag.compare_exchange_strong(expected, false);</td><td align="left">&lt;<strong>aarch64_cas1_acq_rel&gt;: __aarch64_cas1_acq_rel(): │ adrp x16, 11000 &lt;</strong>data_start&gt; │ ldrb w16, [x16, #25] │ ↓ cbz w16, 14 │ casalb w0, w1, [x2] │ ← ret │14: uxtb w16, w0 │ 18: ldaxrb w0, [x2] │ cmp w0, w16 │ ↓ b.ne 2c │ stlxrb w17, w1, [x2] │ ↑ cbnz w17, 18 │2c: ← ret</td></tr></tbody></table><p>请注意用于选择适当逻辑的分支指令(用绿色突出显示)。</p><h2 id="LSE-的表现"><a href="#LSE-的表现" class="headerlink" title="LSE 的表现"></a>LSE 的表现</h2><p>虽然这些听起来很有趣，但是真的有帮助吗？如果新的指令需要更多的周期怎么办。只有一种方法可以找到答案: 基准测试。</p><p>基准测试: 每个线程(总共 n 个线程)都尝试获得锁，这会导致严重的争用。一旦线程拥有了互斥锁，它就会执行基于 crc32的软件，在释放它之前让 cpu 一直处于繁忙状态。每个线程都执行这个流程 m 次。</p><p><img src="https://mysqlonarm.github.io/images/blog10/lse-microbenchmark.png" alt="img"></p><p>Machine: Bare-Metal with 128 cores ARM Kunpeng 920 2.6 Ghz.</p><ul><li>用例表示一个严重的争用，每个线程主要花费时间获得锁。这样的工作负载非常快(crc32在16KB 块上)</li><li>这清楚地证明，LSE严重争用的条件下起到帮助作用。</li></ul><p>但是微基准测试由于应用本身的特性，包括其他重叠部分，如 IO、其他处理元素等，有时不能显示原始应用所需的增益。.现在让我们评估一下lse使能在 MySQL 性能。</p><h3 id="MySQL-benchmarking-with-LSE"><a href="#MySQL-benchmarking-with-LSE" class="headerlink" title="MySQL benchmarking with LSE"></a>MySQL benchmarking with LSE</h3><p>环境描述:</p><ul><li>Server: MySQL-8.0.21, OS: CentOS-7</li><li>Sysbench based point-select, read-only, read-write, update-index and update-non-index workload.</li><li>Executed for higher scalability (&gt;= 64) to explore contention.</li><li>Configuration: 32 cores (single NUMA) ARM Kunpeng 920 2.6 Ghz (28 cores for server, 4 for sysbench)</li><li>Tried 2 use-cases uniform, zipfian (more contention)</li><li>baseline=lse-disabled, lse=lse-enabled (-march=armv8-a+lse).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/sysbench-uniform-lseenabled.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/sysbench-zipfian-lseenabled.png" alt="img"></p><h3 id="观察结果"><a href="#观察结果" class="headerlink" title="观察结果:"></a>观察结果:</h3><ul><li>在Uniform cases ，使用LSE下，我们几乎看不到任何区别</li><li>在Zipfian cases, LSE对于更新用例，会略微退化，但是始终如一(2-4%).</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LSE 作为特性看起来很有前途，但在 MySQL 用例中确实没有多少提升。可能一旦 MySQL 被调整到使用更多的原子，LSE 可能会显示 +ve 的差异。在那之前，如果不启用 LSE，我们在Mysql不会失去任何性能。</p><p><em>如果有疑问，请联系我，或在下方留言</em></p></div><div id="English" class="tab-content"><p>ARM introduced LSE (Large System Extensions) as part of its ARMv8.1 specs. This means if your processor is ARMv8.1 compatible it would support LSE. LSE are meant to optimize atomic instructions by replacing the old styled exclusive load-store using a single CAS (compare-and-swap) or SWP (for exchange), etc…. Said extensions are known to inherently increase performance of applications using atomics.</p><h2 id="Understanding-LSE"><a href="#Understanding-LSE" class="headerlink" title="Understanding LSE"></a>Understanding LSE</h2><p>To better understand LSE let’s take a working example to see how the code is generated and possible optimization.</p><h3 id="LSE-turned-off-1"><a href="#LSE-turned-off-1" class="headerlink" title="LSE turned off"></a>LSE turned off</h3><p><img src="https://mysqlonarm.github.io/images/blog10/img1.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/img2.png" alt="img"></p><p>As you can see there is a loop for doing CAS. Load the value, check with expected value and if different then store the value. Main loop is a 5 step process with 2 exclusive instructions with respective memory ordering. SWAP too has a loop for checking if the store is successful.</p><hr><p>ARM has multiple variant of load/store instructions so before we proceed let’s take a minute to understand these variants.</p><p><img src="https://mysqlonarm.github.io/images/blog10/img3.png" alt="img"></p><p><strong>stlxrb:</strong> store exclusive with release semantics/ordering. Provide exclusive access to the said cache line. “b” represents byte. (other variant half-word (2), word(4), double-word (8)).</p><p><strong>stlrb:</strong> store with release semantics/ordering helping suggest ordering semantics only but not exclusive access.</p><p><strong>strb:</strong> normal store to a non-atomic variable (no ordering semantics)</p><p>Naturally one may ask how come both <code>seq-cst</code> and <code>release memory order</code> generate the same asm instruction. This is because <strong>store-release in ARM-v8 is multi-copy atomic</strong>, that is, if one agent has seen a store-release, then all agents have seen the store-release. There are no requirements for ordinary stores to be multi-copy atomic. [Something similar to mov+fence or xchg in x86_64 domain].</p><hr><h3 id="LSE-turned-on"><a href="#LSE-turned-on" class="headerlink" title="LSE turned on"></a>LSE turned on</h3><p>So let’s now see what would happen if we now turn-lse on. LSE support was added with ARM-v8.1 specs and so if the default compilation is done, gcc will try to make binary compatible with a wider aarch64 processors and may not enable the specific functionality. In order to enable lse user need to specify extra compilation flags:</p><p>There are multiple ways to turn-on lse:</p><ul><li>Compile with gcc-6+ by specifying lse flag as -march=armv8-a+lse</li><li>Compile with gcc-6+ by specifying ARMv8.1 (or higher) (that will auto-enable all ARMv8.1 functionalities). -march=armv8.1-a</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/img4.png" alt="img"></p><p>No more while loop. Single instruction (CASALB) to do the compare and swap (that takes care of load and store) and same way SWAPLB to do the exchange. Sounds optimized. More about performance below.</p><p>But there is one problem, what if binaries are compiled with +lse support but the target machine doesn’t support lse as it is only arm-v8 compatible. This problem is solved with gcc-9.4+ by introducing <code>-moutline-atomics</code> (default enabled with gcc-10.1 can be disabled with <code>-mno-outline-atomics</code>). GCC auto emits a code with dynamic check with both variants (lse and non-lse). Runtime a decision is taken and accordingly said variant is executed.</p><p>Let’s see what is emitted if compiled with gcc-10 (with <code>-moutline-atomic</code> making it compatible on all aarch64 machines)</p><table><thead><tr><th align="left">code</th><th align="left">asm (perf output)</th></tr></thead><tbody><tr><td align="left">bool expected = true; flag.compare_exchange_strong(expected, false);</td><td align="left">&lt;<strong>aarch64_cas1_acq_rel&gt;: __aarch64_cas1_acq_rel(): │ adrp x16, 11000 &lt;</strong>data_start&gt; │ ldrb w16, [x16, #25] │ ↓ cbz w16, 14 │ casalb w0, w1, [x2] │ ← ret │14: uxtb w16, w0 │ 18: ldaxrb w0, [x2] │ cmp w0, w16 │ ↓ b.ne 2c │ stlxrb w17, w1, [x2] │ ↑ cbnz w17, 18 │2c: ← ret</td></tr></tbody></table><p>Notice the branching instruction (highlighted in green) to select appropriate logic.</p><h2 id="LSE-in-action"><a href="#LSE-in-action" class="headerlink" title="LSE in action"></a>LSE in action</h2><p>While all this sounds interesting but does it really help? What if the new instruction takes more cycles. Only one way to find out: Benchmark.</p><p>Benchmark: Simple spin-mutex with each thread (total N threads) trying to get the lock there-by causing heavy contention. Once the thread has the mutex it performs software based crc32 keeping the cpu bit busy before releasing it. Each thread does this M times.</p><p><img src="https://mysqlonarm.github.io/images/blog10/lse-microbenchmark.png" alt="img"></p><p>Machine: Bare-Metal with 128 cores ARM Kunpeng 920 2.6 Ghz.</p><ul><li>Use-case represent a serious contention with each thread mostly spending time for obtaining lock. Workload as such is pretty quick (crc32 on 16KB block).</li><li>This clearly proves that LSE helps in heavily contented cases.</li></ul><p>But micro-benchmark sometime fails to show the needed gain with original application due to nature of application including other overlap components like IO, other processing element, etc… So let’s now evaluate MySQL performance with lse-enabled.</p><h3 id="MySQL-benchmarking-with-LSE-1"><a href="#MySQL-benchmarking-with-LSE-1" class="headerlink" title="MySQL benchmarking with LSE"></a>MySQL benchmarking with LSE</h3><p>Workload:</p><ul><li>Server: MySQL-8.0.21, OS: CentOS-7</li><li>Sysbench based point-select, read-only, read-write, update-index and update-non-index workload.</li><li>Executed for higher scalability (&gt;= 64) to explore contention.</li><li>Configuration: 32 cores (single NUMA) ARM Kunpeng 920 2.6 Ghz (28 cores for server, 4 for sysbench)</li><li>Tried 2 use-cases uniform, zipfian (more contention)</li><li>baseline=lse-disabled, lse=lse-enabled (-march=armv8-a+lse).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/sysbench-uniform-lseenabled.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/sysbench-zipfian-lseenabled.png" alt="img"></p><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations:"></a>Observations:</h3><ul><li>With Uniform we hardly see any difference with use of LSE</li><li>With Zipfian LSE tend to regress marginally but consistently (by 2-4%) for update use-cases.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>LSE as feature looks promising but fails to perform in MySQL use-case. May be once MySQL is tuned to use more atomics, LSE could show a +ve difference. Till then nothing we would not loose if LSE is not enabled.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/ARM-LSE-and-MySQL/&quot;&gt;https://mysqlonarm.github.io/ARM-LSE-and-MySQL/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;来看Mysql大牛Krunal带你分析LSE在Mysql上的情况。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Backtraces in PostgreSQL</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/backtraces-in-postgresql/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/backtraces-in-postgresql/</id>
    <published>2020-08-04T06:09:25.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html">https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html</a></p><p>PGSQL 13引入了Backtraces特性，方便客户和管理者定位疑难问题，来看社区大牛Amit带你玩转它！</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>PostgreSQL 13引入了一个简单但非常有用的功能，在发生错误时将堆栈跟踪记录到服务器日志中。让我们看看细节</p><p>有一个 GUC 可以启用 生成 stacktrace: backtrace_functions. 设置为逗号隔开的function名字。</p><p>SET backtrace_functions TO ‘func1,func2’;</p><p>如果从这些函数之一抛出错误，将生成回溯跟踪并记录到服务器日志中。</p><p>注意只有超级用户可以设置该GUC。可以在本地会话中设置，也可以在postgresql.conf文件中全局设置。</p><p>在客户报告错误消息的情况下，很容易看出它是如何起作用的。我们可以在源代码中通过 grep 找到它的来源。但除此之外，就只能靠猜测了。现在不是了，随着这个功能的出现，现在，您可以要求客户将 backtrace_functions设置为所有发出此错误消息的函数，并获取堆栈跟踪。在大多数情况下，错误的根本原因并不是发生错误的函数; 它位于堆栈中间的某个位置; 因此堆栈跟踪是关键的</p><p> 这个功能已经在其他数据库中使用，比如 MySQL，Greenplum，Oracle。</p><p>但在 PostgreSQL 中仍然缺少的东西——这个问题也存在于大多数其他数据库中——是能够在服务器后端由于内存区段错误或其他类似意外信号崩溃时生成堆栈跟踪，或者当服务器由于某种原因而宕机时生成堆栈跟踪。这种能力会带来更大的不同。我们将摆脱生成核心文件的解释步骤。更重要的是，这在崩溃只是随机发生的情况下有所帮助。即使发生了单个意外的崩溃，客户也总是准备好了回溯。我希望这能在 PostgreSQL 的下一个主要版本中实现</p><p>让我们看看 PostgreSQL 堆栈跟踪日志是什么样的。我们将尝试使用一个不存在的类型来创建表。假设我们知道“ type does not exist”错误来自源代码中的 typenameType ()。所以我们这样做:</p><p>postgres=# set backtrace_functions TO ‘typenameType’;<br>postgres=# create table tab (id invalidtype);<br>ERROR: type “invalidtype” does not exist<br>LINE 1: create table tab (id invalidtype);</p><p> 以下是服务器日志中的一个片段: :<br>2020-07-28 20:17:01.482 CST [22454] ERROR: type “invalidtype” does not exist at character 22<br>2020-07-28 20:17:01.482 CST [22454] BACKTRACE:<br>  postgres: amit postgres [local] CREATE TABLE(typenameType+0xa4) [0xaaaaafcd2ac4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x20f550) [0xaaaaafcd4550]<br>  postgres: amit postgres [local] CREATE TABLE(transformCreateStmt+0x53c) [0xaaaaafcd7a10]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44df20) [0xaaaaaff12f20]<br>  postgres: amit postgres [local] CREATE TABLE(standard_ProcessUtility+0x16c) [0xaaaaaff1225c]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44a4e4) [0xaaaaaff0f4e4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44af88) [0xaaaaaff0ff88]<br>  postgres: amit postgres [local] CREATE TABLE(PortalRun+0x198) [0xaaaaaff10ed8]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44764c) [0xaaaaaff0c64c]<br>  postgres: amit postgres [local] CREATE TABLE(PostgresMain+0x970) [0xaaaaaff0d3d4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x3b3be4) [0xaaaaafe78be4]<br>  postgres: amit postgres [local] CREATE TABLE(PostmasterMain+0xdc0) [0xaaaaafe79b70]<br>  postgres: amit postgres [local] CREATE TABLE(main+0x480) [0xaaaaafb82510]<br>  /lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffaac956e0]<br>  postgres: amit postgres [local] CREATE TABLE(+0xbd5d8) [0xaaaaafb825d8]<br>2020-07-29 18:01:02.726 CST [28776] STATEMENT: create table tab (id invalidtype);  </p><p> 回溯的每一行都有函数名、该函数的偏移量和该帧的返回地址</p><p>对于某些堆栈帧，函数名不存在; 相反，函数地址存在。这些都是静态函数。对于这样的函数，函数名不会公开。但是我们可以通过 addr2line 命令行工具从他们的地址中获取他们的名字:</p><p>$ addr2line 0x20f550 0x44df20 -a -f -e <code>which postgres</code><br>0x000000000020f550<br>transformColumnDefinition<br>:?<br>0x000000000044df20<br>ProcessUtilitySlow.constprop.0<br>:?</p><p>如果是调试版本，甚至会打印文件名和偏移量</p><p>现在让我们看看这个简单的特性是如何实现的</p><p>在包括 PostgreSQL、 Greenplum、 MySQL 在内的大多数 RDBMS 中，这个特性都是通过一个简单的函数 backtrace ()来生成 stacktrace:</p><p>int backtrace(void **buffer, int size);</p><p>这个函数只返回帧的所有返回地址。因此，接下来应该调用 backtrace_symbols () ，该函数将 backtrace ()返回的地址转换成字符串，如果可用的话，使用函数名来描述地址:</p><p>char **backtrace_symbols(void *const *buffer, int size);</p><p>这些函数的所有细节都在其man手册页中得到了很好的描述。这些功能在大多数平台上都是可用的.</p><p>注意以下几点:</p><p>\1. 要使函数名可用于 backtrace_symbols () ，必须使用链接器选项构建可执行文件，这些链接器选项允许将所有这些符号添加到“动态符号表”中。这些选项可以通过以下方式之一给出(这些是 gcc 编译器选项) :<br>gcc -rdynamic<br>gcc -Wl,-E</p><p>\2. 当使用 gcc -O2或更高的优化级别编译时，有时可能会丢失特定的堆栈帧。例如，检查这个示例程序<a href="https://drive.google.com/file/d/1UYvT3POmZFmtSa17PcuNvyo9iva8XOS7/view?usp=sharing">backtrace.c 反向追踪</a> 从backtrace() man手册页.</p><p>不使用 -O2来进行编译 :<br>amit:pg:error$ gcc -rdynamic -o backtrace backtrace.c<br>I get the full stack :<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 11 addresses<br>./backtrace(myfunc3+0x2c) [0xaaaad6b2edc0]<br>./backtrace(+0xe84) [0xaaaad6b2ee84]<br>./backtrace(myfunc+0x2c) [0xaaaad6b2eebc]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(main+0x60) [0xaaaad6b2ef28]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffff8c5ba6e0]<br>./backtrace(+0xcc4) [0xaaaad6b2ecc4]</p><p>使用 -O2来进行编译  :<br>amit:pg:error$ gcc -O2 -rdynamic -o backtrace backtrace.c<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 4 addresses<br>./backtrace(myfunc3+0x38) [0xaaaac7183e40]<br>./backtrace(main+0x4c) [0xaaaac7183cfc]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffb91286e0]<br>./backtrace(+0xd38) [0xaaaac7183d38]</p><p>myfunc2()和 myfunc ()没有框架。一种可能性是编译器用尾端调用 myfunc3()替换了 myfunc ()和 myfunc2()的递归调用，这被称为尾端调用优化.</p><p>重点是: 我们需要意识到在一些情况下这种缺失的框架</p></div><div id="English" class="tab-content"><p>PostgreSQL 13 has introduced a simple but extremely useful capability to log a stack trace into the server logs when an error is reported. Let’s see the details.</p><p>There is a GUC to enable stacktrace generation : backtrace_functions. Set it to a comma-separated function names.</p><p>SET backtrace_functions TO ‘func1,func2’;</p><p>If the error is thrown from one of these functions, a backtrace will be generated and logged into the server log.</p><p>Note that only superusers can set the backtrace_functions GUC. It can be set locally in a session, or can be included in postgresql.conf file to globally set it.</p><p>It’s easy to see how it would help in a situation where a customer reports an error message. We can find from where it came from by grep’ing for it in the source code. But beyond that, it was all guess work. Not anymore. Now, you can ask the customer to set backtrace_functions to all such functions which are emitting this error message, and get the stack trace. In most cases, the root cause of the error is not in the function which emits the error; its located somewhere in the middle of the stack; hence the stack trace is critical.</p><p>This capability is already available in many other databases like MySQL, Greenplum, Oracle.</p><p>What’s still missing in PostgreSQL - and is present in most of these other databases - is being able to generate stack trace when a server backend crashes with a segmentation fault or other such unexpected signals, or when the server PANICs due to some reason. This capability would make a much bigger difference. We will get rid of having to explain steps to generate core file. More importantly, this helps in situations where the crash happens only randomly. Even with a single unexpected crash, the customer would always be ready with a backtrace. I am hopeful this would be implemented in the next major release of PostgreSQL.</p><p>Let’s see how a PostgreSQL stack trace log looks like. We will try to use a non-existent type to create a table. Supposing we know that the “type does not exist” error comes from typenameType() in the source code. So we do this :</p><p>postgres=# set backtrace_functions TO ‘typenameType’;<br>postgres=# create table tab (id invalidtype);<br>ERROR:  type “invalidtype” does not exist<br>LINE 1: create table tab (id invalidtype);</p><p>Here’s a snippet from the server log :<br>2020-07-28 20:17:01.482 CST [22454] ERROR:  type “invalidtype” does not exist at character 22<br>2020-07-28 20:17:01.482 CST [22454] BACKTRACE:<br>    postgres: amit postgres [local] CREATE TABLE(typenameType+0xa4) [0xaaaaafcd2ac4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x20f550) [0xaaaaafcd4550]<br>    postgres: amit postgres [local] CREATE TABLE(transformCreateStmt+0x53c) [0xaaaaafcd7a10]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44df20) [0xaaaaaff12f20]<br>    postgres: amit postgres [local] CREATE TABLE(standard_ProcessUtility+0x16c) [0xaaaaaff1225c]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44a4e4) [0xaaaaaff0f4e4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44af88) [0xaaaaaff0ff88]<br>    postgres: amit postgres [local] CREATE TABLE(PortalRun+0x198) [0xaaaaaff10ed8]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44764c) [0xaaaaaff0c64c]<br>    postgres: amit postgres [local] CREATE TABLE(PostgresMain+0x970) [0xaaaaaff0d3d4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x3b3be4) [0xaaaaafe78be4]<br>    postgres: amit postgres [local] CREATE TABLE(PostmasterMain+0xdc0) [0xaaaaafe79b70]<br>    postgres: amit postgres [local] CREATE TABLE(main+0x480) [0xaaaaafb82510]<br>    /lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffaac956e0]<br>    postgres: amit postgres [local] CREATE TABLE(+0xbd5d8) [0xaaaaafb825d8]<br>2020-07-29 18:01:02.726 CST [28776] STATEMENT:  create table tab (id invalidtype);    </p><p>Each line of the backtrace has the function name, an offset into that function, and the return address of that frame.</p><p>For some stack frames, the function name is not present; instead, the function address is present. These are static functions. For such functions, the function names are not exposed. But we may be able to get their names from their addresses, with the help of addr2line command-line tool :</p><p>$ addr2line  0x20f550 0x44df20 -a -f -e <code>which postgres</code><br>0x000000000020f550<br>transformColumnDefinition<br>:?<br>0x000000000044df20<br>ProcessUtilitySlow.constprop.0<br>:?</p><p>If it’s a debug build, even the file name and offset is printed.</p><p>Now let’s see how this simple feature is implemented.</p><p>In most of the RDBMS’s including PostgreSQL, Greenplum, MySQL, the feature is implemented using a simple function backtrace() to generate the stacktrace:</p><p>int backtrace(void **buffer, int size);</p><p>This function only returns all the return addresses of the frames.  So it should be followed by a call to backtrace_symbols() that converts the addresses returned by backtrace() into strings that describe the addresses using the function names if available :</p><p>char **backtrace_symbols(void *const *buffer, int size);</p><p>All the details of these functions are nicely described in their man pages. These functions are available in most of the platforms.</p><p>Note a couple of points :</p><ol><li><p>For the function names to be available for backtrace_symbols(), the executable has to be built using linker options that allow adding all these symbols into a “dynamic symbol table”. These options can be given with one of the following ways (these are gcc compiler options) :<br>gcc -rdynamic<br>gcc -Wl,-E</p></li><li><p>Sometimes particular stack frames might be missing, when compiled with gcc -O2 or higher optimization level. E.g. check this sample program backtrace.c from the backtrace() man pages.</p></li></ol><p>I compile it without -O2 :<br>amit:pg:error$ gcc -rdynamic -o backtrace backtrace.c<br>I get the full stack :<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 11 addresses<br>./backtrace(myfunc3+0x2c) [0xaaaad6b2edc0]<br>./backtrace(+0xe84) [0xaaaad6b2ee84]<br>./backtrace(myfunc+0x2c) [0xaaaad6b2eebc]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(main+0x60) [0xaaaad6b2ef28]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffff8c5ba6e0]<br>./backtrace(+0xcc4) [0xaaaad6b2ecc4]</p><p>Now I compile it with -O2 :<br>amit:pg:error$ gcc -O2 -rdynamic -o backtrace backtrace.c<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 4 addresses<br>./backtrace(myfunc3+0x38) [0xaaaac7183e40]<br>./backtrace(main+0x4c) [0xaaaac7183cfc]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffb91286e0]<br>./backtrace(+0xd38) [0xaaaac7183d38]</p><p>There is no frame for myfunc2() and myfunc(). One possibility is that the compiler has replaced the recursive calls of myfunc() and also myfunc2() call with the tail end call myfunc3(), which is called tail call optimization.</p><p>The point being: we need to be aware of such missing frames in a few scenarios.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PGSQL 13引入了Backtraces特性，方便客户和管理者定位疑难问题，来看社区大牛Amit带你玩转它！&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>HAproxy X86 vs ARM64性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/07/14/haproxy-x86-vs-arm64-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/07/14/haproxy-x86-vs-arm64-xing-neng-bi-pin/</id>
    <published>2020-07-14T03:20:58.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者: <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接: <a href="https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566">https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566</a></p><p>本文是由Apache Tomcat PMC Martin带来的Haproxy最新版本的性能测试报告。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>HAProxy v2.2在几天前刚刚<a href="https://www.haproxy.com/fr/blog/announcing-haproxy-2-2/">发布</a>，所以我决定在 x86_64和 aarch64 虚拟机上对它运行<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">负载测试</a>:</p><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    x86_64</span><br><span class="line">CPU op-mode(s):                  32-bit, 64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">Address sizes:                   42 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              2</span><br><span class="line">Core(s) per socket:              4</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       GenuineIntel</span><br><span class="line">CPU family:                      6</span><br><span class="line">Model:                           85</span><br><span class="line">Model name:                      Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:                        7</span><br><span class="line">CPU MHz:                         3000.000</span><br><span class="line">BogoMIPS:                        6000.00</span><br><span class="line">Hypervisor vendor:               KVM</span><br><span class="line">Virtualization type:             full</span><br><span class="line">L1d cache:                       128 KiB</span><br><span class="line">L1i cache:                       128 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        30.3 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nons</span><br><span class="line">                                 top_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowpref</span><br><span class="line">                                 etch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx51</span><br><span class="line">                                 2cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    aarch64</span><br><span class="line">CPU op-mode(s):                  64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              1</span><br><span class="line">Core(s) per socket:              8</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       0x48</span><br><span class="line">Model:                           0</span><br><span class="line">Stepping:                        0x1</span><br><span class="line">CPU max MHz:                     2400.0000</span><br><span class="line">CPU min MHz:                     2400.0000</span><br><span class="line">BogoMIPS:                        200.00</span><br><span class="line">L1d cache:                       512 KiB</span><br><span class="line">L1i cache:                       512 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        32 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure><p>注意: 我尽可能的让虚拟机的硬件配置更接近，即使用相同的 RAM 类型和大小、相同的磁盘、网卡和带宽。此外，cpu 尽可能相似，但难免有一些差异：</p><ul><li>CPU 频率: 3000 MHz (x86 _ 64) vs 2400 MHz (aarch64)</li><li>BogoMIPS: 6000(x86 _ 64) vs 200(aarch64)</li><li>一级缓存: 128 KiB (x86 _ 64) vs 512 KiB (aarch64)</li></ul><p>两个虚拟机都运行在最新版的Ubuntu 20.04上。</p><p>我的HAProxy 是从<a href="https://github.com/haproxy/haproxy">master</a>分支的源代码构建的，代码与HAProxy v2.2的几乎没有区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">HA-Proxy version 2.3-dev0 2020&#x2F;07&#x2F;07 - https:&#x2F;&#x2F;haproxy.org&#x2F;</span><br><span class="line">Status: development branch - not safe for use in production.</span><br><span class="line">Known bugs: https:&#x2F;&#x2F;github.com&#x2F;haproxy&#x2F;haproxy&#x2F;issues?q&#x3D;is:issue+is:open</span><br><span class="line">Running on: Linux 5.4.0-40-generic #44-Ubuntu SMP Mon Jun 22 23:59:48 UTC 2020 aarch64</span><br><span class="line">Build options :</span><br><span class="line">  TARGET  &#x3D; linux-glibc</span><br><span class="line">  CPU     &#x3D; generic</span><br><span class="line">  CC      &#x3D; clang-9</span><br><span class="line">  CFLAGS  &#x3D; -O2 -Wall -Wextra -Wdeclaration-after-statement -fwrapv -Wno-address-of-packed-member -Wno-unused-label -Wno-sign-compare -Wno-unused-parameter -Wno-missing-field-initializers -Wno-string-plus-int -Wtype-limits -Wshift-negative-value -Wnull-dereference -Werror</span><br><span class="line">  OPTIONS &#x3D; USE_PCRE&#x3D;1 USE_PCRE_JIT&#x3D;1 USE_OPENSSL&#x3D;1 USE_LUA&#x3D;1 USE_ZLIB&#x3D;1 USE_DEVICEATLAS&#x3D;1 USE_51DEGREES&#x3D;1 USE_WURFL&#x3D;1 USE_SYSTEMD&#x3D;1</span><br><span class="line"></span><br><span class="line">Feature list : +EPOLL -KQUEUE +NETFILTER +PCRE +PCRE_JIT -PCRE2 -PCRE2_JIT +POLL -PRIVATE_CACHE +THREAD -PTHREAD_PSHARED +BACKTRACE -STATIC_PCRE -STATIC_PCRE2 +TPROXY +LINUX_TPROXY +LINUX_SPLICE +LIBCRYPT +CRYPT_H +GETADDRINFO +OPENSSL +LUA +FUTEX +ACCEPT4 +ZLIB -SLZ +CPU_AFFINITY +TFO +NS +DL +RT +DEVICEATLAS +51DEGREES +WURFL +SYSTEMD -OBSOLETE_LINKER +PRCTL +THREAD_DUMP -EVPORTS</span><br><span class="line"></span><br><span class="line">Default settings :</span><br><span class="line">  bufsize &#x3D; 16384, maxrewrite &#x3D; 1024, maxpollevents &#x3D; 200</span><br><span class="line"></span><br><span class="line">Built with multi-threading support (MAX_THREADS&#x3D;64, default&#x3D;8).</span><br><span class="line">Built with OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">Running on OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">OpenSSL library supports TLS extensions : yes</span><br><span class="line">OpenSSL library supports SNI : yes</span><br><span class="line">OpenSSL library supports : TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3</span><br><span class="line">Built with Lua version : Lua 5.3.3</span><br><span class="line">Built with DeviceAtlas support (dummy library only).</span><br><span class="line">Built with 51Degrees Pattern support (dummy library).</span><br><span class="line">Built with WURFL support (dummy library version 1.11.2.100)</span><br><span class="line">Built with network namespace support.</span><br><span class="line">Built with zlib version : 1.2.11</span><br><span class="line">Running on zlib version : 1.2.11</span><br><span class="line">Compression algorithms supported : identity(&quot;identity&quot;), deflate(&quot;deflate&quot;), raw-deflate(&quot;deflate&quot;), gzip(&quot;gzip&quot;)</span><br><span class="line">Built with transparent proxy support using: IP_TRANSPARENT IPV6_TRANSPARENT IP_FREEBIND</span><br><span class="line">Built with PCRE version : 8.39 2016-06-14</span><br><span class="line">Running on PCRE version : 8.39 2016-06-14</span><br><span class="line">PCRE library supports JIT : yes</span><br><span class="line">Encrypted password support via crypt(3): yes</span><br><span class="line">Built with clang compiler version 9.0.1 </span><br><span class="line"></span><br><span class="line">Available polling systems :</span><br><span class="line">      epoll : pref&#x3D;300,  test result OK</span><br><span class="line">       poll : pref&#x3D;200,  test result OK</span><br><span class="line">     select : pref&#x3D;150,  test result OK</span><br><span class="line">Total: 3 (3 usable), will use epoll.</span><br><span class="line"></span><br><span class="line">Available multiplexer protocols :</span><br><span class="line">(protocols marked as &lt;default&gt; cannot be specified using &#39;proto&#39; keyword)</span><br><span class="line">            fcgi : mode&#x3D;HTTP       side&#x3D;BE        mux&#x3D;FCGI</span><br><span class="line">       &lt;default&gt; : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H1</span><br><span class="line">              h2 : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H2</span><br><span class="line">       &lt;default&gt; : mode&#x3D;TCP        side&#x3D;FE|BE     mux&#x3D;PASS</span><br><span class="line"></span><br><span class="line">Available services : none</span><br><span class="line"></span><br><span class="line">Available filters :</span><br><span class="line">[SPOE] spoe</span><br><span class="line">[COMP] compression</span><br><span class="line">[TRACE] trace</span><br><span class="line">[CACHE] cache</span><br><span class="line">[FCGI] fcgi-app</span><br></pre></td></tr></table></figure><p>我已经试图通过遵循我在<a href="https://cbonte.github.io/haproxy-dconv/">官方文档</a>和网络上找到的所有最佳实践来尽可能地优化它。</p><p>HAProxy的配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log stdout format raw local0 err</span><br><span class="line">#  nbproc                            8</span><br><span class="line">  nbthread                          32</span><br><span class="line">  cpu-map                           1&#x2F;all 0-7</span><br><span class="line">  tune.ssl.default-dh-param         2048</span><br><span class="line">  tune.ssl.capture-cipherlist-size  1</span><br><span class="line">  ssl-server-verify                 none</span><br><span class="line">  maxconn                           32748</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  timeout client                    60s</span><br><span class="line">  timeout client-fin                 1s</span><br><span class="line">  timeout server                    30s</span><br><span class="line">  timeout server-fin                 1s</span><br><span class="line">  timeout connect                   10s</span><br><span class="line">  timeout http-request              10s</span><br><span class="line">  timeout http-keep-alive           10s</span><br><span class="line">  timeout queue                     10m</span><br><span class="line">  timeout check                     10s</span><br><span class="line">  mode                              http</span><br><span class="line">  log                               global</span><br><span class="line">  option                            dontlog-normal</span><br><span class="line">  option                            httplog</span><br><span class="line">  option                            dontlognull</span><br><span class="line">  option                            http-use-htx</span><br><span class="line">  option                            http-server-close</span><br><span class="line">  option                            http-buffer-request</span><br><span class="line">  option                            redispatch</span><br><span class="line">  retries                           3000</span><br><span class="line"></span><br><span class="line">frontend test_fe</span><br><span class="line">  bind :::8080</span><br><span class="line">  #bind :::8080 ssl crt &#x2F;home&#x2F;ubuntu&#x2F;tests&#x2F;tls&#x2F;server.pem</span><br><span class="line">  default_backend test_be</span><br><span class="line"></span><br><span class="line">backend test_be</span><br><span class="line">  #balance roundrobin</span><br><span class="line">  balance leastconn</span><br><span class="line">  #balance random(2)</span><br><span class="line">  server go1 127.0.0.1:8081 no-check </span><br><span class="line">  server go2 127.0.0.1:8082 no-check</span><br><span class="line">  server go3 127.0.0.1:8083 no-check </span><br><span class="line">  server go4 127.0.0.1:8084 no-check</span><br></pre></td></tr></table></figure><p>通过这种方式，HAProxy 被用作四个 HTTP 服务的前端负载均衡器。</p><p>想使用 SSL方式的话，只需要注释掉第34行并取消注释第35行。</p><p>我使用了<a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#nbthread">多线程</a>设置以获得最佳结果。正如文档所说，这是推荐的设置，而且它也使吞吐量提高了近两倍！此外经过我把吞吐量从8个线程增加到16个线程，再从16个线程增加到32个线程的设置后，发现使用32个线程的效果最好，当使用64个线程时吞吐量开始下降。</p><p>我还使用<code>CPU-map 1/all 0-7</code>将线程<a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#cpu-map">固定</a>在同一个 CPU中。</p><p>另一个重要的设置是用于平衡后端的算法。就像Willy Tarreau的<a href="https://www.haproxy.com/blog/power-of-two-load-balancing/">测试</a>一样。</p><p>正如在 HAProxy Enterprice <a href="https://www.haproxy.com/documentation/hapee/1-7r2/configuration/system-tuning/#disable-irqbalance">文档</a>中所推荐的，我已经禁用了<code>irqbalance</code>。</p><p>最后，我应用了以下内核设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;1024 65024&quot;</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_max_syn_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.core.netdev_max_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">sudo sysctl -w fs.file-max&#x3D;500000</span><br></pre></td></tr></table></figure><p><code>fs.file-max</code> 也与<code>/etc/security/limits. conf</code>中的一些更改有关:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root soft nofile 500000</span><br><span class="line">root hard nofile 500000</span><br><span class="line">* soft nofile 500000</span><br><span class="line">* hard nofile 500000</span><br></pre></td></tr></table></figure><p>对于后端，我使用了用 Golang 编写的非常简单的 HTTP 服务器。他们只是将“ Hello World”写回客户机，而不从磁盘或网络读/写:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; run with: env PORT&#x3D;8081 go run http-server.go</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net&#x2F;http&quot;</span><br><span class="line">    &quot;os&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">    port :&#x3D; os.Getenv(&quot;PORT&quot;)</span><br><span class="line">    if port &#x3D;&#x3D; &quot;&quot; &#123;</span><br><span class="line">      log.Fatal(&quot;Please specify the HTTP port as environment variable, e.g. env PORT&#x3D;8081 go run http-server.go&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    http.HandleFunc(&quot;&#x2F;&quot;, func(w http.ResponseWriter, r *http.Request)&#123;</span><br><span class="line">        fmt.Fprintf(w, &quot;Hello World&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(&quot;:&quot; + port, nil))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对用负载测试客户端，我使用了与<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">测试Apache Tomcat</a>相同设置的<a href="https://github.com/wg/wrk">WRK</a>。</p><p>结果如下:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats Avg    Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency     6.67ms  8.82ms 196.74ms  89.85%</span><br><span class="line"> Req&#x2F;Sec     2.60k   337.06   5.79k   75.79%</span><br><span class="line"> 621350 requests in 30.09s, 75.85MB read</span><br><span class="line">Requests&#x2F;sec: 20651.69</span><br><span class="line">Transfer&#x2F;sec: 2.52MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats  Avg    Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency      3.32ms  4.46ms  75.42ms   94.58%</span><br><span class="line"> Req&#x2F;Sec      4.71k   538.41   8.84k    82.41%</span><br><span class="line"> 1127664 requests in 30.10s, 137.65MB read</span><br><span class="line">Requests&#x2F;sec: 37464.85</span><br><span class="line">Transfer&#x2F;sec: 4.57MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats    Avg   Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       7.92ms  12.50ms  248.52ms 91.18%</span><br><span class="line"> Req&#x2F;Sec       2.42k   338.67   4.34k    80.88%</span><br><span class="line"> 578210 requests in 30.08s, 70.58MB read</span><br><span class="line">Requests&#x2F;sec: 19220.81</span><br><span class="line">Transfer&#x2F;sec: 2.35MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats   Avg   Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       3.56ms 4.83ms  111.51ms 94.25%</span><br><span class="line"> Req&#x2F;Sec       4.46k  609.37   7.23k   85.60%</span><br><span class="line"> 1066831 requests in 30.07s, 130.23MB read</span><br><span class="line">Requests&#x2F;sec: 35474.26</span><br><span class="line">Transfer&#x2F;sec: 4.33MB</span><br></pre></td></tr></table></figure><p>我们可以发现：</p><ul><li>在 x86_64 VM 上，HAProxy 的速度几乎是 aarch64 VM 的两倍。</li><li>并且 TLS offloading减少了5-8% 的吞吐量</li></ul><hr><p><strong>更新1</strong>(2020年7月10日) : 为了确定基于 Golang 的 HTTP 服务器是否是上述测试中的瓶颈，我决定直接针对一个后端(即跳过 HAProxy)运行相同的 WRK 负载测试。</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   615.31us  586.70us  22.44ms   90.61%</span><br><span class="line">    Req&#x2F;Sec    20.05k     1.57k   42.29k    73.62%</span><br><span class="line">  4794299 requests in 30.09s, 585.24MB read</span><br><span class="line">Requests&#x2F;sec: 159319.75</span><br><span class="line">Transfer&#x2F;sec:     19.45MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   774.24us  484.99us  36.43ms   97.04%</span><br><span class="line">    Req&#x2F;Sec    15.28k   413.04    16.89k    73.57%</span><br><span class="line">  3658911 requests in 30.10s, 446.64MB read</span><br><span class="line">Requests&#x2F;sec: 121561.40</span><br><span class="line">Transfer&#x2F;sec:     14.84MB</span><br></pre></td></tr></table></figure><p>在这里我们看到运行在 aarch64上的 HTTP 服务比运行在 x86– 64上的要快30% ！</p><p>更重要的观察结果是，当根本不使用负载均衡器时，arm64的吞吐量要好几倍！我认为问题在于我的设置ーー HAProxy 和4个后端服务器都运行在同一个虚拟机上，所以它们在争夺资源！下面我计划把Golang服务固定到他们自己的 CPU 核心上，让 HAProxy 只使用其他4个 CPU 核心！敬请期待最新消息！</p><hr><p><strong>更新2</strong>(2020年7月10日) :</p><p>为了将进程固定到特定的 cpu，我将使用<code>numactl</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ numactl — hardware</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7</span><br><span class="line">node 0 size: 16012 MB</span><br><span class="line">node 0 free: 170 MB</span><br><span class="line">node distances:</span><br><span class="line">node 0</span><br><span class="line">0: 10</span><br></pre></td></tr></table></figure><p>我已经将 Golang HTTP 服务固定在以下几个方面:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numactl — cpunodebind&#x3D;0 — membind&#x3D;0 — physcpubind&#x3D;4 env PORT&#x3D;8081 go run etc&#x2F;haproxy&#x2F;load&#x2F;http-server.</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>例如，这个后端实例被固定到 CPU 节点0和物理 CPU 4。其他三个后端服务分别固定在物理 cpu 5、6和7上。</p><p>我还对 HAProxy 的配置做了一些改动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nbthread 4</span><br><span class="line">cpu-map 1&#x2F;all 0–3</span><br><span class="line"></span><br><span class="line">Nbthread 4cpu-map 1&#x2F;all 0-3</span><br></pre></td></tr></table></figure><p>也就是说，HAProxy 将产生4个线程，它们将被固定到物理 cpu 0-3上。</p><p>通过这些改变，aarch64的结果保持不变:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     1.44ms    2.11ms  36.48ms   88.36%</span><br><span class="line">    Req&#x2F;Sec     4.98k   651.34     6.62k    74.40%</span><br><span class="line">  596102 requests in 30.10s, 72.77MB read</span><br><span class="line">Requests&#x2F;sec:  19804.19</span><br><span class="line">Transfer&#x2F;sec:      2.42MB</span><br></pre></td></tr></table></figure><p>但是 x86_64下降了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   767.40us  153.24us  19.07ms   97.72%</span><br><span class="line">    Req&#x2F;Sec     5.21k   173.41     5.51k    63.46%</span><br><span class="line">  623911 requests in 30.10s, 76.16MB read</span><br><span class="line">Requests&#x2F;sec:  20727.89</span><br><span class="line">Transfer&#x2F;sec:      2.53MB</span><br></pre></td></tr></table></figure><p>对于 HTTP (没有 TLS)也是如此:</p><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency     1.40ms    2.16ms  36.55ms   88.08%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.55k   462.65     6.97k    69.85%                                                                                                                                                             </span><br><span class="line">  665269 requests in 30.10s, 81.21MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  22102.12                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.70MB</span><br></pre></td></tr></table></figure><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency   726.01us  125.04us   6.42ms   93.95%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.51k   165.80     5.80k    57.24%                                                                                                                                                             </span><br><span class="line">  658777 requests in 30.10s, 80.42MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  21886.50                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.67MB</span><br></pre></td></tr></table></figure><p>因此，现在 HAProxy 在 aarch64上的速度比 x86_64稍快一些，但仍然远远低于每秒120000多个请求的“空负载均衡器”方法。</p><hr><p><strong>更新3</strong>(2020年7月10日) : 在看到 Golang HTTP 服务的性能非常好(120-160K reqs/sec)并简化设置之后，我决定从 Update 2中删除 CPU固定，并使用来自其他 VM 的后端，例如，当在aarch64虚拟机上运行HAProxy时，它将在x86_64上运行的后端之间进行负载均衡；当使用WRK在x86_64上运行HAProxy时，它将使用aarch64虚拟机上运行的 Golang HTTP服务。以下是新的结果：</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.33ms    4.93ms  76.85ms   89.14%</span><br><span class="line">    Req&#x2F;Sec     2.10k   316.84     3.52k    74.50%</span><br><span class="line">  501840 requests in 30.07s, 61.26MB read</span><br><span class="line">Requests&#x2F;sec:  16688.53</span><br><span class="line">Transfer&#x2F;sec:      2.04MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     5.32ms    6.71ms  71.29ms   90.25%</span><br><span class="line">    Req&#x2F;Sec     3.26k   639.12     4.14k    65.52%</span><br><span class="line">  779297 requests in 30.08s, 95.13MB read</span><br><span class="line">Requests&#x2F;sec:  25908.50</span><br><span class="line">Transfer&#x2F;sec:      3.16MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.17ms    5.41ms 292.21ms   91.08%</span><br><span class="line">    Req&#x2F;Sec     2.13k   238.74     3.85k    86.32%</span><br><span class="line">  506111 requests in 30.09s, 61.78MB read</span><br><span class="line">Requests&#x2F;sec:  16821.60</span><br><span class="line">Transfer&#x2F;sec:      2.05MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     3.40ms    2.54ms  58.66ms   97.27%</span><br><span class="line">    Req&#x2F;Sec     3.82k   385.85     4.55k    92.10%</span><br><span class="line">  914329 requests in 30.10s, 111.61MB read</span><br><span class="line">Requests&#x2F;sec:  30376.95</span><br><span class="line">Transfer&#x2F;sec:      3.71MB</span><br></pre></td></tr></table></figure><hr><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>HAProxy 2.2 has been <a href="https://www.haproxy.com/fr/blog/announcing-haproxy-2-2/">released</a> few days ago so I’ve decided to run <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">my load tests</a> against it on my x86_64 and aarch64 VMs:</p><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    x86_64</span><br><span class="line">CPU op-mode(s):                  32-bit, 64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">Address sizes:                   42 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              2</span><br><span class="line">Core(s) per socket:              4</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       GenuineIntel</span><br><span class="line">CPU family:                      6</span><br><span class="line">Model:                           85</span><br><span class="line">Model name:                      Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:                        7</span><br><span class="line">CPU MHz:                         3000.000</span><br><span class="line">BogoMIPS:                        6000.00</span><br><span class="line">Hypervisor vendor:               KVM</span><br><span class="line">Virtualization type:             full</span><br><span class="line">L1d cache:                       128 KiB</span><br><span class="line">L1i cache:                       128 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        30.3 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nons</span><br><span class="line">                                 top_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowpref</span><br><span class="line">                                 etch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx51</span><br><span class="line">                                 2cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    aarch64</span><br><span class="line">CPU op-mode(s):                  64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              1</span><br><span class="line">Core(s) per socket:              8</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       0x48</span><br><span class="line">Model:                           0</span><br><span class="line">Stepping:                        0x1</span><br><span class="line">CPU max MHz:                     2400.0000</span><br><span class="line">CPU min MHz:                     2400.0000</span><br><span class="line">BogoMIPS:                        200.00</span><br><span class="line">L1d cache:                       512 KiB</span><br><span class="line">L1i cache:                       512 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        32 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure><p>Note: the VMs are as close as possible in their hardware capabilities — same type and amount of RAM, same disks, network cards and bandwidth. Also the CPUs are as similar as possible but there are some differences</p><ul><li>the CPU frequency: 3000 MHz (x86_64) vs 2400 MHz (aarch64)</li><li>BogoMIPS: 6000 (x86_64) vs 200 (aarch64)</li><li>Level 1 caches: 128 KiB (x86_64) vs 512 KiB (aarch64)</li></ul><p>Both VMs run Ubuntu 20.04 with latest software updates.</p><p>HAProxy is built from source for the <a href="https://github.com/haproxy/haproxy">master</a> branch, so it might have few changes since the cut of haproxy-2.2 tag!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">HA-Proxy version 2.3-dev0 2020&#x2F;07&#x2F;07 - https:&#x2F;&#x2F;haproxy.org&#x2F;</span><br><span class="line">Status: development branch - not safe for use in production.</span><br><span class="line">Known bugs: https:&#x2F;&#x2F;github.com&#x2F;haproxy&#x2F;haproxy&#x2F;issues?q&#x3D;is:issue+is:open</span><br><span class="line">Running on: Linux 5.4.0-40-generic #44-Ubuntu SMP Mon Jun 22 23:59:48 UTC 2020 aarch64</span><br><span class="line">Build options :</span><br><span class="line">  TARGET  &#x3D; linux-glibc</span><br><span class="line">  CPU     &#x3D; generic</span><br><span class="line">  CC      &#x3D; clang-9</span><br><span class="line">  CFLAGS  &#x3D; -O2 -Wall -Wextra -Wdeclaration-after-statement -fwrapv -Wno-address-of-packed-member -Wno-unused-label -Wno-sign-compare -Wno-unused-parameter -Wno-missing-field-initializers -Wno-string-plus-int -Wtype-limits -Wshift-negative-value -Wnull-dereference -Werror</span><br><span class="line">  OPTIONS &#x3D; USE_PCRE&#x3D;1 USE_PCRE_JIT&#x3D;1 USE_OPENSSL&#x3D;1 USE_LUA&#x3D;1 USE_ZLIB&#x3D;1 USE_DEVICEATLAS&#x3D;1 USE_51DEGREES&#x3D;1 USE_WURFL&#x3D;1 USE_SYSTEMD&#x3D;1</span><br><span class="line"></span><br><span class="line">Feature list : +EPOLL -KQUEUE +NETFILTER +PCRE +PCRE_JIT -PCRE2 -PCRE2_JIT +POLL -PRIVATE_CACHE +THREAD -PTHREAD_PSHARED +BACKTRACE -STATIC_PCRE -STATIC_PCRE2 +TPROXY +LINUX_TPROXY +LINUX_SPLICE +LIBCRYPT +CRYPT_H +GETADDRINFO +OPENSSL +LUA +FUTEX +ACCEPT4 +ZLIB -SLZ +CPU_AFFINITY +TFO +NS +DL +RT +DEVICEATLAS +51DEGREES +WURFL +SYSTEMD -OBSOLETE_LINKER +PRCTL +THREAD_DUMP -EVPORTS</span><br><span class="line"></span><br><span class="line">Default settings :</span><br><span class="line">  bufsize &#x3D; 16384, maxrewrite &#x3D; 1024, maxpollevents &#x3D; 200</span><br><span class="line"></span><br><span class="line">Built with multi-threading support (MAX_THREADS&#x3D;64, default&#x3D;8).</span><br><span class="line">Built with OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">Running on OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">OpenSSL library supports TLS extensions : yes</span><br><span class="line">OpenSSL library supports SNI : yes</span><br><span class="line">OpenSSL library supports : TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3</span><br><span class="line">Built with Lua version : Lua 5.3.3</span><br><span class="line">Built with DeviceAtlas support (dummy library only).</span><br><span class="line">Built with 51Degrees Pattern support (dummy library).</span><br><span class="line">Built with WURFL support (dummy library version 1.11.2.100)</span><br><span class="line">Built with network namespace support.</span><br><span class="line">Built with zlib version : 1.2.11</span><br><span class="line">Running on zlib version : 1.2.11</span><br><span class="line">Compression algorithms supported : identity(&quot;identity&quot;), deflate(&quot;deflate&quot;), raw-deflate(&quot;deflate&quot;), gzip(&quot;gzip&quot;)</span><br><span class="line">Built with transparent proxy support using: IP_TRANSPARENT IPV6_TRANSPARENT IP_FREEBIND</span><br><span class="line">Built with PCRE version : 8.39 2016-06-14</span><br><span class="line">Running on PCRE version : 8.39 2016-06-14</span><br><span class="line">PCRE library supports JIT : yes</span><br><span class="line">Encrypted password support via crypt(3): yes</span><br><span class="line">Built with clang compiler version 9.0.1 </span><br><span class="line"></span><br><span class="line">Available polling systems :</span><br><span class="line">      epoll : pref&#x3D;300,  test result OK</span><br><span class="line">       poll : pref&#x3D;200,  test result OK</span><br><span class="line">     select : pref&#x3D;150,  test result OK</span><br><span class="line">Total: 3 (3 usable), will use epoll.</span><br><span class="line"></span><br><span class="line">Available multiplexer protocols :</span><br><span class="line">(protocols marked as &lt;default&gt; cannot be specified using &#39;proto&#39; keyword)</span><br><span class="line">            fcgi : mode&#x3D;HTTP       side&#x3D;BE        mux&#x3D;FCGI</span><br><span class="line">       &lt;default&gt; : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H1</span><br><span class="line">              h2 : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H2</span><br><span class="line">       &lt;default&gt; : mode&#x3D;TCP        side&#x3D;FE|BE     mux&#x3D;PASS</span><br><span class="line"></span><br><span class="line">Available services : none</span><br><span class="line"></span><br><span class="line">Available filters :</span><br><span class="line">[SPOE] spoe</span><br><span class="line">[COMP] compression</span><br><span class="line">[TRACE] trace</span><br><span class="line">[CACHE] cache</span><br><span class="line">[FCGI] fcgi-app</span><br></pre></td></tr></table></figure><p>I’ve tried to fine tune it as much as I could by following all best practices I was able to find in the <a href="https://cbonte.github.io/haproxy-dconv/">official documentation</a> and in the web.</p><p>The HAProxy config is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log stdout format raw local0 err</span><br><span class="line">#  nbproc                            8</span><br><span class="line">  nbthread                          32</span><br><span class="line">  cpu-map                           1&#x2F;all 0-7</span><br><span class="line">  tune.ssl.default-dh-param         2048</span><br><span class="line">  tune.ssl.capture-cipherlist-size  1</span><br><span class="line">  ssl-server-verify                 none</span><br><span class="line">  maxconn                           32748</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  timeout client                    60s</span><br><span class="line">  timeout client-fin                 1s</span><br><span class="line">  timeout server                    30s</span><br><span class="line">  timeout server-fin                 1s</span><br><span class="line">  timeout connect                   10s</span><br><span class="line">  timeout http-request              10s</span><br><span class="line">  timeout http-keep-alive           10s</span><br><span class="line">  timeout queue                     10m</span><br><span class="line">  timeout check                     10s</span><br><span class="line">  mode                              http</span><br><span class="line">  log                               global</span><br><span class="line">  option                            dontlog-normal</span><br><span class="line">  option                            httplog</span><br><span class="line">  option                            dontlognull</span><br><span class="line">  option                            http-use-htx</span><br><span class="line">  option                            http-server-close</span><br><span class="line">  option                            http-buffer-request</span><br><span class="line">  option                            redispatch</span><br><span class="line">  retries                           3000</span><br><span class="line"></span><br><span class="line">frontend test_fe</span><br><span class="line">  bind :::8080</span><br><span class="line">  #bind :::8080 ssl crt &#x2F;home&#x2F;ubuntu&#x2F;tests&#x2F;tls&#x2F;server.pem</span><br><span class="line">  default_backend test_be</span><br><span class="line"></span><br><span class="line">backend test_be</span><br><span class="line">  #balance roundrobin</span><br><span class="line">  balance leastconn</span><br><span class="line">  #balance random(2)</span><br><span class="line">  server go1 127.0.0.1:8081 no-check </span><br><span class="line">  server go2 127.0.0.1:8082 no-check</span><br><span class="line">  server go3 127.0.0.1:8083 no-check </span><br><span class="line">  server go4 127.0.0.1:8084 no-check</span><br></pre></td></tr></table></figure><p>This way HAProxy is used as a load balancer in front of four HTTP servers.</p><p>To also use it as a SSL terminator one just needs to comment out line 34 and uncomment line 35.</p><p>The best results I’ve achieved by using the <a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#nbthread">multithreaded</a> setup. As the documentation says this is the recommended setup anyway but it also gave me almost twice better throughput! In addition the best results were with 32 threads. The throughput was increasing from 8 to 16 and from 16 to 32, but dropped when used 64 threads.</p><p>I’ve also <a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#cpu-map">pinned</a> the threads to stay at the same CPU for its lifetime with <code>cpu-map 1/all 0–7</code>.</p><p>The other important setting is the algorithm to use to balance between the backends. Just like in Willy Tarreau’s <a href="https://www.haproxy.com/blog/power-of-two-load-balancing/">tests</a> for me <code>leastconn</code> gave the best performance.</p><p>As recommended at HAProxy Enterprice <a href="https://www.haproxy.com/documentation/hapee/1-7r2/configuration/system-tuning/#disable-irqbalance">documentation</a> I’ve disabled <code>irqbalance</code>.</p><p>Finally I’ve applied the following kernel settings:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;1024 65024&quot;</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_max_syn_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.core.netdev_max_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">sudo sysctl -w fs.file-max&#x3D;500000</span><br></pre></td></tr></table></figure><p><code>fs.file-max</code> is related also with a change in <code>/etc/security/limits.conf</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root soft nofile 500000</span><br><span class="line">root hard nofile 500000</span><br><span class="line">* soft nofile 500000</span><br><span class="line">* hard nofile 500000</span><br></pre></td></tr></table></figure><p>For backend I used very simple HTTP servers written in Golang. They just write “Hello World” back to the client without reading/writing from/to disk or to the network:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; run with: env PORT&#x3D;8081 go run http-server.go</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net&#x2F;http&quot;</span><br><span class="line">    &quot;os&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">    port :&#x3D; os.Getenv(&quot;PORT&quot;)</span><br><span class="line">    if port &#x3D;&#x3D; &quot;&quot; &#123;</span><br><span class="line">      log.Fatal(&quot;Please specify the HTTP port as environment variable, e.g. env PORT&#x3D;8081 go run http-server.go&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    http.HandleFunc(&quot;&#x2F;&quot;, func(w http.ResponseWriter, r *http.Request)&#123;</span><br><span class="line">        fmt.Fprintf(w, &quot;Hello World&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(&quot;:&quot; + port, nil))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As load testing client I have used <a href="https://github.com/wg/wrk">WRK</a> with the same setup as for <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">testing Apache Tomcat</a>.</p><p>And now the results:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats Avg    Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency     6.67ms  8.82ms 196.74ms  89.85%</span><br><span class="line"> Req&#x2F;Sec     2.60k   337.06   5.79k   75.79%</span><br><span class="line"> 621350 requests in 30.09s, 75.85MB read</span><br><span class="line">Requests&#x2F;sec: 20651.69</span><br><span class="line">Transfer&#x2F;sec: 2.52MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats  Avg    Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency      3.32ms  4.46ms  75.42ms   94.58%</span><br><span class="line"> Req&#x2F;Sec      4.71k   538.41   8.84k    82.41%</span><br><span class="line"> 1127664 requests in 30.10s, 137.65MB read</span><br><span class="line">Requests&#x2F;sec: 37464.85</span><br><span class="line">Transfer&#x2F;sec: 4.57MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats    Avg   Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       7.92ms  12.50ms  248.52ms 91.18%</span><br><span class="line"> Req&#x2F;Sec       2.42k   338.67   4.34k    80.88%</span><br><span class="line"> 578210 requests in 30.08s, 70.58MB read</span><br><span class="line">Requests&#x2F;sec: 19220.81</span><br><span class="line">Transfer&#x2F;sec: 2.35MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats   Avg   Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       3.56ms 4.83ms  111.51ms 94.25%</span><br><span class="line"> Req&#x2F;Sec       4.46k  609.37   7.23k   85.60%</span><br><span class="line"> 1066831 requests in 30.07s, 130.23MB read</span><br><span class="line">Requests&#x2F;sec: 35474.26</span><br><span class="line">Transfer&#x2F;sec: 4.33MB</span><br></pre></td></tr></table></figure><p>What we see here is:</p><ul><li>that HAProxy is almost twice faster on the x86_64 VM than the aarch64 VM!</li><li>and also that TLS offloading decreases the throughput with around 5–8%</li></ul><hr><p><strong>Update 1</strong> (Jul 10 2020): To see whether the Golang based HTTP servers are not the bottleneck in the above testing I’ve decided to run the same WRK load tests directly against <strong>one</strong> of the backends, i.e. skip HAProxy.</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   615.31us  586.70us  22.44ms   90.61%</span><br><span class="line">    Req&#x2F;Sec    20.05k     1.57k   42.29k    73.62%</span><br><span class="line">  4794299 requests in 30.09s, 585.24MB read</span><br><span class="line">Requests&#x2F;sec: 159319.75</span><br><span class="line">Transfer&#x2F;sec:     19.45MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   774.24us  484.99us  36.43ms   97.04%</span><br><span class="line">    Req&#x2F;Sec    15.28k   413.04    16.89k    73.57%</span><br><span class="line">  3658911 requests in 30.10s, 446.64MB read</span><br><span class="line">Requests&#x2F;sec: 121561.40</span><br><span class="line">Transfer&#x2F;sec:     14.84MB</span><br></pre></td></tr></table></figure><p>Here we see that the HTTP server running on aarch64 is around 30% faster than on x86_64!</p><p>And the more important observation is that the throughput is several times better when not using load balancer at all! I think the problem here is in my setup — both HAProxy and the 4 backend servers run on the same VM, so they fight for resources! I will pin the Golang servers to their own CPU cores and let HAProxy use only the other 4 CPU cores! Stay tuned for an update!</p><hr><p><strong>Update 2</strong> (Jul 10 2020):</p><p>To pin the processes to specific CPUs I will use <code>numactl</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ numactl — hardware</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7</span><br><span class="line">node 0 size: 16012 MB</span><br><span class="line">node 0 free: 170 MB</span><br><span class="line">node distances:</span><br><span class="line">node 0</span><br><span class="line">0: 10</span><br></pre></td></tr></table></figure><p>I’ve pinned the Golang HTTP servers with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numactl — cpunodebind&#x3D;0 — membind&#x3D;0 — physcpubind&#x3D;4 env PORT&#x3D;8081 go run etc&#x2F;haproxy&#x2F;load&#x2F;http-server.</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>i.e. this backend instance is pinned to CPU node 0 and to physical CPU 4. The other three backend servers are pinned respectively to physical CPUs 5, 6 and 7.</p><p>Also I’ve changed slightly the HAProxy configuration:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nbthread 4</span><br><span class="line">cpu-map 1&#x2F;all 0–3</span><br><span class="line"></span><br><span class="line">Nbthread 4cpu-map 1&#x2F;all 0-3</span><br></pre></td></tr></table></figure><p>i.e. HAProxy will spawn 4 threads and they will be pinned to physical CPUs 0–3.</p><p>With these changes the results stayed the same for aarch64:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     1.44ms    2.11ms  36.48ms   88.36%</span><br><span class="line">    Req&#x2F;Sec     4.98k   651.34     6.62k    74.40%</span><br><span class="line">  596102 requests in 30.10s, 72.77MB read</span><br><span class="line">Requests&#x2F;sec:  19804.19</span><br><span class="line">Transfer&#x2F;sec:      2.42MB</span><br></pre></td></tr></table></figure><p>but dropped for x86_64:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   767.40us  153.24us  19.07ms   97.72%</span><br><span class="line">    Req&#x2F;Sec     5.21k   173.41     5.51k    63.46%</span><br><span class="line">  623911 requests in 30.10s, 76.16MB read</span><br><span class="line">Requests&#x2F;sec:  20727.89</span><br><span class="line">Transfer&#x2F;sec:      2.53MB</span><br></pre></td></tr></table></figure><p>and same for HTTP (no TLS):</p><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency     1.40ms    2.16ms  36.55ms   88.08%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.55k   462.65     6.97k    69.85%                                                                                                                                                             </span><br><span class="line">  665269 requests in 30.10s, 81.21MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  22102.12                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.70MB</span><br></pre></td></tr></table></figure><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency   726.01us  125.04us   6.42ms   93.95%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.51k   165.80     5.80k    57.24%                                                                                                                                                             </span><br><span class="line">  658777 requests in 30.10s, 80.42MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  21886.50                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.67MB</span><br></pre></td></tr></table></figure><p>So now HAProxy is a bit faster on aarch64 than on x86_64 but still far slower than the “no load balancer” approach with 120 000+ requests per second.</p><hr><p><strong>Update 3</strong> (Jul 10 2020): After seeing that the performance of the Golang HTTP server is so good (120–160K reqs/sec) and to simplify the setup I’ve decided to remove the CPU pinning from Update 2 and to use the backends from the other VM, i.e. when hitting HAProxy on the aarch64 VM it will load balance between the backends running on the x86_64 and when WRK hits HAProxy running on the x86_64 VM it will use the Golang HTTP servers running on the aarch64 VM. And here are the new results:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.33ms    4.93ms  76.85ms   89.14%</span><br><span class="line">    Req&#x2F;Sec     2.10k   316.84     3.52k    74.50%</span><br><span class="line">  501840 requests in 30.07s, 61.26MB read</span><br><span class="line">Requests&#x2F;sec:  16688.53</span><br><span class="line">Transfer&#x2F;sec:      2.04MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     5.32ms    6.71ms  71.29ms   90.25%</span><br><span class="line">    Req&#x2F;Sec     3.26k   639.12     4.14k    65.52%</span><br><span class="line">  779297 requests in 30.08s, 95.13MB read</span><br><span class="line">Requests&#x2F;sec:  25908.50</span><br><span class="line">Transfer&#x2F;sec:      3.16MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.17ms    5.41ms 292.21ms   91.08%</span><br><span class="line">    Req&#x2F;Sec     2.13k   238.74     3.85k    86.32%</span><br><span class="line">  506111 requests in 30.09s, 61.78MB read</span><br><span class="line">Requests&#x2F;sec:  16821.60</span><br><span class="line">Transfer&#x2F;sec:      2.05MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     3.40ms    2.54ms  58.66ms   97.27%</span><br><span class="line">    Req&#x2F;Sec     3.82k   385.85     4.55k    92.10%</span><br><span class="line">  914329 requests in 30.10s, 111.61MB read</span><br><span class="line">Requests&#x2F;sec:  30376.95</span><br><span class="line">Transfer&#x2F;sec:      3.71MB</span><br></pre></td></tr></table></figure><hr><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者: &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接: &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566&quot;&gt;https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文是由Apache Tomcat PMC Martin带来的Haproxy最新版本的性能测试报告。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>用MySQL EventMutex来理解内存屏障那些事儿</title>
    <link href="https://kunpengcompute.github.io/2020/07/09/yong-mysql-eventmutex-lai-li-jie-nei-cun-ping-zhang-na-xie-shi-er/"/>
    <id>https://kunpengcompute.github.io/2020/07/09/yong-mysql-eventmutex-lai-li-jie-nei-cun-ping-zhang-na-xie-shi-er/</id>
    <published>2020-07-09T01:49:10.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接:  <a href="https://mysqlonarm.github.io/Understanding-Memory-Barrier/">https://mysqlonarm.github.io/Understanding-Memory-Barrier/</a></p><p>组内Mysql大牛Krunal利用Mysql EventMutex来让你彻底理解内存屏障问题，如何优化等。中文版实在是不好翻译，强烈建议阅读英文版增加理解。瓜已经备好了，还等啥？！</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>MySQL 有多种互斥实现，即封装在 pthread 上的、基于 futex 的、基于 Spin-Lock 的(EventMutex)。它们都有自己的优点和缺点，但由于长期以来 MySQL 一直默认使用 EventMutex，因为它被认为是 MySQL应用场景的最佳选择。</p><p>EventMutex 被转换为使用 C++原子操作(MySQL 增加了对C++ 11的支持)。鉴于 MySQL 现在也支持 ARM，正确地使用内存屏障也是保持 EventMutex 向前优化发展的关键。</p><p>在本文中，我们将使用 EventMutex 的一个示例，了解内存障碍，并查看缺少什么，可以优化什么等等。.</p><h2 id="理解获取和释放内存顺序"><a href="#理解获取和释放内存顺序" class="headerlink" title="理解获取和释放内存顺序"></a>理解获取和释放内存顺序</h2><p>ARM/PowerPC 平台使用弱内存模型，这意味着计算操作可以更自由地重新排序，因此同步地确保逻辑正确的屏障非常重要。最简单的方案是依赖使用循序一致性的默认方案(就像x86那样) ，但是这可能会大大影响其他架构的性能。</p><p>通常开发人员必须面对两个障碍(即 顺序) : 获取和释放。</p><ul><li>获取内存顺序意味着在这个内存顺序/屏障之后的任何操作都不能调度/重新排序到该内存顺序/屏障之前(但是在获取该获取内存顺序之前的操作是可以调度/重新排序到它之后)</li><li>释放内存顺序意味着在这个内存顺序/屏障之前的任何操作都不能调度/重新排序到释放该内存顺序/屏障之后(但是在释放该获取内存顺序之后的操作是可以调度/重新排序到它之前)</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img1.png" alt="img"></p><h2 id="理解-EventMutex-结构"><a href="#理解-EventMutex-结构" class="headerlink" title="理解 EventMutex 结构"></a>理解 EventMutex 结构</h2><p>EventMutex 提供了一个普通的互斥类接口，用于帮助同步对临界区的访问。</p><ul><li>Enter (lock mutex)<ul><li>try_lock (如果获得锁，立即返回).<ul><li>利用compare-and-exchange (CAX) 接口设置 m_lock_word 原子变量.</li></ul></li><li>如果锁获取失败<ul><li>进入一个自旋循环，多次尝试后暂停，检查锁是否再次可用。</li><li>如果在“ N”次尝试之后(由 innodb_sync_spin_loops 控制)锁仍然不可用，那么释放(释放 cpu 控制)并通过在 InnoDB 自制的同步数组(sync-array)中注册该线程，让其进入等待状态。另外，在保留插槽后设置一个等待标志(这样可以确保我们在 sync-array 中得到一个插槽)。等待标志是另一个用于协调信号机制的原子变量。</li></ul></li></ul></li><li>Exit (unlock mutex)<ul><li>切换原子变量(m_lock_word)以表示离开临界区。</li><li>检查是否设置了等待标志。如果有设置，那么通过同步数组(sync-array)框架向等待线程发送信号来唤醒它们。</li></ul></li></ul><p>看起来非常简单直接。不是吗？<br>引入内存障碍会使这个过程变得复杂，因为忽略它们将意味着重新排序，这可能会导致代码中出现竞争。</p><p><img src="https://mysqlonarm.github.io/images/blog9/img2.png" alt="img"></p><hr><ul><li>从上面的序列可以很清楚地看出，当锁定 m_lock_word (false-&gt; true)时，它起始于临界区(如果 CAX 成功的话) ，因此在m_lock_word 被获取(设置为 true)之前，流程不应该执行来自临界区的任何语句。</li><li>回到我们的获取-释放屏障的部分，它建议 m_lock_word 应该能<strong>获取一个屏障</strong>一旦它成功了。 <strong>(而不是像它现在这样默认的保持顺序一致(seq_cst))</strong>.</li><li>但是，等等，有两个潜在的结果。失败怎么办？即使在失败的情况下，后续的执行，如自旋，睡眠等待和设置等待标识应该在CAX 评估后。这再次表明<strong>获得屏障</strong>失败的例子 <strong>(而不是像它现在这样默认的保持顺序一致(seq_cst))</strong>. </li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img3.png" alt="img"></p><hr><ul><li>现在让我们看看 m_lock_word <strong>释放屏障</strong>的例子。通常一个释放屏障过程发生在临界区结束要改变m_lock_word的时候。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img4.png" alt="img"></p><hr><ul><li>还有另外一个原子变量(等待标志)也需要一个合适的屏障</li><li>设置服务员标志的动作应该发生在只有当流程中已确保可以得到一个同步阵列(sync-array)插槽的时候。这自然而然就需要一个 <strong>释放屏障</strong>，在set_waiter以上的代码都不会被重新排序。注意: 这是不同的原子操作，所以这里不适用于协调m_lock_word 的获取和释放。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img5.png" alt="img"></p><hr><ul><li>同样的信号相关的逻辑也应该在等待标志被清除之后再进行，所以它应该使用一个<strong>获取屏障</strong>，以确保在清除等待之前不会被重新调度。<strong>(而不是像现在这样释放)</strong>。</li><li>这也将帮助我们使用<strong>relaxed屏障</strong>(vs 获取)来改变waiter-load标志检查。(这里有一个潜在的问题，我们将在下面讨论)。</li><li>有了所有这些，我们也应该能够解决那些比较明显的内存屏障了。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img6.png" alt="img"></p><hr><h3 id="异常情况"><a href="#异常情况" class="headerlink" title="异常情况:"></a>异常情况:</h3><ul><li>在release-barrier的</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock_word</span><br></pre></td></tr></table></figure><p>  在之后的 acquire-barrier</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waiter</span><br></pre></td></tr></table></figure><p>  这是能够重新排序的。</p><ul><li>这甚至是我对潜在风险的理解，我认为这就是为什么 MySQL 在这两个操作之间引入了一个内存屏障。如上所述<a href="https://preshing.com/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock/">blog</a>, C++ 标准应该限制编译器这样做。</li></ul><ul><li><p><strong>relaxed barrier</strong></p></li></ul><p>  对于等待标识(同时检查其值时)有潜在的重新排序，可以将加载指令移动到 m_lock_word释放之前(注意:<strong>释放屏障</strong> 可以让后续的指示得到预先安排，就是排在它之前). </p><ul><li>If “waiter” is true then 调用信号循环来唤醒线程.</li><li>If “waiter” is false then 信号循环将不会被该线程调用，而可能被其他线程调用。</li><li>如果只有2个线程，并且 thread-1通过在释放屏障之前重新排序得到 waiter = false，然后立即发布waiter被 thread-2设置为 true 并继续等待会怎么样。现在，thread-1 将永远不会向thread-2发出信号。</li></ul><p>因此，使用一个<strong>relaxed 屏障</strong>是不可能的，所以让我们转换它使用一个<strong>获取屏障</strong>，应该避免移动后续语句超出上述情况，并作为澄清以上release-acquire需要遵循 C++ 标准。</p><p>所有这些都是为了节省额外的内存屏障。内存屏障的意图是协调同步非原子的操作，因为示例代码中有固有的原子(等待标志)使用适当的内存屏障可以帮助达到所需的效果。</p><p>所以有了这些注意事项，代码就会变成这样</p><p><img src="https://mysqlonarm.github.io/images/blog9/img7.png" alt="img"></p><hr><h3 id="我们从这次代码改造中得到了什么"><a href="#我们从这次代码改造中得到了什么" class="headerlink" title="我们从这次代码改造中得到了什么?"></a>我们从这次代码改造中得到了什么?</h3><p>我们实现了三个目标</p><ul><li>修正了内存屏障的使用，这也有助于澄清代码/流程/开发人员的意图。(这是使用内存屏障所强调的重要事情之一。正确的使用将有助于使代码流程被理解和遵循)。</li><li>从严格的顺序排序移动到单向屏障而不失去正确性(获取和释放)</li><li>避免在非原子操作同步中使用内存屏障。</li></ul><p>除非具有性能影响，否则没有理由进行改造，这次改造也不例外。改造后 ARM 的性能提高了4-15% ，x86_64的性能提高了4-6% 。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>原子操作是好的，但是内存障碍使它们面临挑战，并且确保正确使用内存屏障是在所有平台上获得最佳性能的关键。Barrier 的改造正在迎头赶上，但仍然在起步阶段(尽管在C+11中有所体现) ，因为大多数软件最近开始移植它。正确使用屏障也有助于描述开发者/代码的意图。</p><p><em>如果你有问题，请联系我。</em></p></div><div id="English" class="tab-content"><p>MySQL has multiple mutex implementations viz. wrapper over pthread, futex based, Spin-Lock based (EventMutex). All of them have their own pros and cons but since long MySQL defaulted to EventMutex as it has been found to be optimal for MySQL use-cases.</p><p>EventMutex was switched to use C++ atomic (with MySQL adding support for C++11). Given that MySQL now also support ARM, ensuring a correct use of memory barrier is key to keep the EventMutex Optimal moving forward too.</p><p>In this article we will use an example of EventMutex and understand the memory barrier and also see what is missing, what could be optimized, etc…</p><h2 id="Understanding-acquire-and-release-memory-order"><a href="#Understanding-acquire-and-release-memory-order" class="headerlink" title="Understanding acquire and release memory order"></a>Understanding acquire and release memory order</h2><p>ARM/PowerPC follows weak memory model that means operations can be re-ordered more freely so ensuring the correct barrier with synchronization logic is important. Easiest alternative is to rely on a default one that uses sequential consistency (as done with x86) but it could affect performance big time on other architectures.</p><p>Often a programmer has to deal with 2 barriers (aka order): acquire and release.</p><ul><li>acquire memory order means any operation after this memory-order/barrier can’t be scheduled/re-ordered before it (but operations before it can be scheduled/re-ordered after it)</li><li>release memory order means any operations before this memory-order/barrier can’t be scheduled/re-ordered after it (but operations after it can be scheduled/re-ordered before it).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img1.png" alt="img"></p><h2 id="Understanding-EventMutex-structure"><a href="#Understanding-EventMutex-structure" class="headerlink" title="Understanding EventMutex structure"></a>Understanding EventMutex structure</h2><p>EventMutex provides a normal mutex-like interface meant to help synchronize access to the critical sections.</p><ul><li>Enter (lock mutex)<ul><li>try_lock (try to get the lock if procured return immediately).<ul><li>Uses an atomic variable (m_lock_word) that is set using compare-and-exchange (CAX) interface.</li></ul></li><li>If fail to procure<ul><li>Enter a spin-loop that does multiple attempts to pause followed by check if the lock is again available.</li><li>If after “N” attempts (controlled by innodb_sync_spin_loops) lock is not available then yield (releasing the cpu control) and enter wait by registering thread in InnoDB home-grown sync array implementation. Also, set a waiter flag after reserving the slot (this ensures we will get a slot in sync-array). Waiter flag is another atomic that is used to coordinate the signal mechanism.</li></ul></li></ul></li><li>Exit (unlock mutex)<ul><li>Toggling the atomic variable (m_lock_word) to signify leaving the critical section.</li><li>Check if the waiter flag is set. If yes then signal the waiting thread through the sync-array framework.</li></ul></li></ul><p>Looks pretty straightforward and simple. Isn’t it?<br>Things get complicated with introduction of memory barriers as ignoring them would mean re-ordering can cause race in your code.</p><p><img src="https://mysqlonarm.github.io/images/blog9/img2.png" alt="img"></p><hr><ul><li>From the above sequence it is pretty clear that while locking m_lock_word (false-&gt;true) it could potentially begin the critical section (if CAX succeeds) and so flow shouldn’t execute any statement from the critical section before the lock word is acquired (set to true).</li><li>Going back to our acquire-release barrier section it suggests m_lock_word should take an <strong>acquire barrier</strong> incase of success <strong>(instead of default (seq_cst) as it currently does)</strong>.</li><li>But wait, there are 2 potential outcomes. What about failure? Even in case of failure, followup actions like spin, sleep and set-waiter should be done only post CAX evaluation. This again suggests use of an <strong>acquire barrier</strong> for failure case too. <strong>(instead of default (seq_cst) as it currently does)</strong>.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img3.png" alt="img"></p><hr><ul><li>Now let’s look at the release barrier for m_lock_word. Naturally a <strong>release barrier</strong> will be placed once a critical section is done when the m_lock_word is toggled.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img4.png" alt="img"></p><hr><ul><li>There is another atomic variable (waiter flag) that needs to get a proper barrier too.</li><li>Action to set a waiter flag should be done only when flow has ensured it can get a sync array slot. This naturally invites the need for a <strong>release barrier</strong> so the code is not re-ordered beyond set_waiter. Note: This is different atomic though so the co-ordination of m_lock_word acquire and release will not apply here.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img5.png" alt="img"></p><hr><ul><li>Same way signal logic should be done only after the waiter flag is cleared so it should use an <strong>acquire barrier</strong> that will ensure it is not re-scheduled before the clear-waiter. <strong>(instead of release as it currently does)</strong>.</li><li>This will also help us change the waiter-load flag check to use <strong>relaxed barrier</strong> (vs acquire). (There is a potential catch here; we will discuss it below).</li><li>With all that in place we should able to get rid of explicit memory_fence too.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img6.png" alt="img"></p><hr><h3 id="Anomalies"><a href="#Anomalies" class="headerlink" title="Anomalies:"></a>Anomalies:</h3><ul><li>release-barrier on</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock_word</span><br></pre></td></tr></table></figure><p>  followed by an acquire-barrier on</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waiter</span><br></pre></td></tr></table></figure><p>  this could be reordered.</p><ul><li>This was even my understanding of potential risk and I presume that’s why MySQL introduced a fence between these 2 operations. As per the said <a href="https://preshing.com/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock/">blog</a>, C++ standard should limit compilers from doing so.</li></ul><ul><li>By using a</li></ul><p>  <strong>relaxed barrier</strong></p><p>  for the waiter (while checking for its value) there is potential re-ordering that could move load instruction before the m_lock_word release (note: <strong>release barrier</strong> can allow followup instructions to get scheduled before it).</p><ul><li>If “waiter” is true then a signal loop will be called.</li><li>If “waiter” is false then the signal loop will not be called by this thread but some other thread may call it.</li><li>What if there are only 2 threads and thread-1 evaluates waiter=false by re-ordering it before the release barrier and then immediately posts that waiter is set to true by thread-2 and goes to wait. Now thread-1 will never signal thread-2.</li></ul><p>So using a <strong>relaxed barrier</strong> is not possible so let’s switch it to use an <strong>acquire barrier</strong> that should avoid moving the followup statement beyond the said point and as clarified above release-acquire needs to follow C++ standard.</p><p>All this to help save an extra memory fence. memory-fence intention is to help co-ordinate non-atomic synchronization since our flow has inherent atomic (waiter) using proper memory barrier can help achieve the needed effect.</p><p>So with all that taken-care this is how things would look</p><p><img src="https://mysqlonarm.github.io/images/blog9/img7.png" alt="img"></p><hr><h3 id="What-we-gained-from-this-revamp"><a href="#What-we-gained-from-this-revamp" class="headerlink" title="What we gained from this revamp?"></a>What we gained from this revamp?</h3><p>So we achieved 3 things</p><ul><li>Corrected use of memory barrier that helps also clarify the code/flow/developer intention. (This is one of the important thing stressed with use of memory barrier. Correct use will help make the code flow naturally obvious to understand and follow).</li><li>Moved from strict sequential ordering to one-way barrier without loosing on correctness. (acquire and release)</li><li>Avoided use of fence memory barrier meant for synchronization of non-atomic.</li></ul><p>Revamp is not justified unless it has performance impact and this revamp is no exception. Revamp helps improve performance on ARM in range of 4-15% and on x86_64 in range of 4-6%.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Atomics are good but memory-barrier make them challanging and ensuring proper use of these barriers is key to the optimal performance on all platforms. Adaptation of barrier is catching up but still naive (though present in C+11) as most of the softwares recently started adapting to it. Proper use of barrier help clear the intention too.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接:  &lt;a href=&quot;https://mysqlonarm.github.io/Understanding-Memory-Barrier/&quot;&gt;https://mysqlonarm.github.io/Understanding-Memory-Barrier/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;组内Mysql大牛Krunal利用Mysql EventMutex来让你彻底理解内存屏障问题，如何优化等。中文版实在是不好翻译，强烈建议阅读英文版增加理解。瓜已经备好了，还等啥？！&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>指定NUMA节点运行MySql</title>
    <link href="https://kunpengcompute.github.io/2020/07/03/zhi-ding-numa-jie-dian-yun-xing-mysql/"/>
    <id>https://kunpengcompute.github.io/2020/07/03/zhi-ding-numa-jie-dian-yun-xing-mysql/</id>
    <published>2020-07-03T03:57:23.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/">https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/</a></p><p>指定NUMA节点来运行Mysql，全路程实践，来试试吧</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>“在选定的 NUMA 节点上运行 MySQL”看起来非常简单，但不幸的是，它并不简单。最近，我遇到了一个情况，需要在2个(甚至4个) NUMA 节点上运行 MySQL。</p><p>当然，我尝试的第一件事就是使用 <code>numactl --physcpubind</code> 限制 CPU/Core 集合，只从选定的NUMA节点选择CPUs和Core。MySQL 配置为 <code>innodb_numa_interleave=1</code> ，因此我希望它仅从这个选定的NUMA 节点分配内存(因为我限制了 CPU/core 的使用)。</p><h3 id="Suprise-1"><a href="#Suprise-1" class="headerlink" title="Suprise-1:"></a>Suprise-1:</h3><p>MySQL 使用 <code>numa_all_nodes_ptr-&gt;maskp</code> 这意味着即使 CPU 任务集被限制为2个 NUMA 节点。</p><p>Daniel Black告诉我并让我意识到的两个问题:</p><ul><li><a href="https://github.com/mysql/mysql-server/pull/104">https://github.com/mysql/mysql-server/pull/104</a> (5.7)</li><li><a href="https://github.com/mysql/mysql-server/pull/138">https://github.com/mysql/mysql-server/pull/138</a> (8.0)</li></ul><p>上述问题建议切换到一个更符合逻辑的 <code>numa_get_mems_allowed()</code>. 根据文档，它应该返回一个节点的掩码，告知这个节点被允许为特定的进程分配内存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ref-from-doc: numa_get_mems_allowed() returns the mask of nodes from which the process is allowed to allocate memory in it&#39;s current cpuset context.</span><br></pre></td></tr></table></figure><p>所以我决定应用这个补丁并继续测试。</p><h3 id="Suprise-2"><a href="#Suprise-2" class="headerlink" title="Suprise-2:"></a>Suprise-2:</h3><p>仅仅使用补丁和限制 cpu/core集合并没有帮助，所以我想尝试使用 membind 选项。</p><h3 id="Suprise-3"><a href="#Suprise-3" class="headerlink" title="Suprise-3:"></a>Suprise-3:</h3><p>所以现在这个命令看起来像:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numactl --physcpubind&#x3D; --membind&#x3D;0,1</span><br></pre></td></tr></table></figure><p>这一次，我当然只希望从选定的 NUMA 节点分配内存，但它仍然没有。它从所有4个节点分配内存。</p><p>经过一番文档搜索，建议对 <code>numa_all_nodes_ptr</code> 查看  <code>mems_allowed</code> 字段，如下所述a</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numa_all_nodes_ptr: The set of nodes to record is derived from &#x2F;proc&#x2F;self&#x2F;status, field &quot;Mems_allowed&quot;. The user should not alter this bitmask.</span><br></pre></td></tr></table></figure><p>正如 Alexey Kopytov 在 PR # 138中指出的, <code>numa_all_nodes_ptr</code> 和<code>numa_get_mems_allowed</code> r允许读取相同的NUMA掩码。</p><p>这意味着 <code>numa_get_mems_allowed</code>已经失效，或者文档需要更新。</p><p><em>为了完全确认，我还尝试了 numctl-interleave，但这也没有帮助</em></p><h3 id="事实验证"><a href="#事实验证" class="headerlink" title="事实验证:"></a>事实验证:</h3><p>因此，我决定使用一个简单的程序(在 MySQL 之外)来验证上述事实。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;numa.h&gt;</span><br><span class="line">#include &lt;numaif.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; *numa_all_nodes_ptr-&gt;maskp &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; *numa_get_mems_allowed()-&gt;maskp &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">numactl --membind&#x3D;0-1 .&#x2F;a.out</span><br><span class="line">15</span><br><span class="line">15</span><br></pre></td></tr></table></figure><p>很明显，当 <code>numa_get_mems_allowed</code> 返回的只是允许分配内存的NUMA节点时，两者似乎返回相同的掩码值。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案:"></a>解决方案:</h3><p>我迫切需要一个解决方案，所以尝试使用一个简单的工作方式手动填补掩码(将继续跟进与操作系统供应商 numactl 行为)。这种方法最终奏效了，现在只能从选定的 NUMA 节点分配内存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+const unsigned long numa_mask &#x3D; 0x3;</span><br><span class="line"> </span><br><span class="line"> struct set_numa_interleave_t &#123;</span><br><span class="line">   set_numa_interleave_t() &#123;</span><br><span class="line">     if (srv_numa_interleave) &#123;</span><br><span class="line">       ib::info(ER_IB_MSG_47) &lt;&lt; &quot;Setting NUMA memory policy to&quot;</span><br><span class="line">                                 &quot; MPOL_INTERLEAVE&quot;;</span><br><span class="line">-      if (set_mempolicy(MPOL_INTERLEAVE, numa_all_nodes_ptr-&gt;maskp,</span><br><span class="line">+      if (set_mempolicy(MPOL_INTERLEAVE, &amp;numa_mask,</span><br><span class="line">                         numa_all_nodes_ptr-&gt;size) !&#x3D; 0) &#123;</span><br><span class="line">         ib::warn(ER_IB_MSG_48) &lt;&lt; &quot;Failed to set NUMA memory&quot;</span><br><span class="line">                                   &quot; policy to MPOL_INTERLEAVE: &quot;</span><br><span class="line">@@ -1000,7 +1001,7 @@ static buf_chunk_t *buf_chunk_init(</span><br><span class="line"> #ifdef HAVE_LIBNUMA</span><br><span class="line">   if (srv_numa_interleave) &#123;</span><br><span class="line">     int st &#x3D; mbind(chunk-&gt;mem, chunk-&gt;mem_size(), MPOL_INTERLEAVE,</span><br><span class="line">-                   numa_all_nodes_ptr-&gt;maskp, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">+                   &amp;numa_mask, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">                    MPOL_MF_MOVE);</span><br><span class="line">     if (st !&#x3D; 0) &#123;</span><br><span class="line">       ib::warn(ER_IB_MSG_54) &lt;&lt; &quot;Failed to set NUMA memory policy of&quot;</span><br></pre></td></tr></table></figure><p>(当然，这需要从源代码重新构建，而不是二进制/包(如果想用，往下看)).</p><h3 id="那你为什么不用…"><a href="#那你为什么不用…" class="headerlink" title="那你为什么不用…  ?"></a>那你为什么不用…  ?</h3><p>当然，大多数人可能会建议通过将 <code>innodb_numa_interleave</code> 关闭而使用 membind 来避免这种情况. 当然，这种方法是可行的，但是这种方法略有不同，因为所有分配的内存都受上述限制的约束，而<code>innodb_numa_interleave</code> 仅在缓冲池分配期间适用。它可能应用于特定的目的，但可能不能像这样比较。</p><p>这已经在我的待办事项列表中，以检查 complete interleave vs innodb_numa_interleave带来的影响。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>NUMA 节点上的平衡分配有多个方面，包括核心选择、内存分配、线程分配(同样在选定的 NUMA 节点上)等等。更过的惊喜和令人兴奋的东西等待我们去探索。</p><p><em>如果你有问题/疑问，请联系我。</em></p></div><div id="English" class="tab-content"><p>“Running MySQL on selected NUMA node(s)” looks pretty straightforward but unfortunately it isn’t. Recently, I was faced with a situation that demanded running MySQL on 2 (out of 4) NUMA nodes.</p><p>Naturally, the first thing I tried was to restrict CPU/Core set using <code>numactl --physcpubind</code> selecting only the said CPUs/cores from the said NUMA nodes. MySQL was configured to use <code>innodb_numa_interleave=1</code> so I was expecting it to allocate memory from the said NUMA nodes only (as I restricted usage of CPU/core).</p><h3 id="Suprise-1-1"><a href="#Suprise-1-1" class="headerlink" title="Suprise-1:"></a>Suprise-1:</h3><p>MySQL uses <code>numa_all_nodes_ptr-&gt;maskp</code> that means all the nodes are opted even though the CPU task-set is limited to 2 NUMA nodes.</p><p>Some lookout pointed me to these 2 issues from Daniel Black</p><ul><li><a href="https://github.com/mysql/mysql-server/pull/104">https://github.com/mysql/mysql-server/pull/104</a> (5.7)</li><li><a href="https://github.com/mysql/mysql-server/pull/138">https://github.com/mysql/mysql-server/pull/138</a> (8.0)</li></ul><p>Issue proposes to switch to a more logical <code>numa_get_mems_allowed()</code>. As per the documentation it should return a mask of the node that are are allowed to allocate memory for the said process.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ref-from-doc: numa_get_mems_allowed() returns the mask of nodes from which the process is allowed to allocate memory in it&#39;s current cpuset context.</span><br></pre></td></tr></table></figure><p>So I decided to apply the patch and proceed.</p><h3 id="Suprise-2-1"><a href="#Suprise-2-1" class="headerlink" title="Suprise-2:"></a>Suprise-2:</h3><p>Just applying patch and relying on cpu/core set didn’t helped. So I thought of trying with membind option.</p><h3 id="Suprise-3-1"><a href="#Suprise-3-1" class="headerlink" title="Suprise-3:"></a>Suprise-3:</h3><p>So now the command looks like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numactl --physcpubind&#x3D; --membind&#x3D;0,1</span><br></pre></td></tr></table></figure><p>This time I surely expected that memory would be allocated from the said NUMA nodes only but it still didn’t. Memory was allocated from all 4 nodes.</p><p>Some more documentation search, suggested that for <code>numa_all_nodes_ptr</code> looks at <code>mems_allowed</code> field as mentioned below</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numa_all_nodes_ptr: The set of nodes to record is derived from &#x2F;proc&#x2F;self&#x2F;status, field &quot;Mems_allowed&quot;. The user should not alter this bitmask.</span><br></pre></td></tr></table></figure><p>and as Alexey Kopytov pointed in PR#138, <code>numa_all_nodes_ptr</code> and <code>numa_get_mems_allowed</code> reads the same mask.</p><p>This tends to suggest that <code>numa_get_mems_allowed</code> is broken or documentation needs to be updated.</p><p><em>Just for completeness, I also tried numctl –interleave but that too didn’t helped</em></p><h3 id="Fact-Validation"><a href="#Fact-Validation" class="headerlink" title="Fact Validation:"></a>Fact Validation:</h3><p>So I decided to try this using a simple program (outside MySQL) to validate the said fact.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;numa.h&gt;</span><br><span class="line">#include &lt;numaif.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; *numa_all_nodes_ptr-&gt;maskp &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; *numa_get_mems_allowed()-&gt;maskp &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">numactl --membind&#x3D;0-1 .&#x2F;a.out</span><br><span class="line">15</span><br><span class="line">15</span><br></pre></td></tr></table></figure><p>It is pretty clear that both seem to return the same mask value when <code>numa_get_mems_allowed</code> should return only memory allowed nodes.</p><h3 id="Workaround"><a href="#Workaround" class="headerlink" title="Workaround:"></a>Workaround:</h3><p>I desperately needed a solution so tried using a simple workaround of manually feeding the mask (will continue to follow up about numactl behavior with OS vendor). This approach finally worked and now I can allocate memory from selected NUMA nodes only.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+const unsigned long numa_mask &#x3D; 0x3;</span><br><span class="line"> </span><br><span class="line"> struct set_numa_interleave_t &#123;</span><br><span class="line">   set_numa_interleave_t() &#123;</span><br><span class="line">     if (srv_numa_interleave) &#123;</span><br><span class="line">       ib::info(ER_IB_MSG_47) &lt;&lt; &quot;Setting NUMA memory policy to&quot;</span><br><span class="line">                                 &quot; MPOL_INTERLEAVE&quot;;</span><br><span class="line">-      if (set_mempolicy(MPOL_INTERLEAVE, numa_all_nodes_ptr-&gt;maskp,</span><br><span class="line">+      if (set_mempolicy(MPOL_INTERLEAVE, &amp;numa_mask,</span><br><span class="line">                         numa_all_nodes_ptr-&gt;size) !&#x3D; 0) &#123;</span><br><span class="line">         ib::warn(ER_IB_MSG_48) &lt;&lt; &quot;Failed to set NUMA memory&quot;</span><br><span class="line">                                   &quot; policy to MPOL_INTERLEAVE: &quot;</span><br><span class="line">@@ -1000,7 +1001,7 @@ static buf_chunk_t *buf_chunk_init(</span><br><span class="line"> #ifdef HAVE_LIBNUMA</span><br><span class="line">   if (srv_numa_interleave) &#123;</span><br><span class="line">     int st &#x3D; mbind(chunk-&gt;mem, chunk-&gt;mem_size(), MPOL_INTERLEAVE,</span><br><span class="line">-                   numa_all_nodes_ptr-&gt;maskp, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">+                   &amp;numa_mask, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">                    MPOL_MF_MOVE);</span><br><span class="line">     if (st !&#x3D; 0) &#123;</span><br><span class="line">       ib::warn(ER_IB_MSG_54) &lt;&lt; &quot;Failed to set NUMA memory policy of&quot;</span><br></pre></td></tr></table></figure><p>(Of-course this needs re-build from source code and not an option for binary/package user (well there is .. check following section)).</p><h3 id="But-then-why-didn’t-you-used-…"><a href="#But-then-why-didn’t-you-used-…" class="headerlink" title="But then why didn’t you used … ?"></a>But then why didn’t you used … ?</h3><p>Naturally, most of you may suggest that this could be avoided by toggling <code>innodb_numa_interleave</code> back to OFF and using membind. Of-course this approach works but this approach is slightly different because then all the memory allocated is bounded by the said restriction vs <code>innodb_numa_interleave</code> is applicable only during buffer pool allocation. It may serve specific purpose but may not be so called comparable.</p><p>This has been on my todo list to check effect of complete interleave vs innodb_numa_interleave.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Balance distribution on NUMA node has multiple aspects including core-selection, memory allocation, thread allocation (equally on selected numa node), etc…. Lot of exciting and surprising things to explore.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;指定NUMA节点来运行Mysql，全路程实践，来试试吧&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>利用SIMD Vectorization优化PostgreSQL</title>
    <link href="https://kunpengcompute.github.io/2020/06/24/li-yong-simd-vectorization-you-hua-postgresql/"/>
    <id>https://kunpengcompute.github.io/2020/06/24/li-yong-simd-vectorization-you-hua-postgresql/</id>
    <published>2020-06-24T09:03:35.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html">https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html</a></p><p>团队大牛Amit利用SIMD向量化对ARM和X86硬件平台在PostgreSQL上的优化，欢迎品鉴，相当硬核，坐稳了吗？</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h3 id="Leveraging-SIMD-Vectorization"><a href="#Leveraging-SIMD-Vectorization" class="headerlink" title="Leveraging SIMD Vectorization"></a>Leveraging SIMD Vectorization</h3><p>随着列式存储数据库的出现，人们迫切需要使用 SIMD 向量处理数据表。 这种情况显然很符合表格数据的排列方式。 让我们首先简单介绍一下什么是 SIMD。 它代表单指令多数据流(Single Instruction Multiple Data)。 在当前，CPU 指令支持这种机制，在这种机制中，同一条指令可以在多个数据元素上同时执行。 例如，你想把所有的列值元素加倍。 或者删除图像像素RGB 值的红色部分。 对于大数据场景来说，这些操作是 CPU 的瓶颈。 因此，SIMD 根据每个数据元素的大小，对2、4、8、16或32个(或更多)数据元素同时进行操作，从而大大缩短了 CPU 时间。 假设我们想对“ int32 arr []”的每个元素执行“ arr [ i ] * = 2”。 通常，我们会遍历每个元素来执行这个操作。 在生成的汇编代码中，MUL 指令将在每个元素上执行。 使用 SIMD，我们将划分4个(或更多)相邻的数组元素加载到128位(或更大) CPU“向量”寄存器中，然后让这个寄存器调用 MUL 指令的“向量化”版本，并对随后的每个4数组元素重复这一步骤。</p><p>我们怎么做才能生成这样的向量化汇编指令？ 一种方法是编写这样的汇编代码。 但是在大多数情况下，我们不会这么做，多亏了以下两个方法的出现:</p><p><strong>1. 内部函数实现向量化</strong></p><p>对于程序员来说，调用内部函数就像调用其他函数一样。 在底层，编译器会用适当的程序集指令替换它。 因此，不必使用 c / c + + 代码中的汇编指令来处理寄存器，而是调用相应的内部函数。 每个 CPU 体系结构都有自己的一组内部函数API 和相应的头文件。 作为一个例子，让我们使用 ARM 架构的 SIMD内部函数对 PostgreSQL 代码片段进行向量化，看看通过向量化代码会产生多大的不同。 在此之前，您可能希望快速浏览<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/introducing-neon-for-armv8-a/single-page#fundamentals">NEON架构预览</a>来了解寄存器(registers)、通道(lanes)和向量(vectors)的命名规范。 NEON是 ARM SIMD 架构的品牌名称(The implementation of the Advanced SIMD extension used in <em>ARM</em> processors is called <em>NEON</em>,)。 NEON 单元是 ARMv8芯片的必备部分。</p><p>下面是 PostgreSQL 代码片段，<a href="https://doxygen.postgresql.org/backend_2utils_2adt_2numeric_8c.html#a802955bc87af4f3479b776760c12422b">mul_var() 函数</a> 用于将两个PostgreSQL NUMERIC 数据类型的值相乘. 就像下面的例子那样:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">   i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">  dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br></pre></td></tr></table></figure><p>其中变量声明为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int32 *dig;</span><br><span class="line">int16 var1digit, *var2digits;</span><br></pre></td></tr></table></figure><p>这个例子，你将可以看到循环迭代 i2 + 1次。 在每次迭代中，i 和 i2都会递减。 这意味着，两个数组中的每个数组都有一个固定的连续区段，我们希望在这个区段中对每个数组元素重复执行相同的算术运算。 这里所做的算法是: 将两个 int16变量相乘，然后将乘积加起来得到一个 int32变量。 有一条汇编指令正是这样做的: VMLA。 相应的 内部函数是: vmlal _ s16()</p><p>让我们首先将上面的反向 for-loop 简化为一个等效的正向循环 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">count &#x3D; i2 + 1;</span><br><span class="line">digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>当我们想要对上面的 multiply + accumulate 语句进行向量化时，我们应用下面这个内部函数： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int16x8_t  vmlaq_s16(int16x8_t a, int16x8_t b, int16x8_t c);</span><br></pre></td></tr></table></figure><p>这句代码执行 a + (b * c)并返回结果。 a b c 是矢量。 类型 int16x8_t 表示该向量位于一个128位的 NEON 寄存器中，该寄存器有8个通道，每个通道有16位有符号整数。 所以 vmlaq_s16并行地对3个向量的所有8个通道执行相同的multiply + accumulate操作，并在一个int16x8_t 向量中再次返回8个结果值。 每个multiply + accumulate操作都包含在所有3个向量中的一个特定通道中。<br>如上面 c 代码片段所示，为了避免溢出，将所得的乘法累计值计入一个32位整数。 因此，我们不能使用vmlaq_s16() ，而必须使用一个对16位值进行操作并返回32位值的内部函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int32x4_t vmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c);</span><br></pre></td></tr></table></figure><p>由于128位矢量只能容纳4个32位数据元素，因此4个元素可以并行化，而不是8个。</p><p>可以看出，所有这些操作都使用128位寄存器，它们不需要完全占用，就像使用 int16x4向量那样。 我们需要首先将 C 数组元素值加载到这些寄存器中，最后将结果值从寄存器取回至结果数组元素中。 我们也有实现这种想法的内部函数。 尽管有混合使用标量和向量的内部函数，然而上面内部函数只使用到了向量。 因此，同样的 var1digit 值可以装载到16x4矢量的所有4个通道中。</p><p>结合这些内部函数，最终的代码会是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;arm_neon.h&gt;</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">int i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">int remainder;</span><br><span class="line">int count &#x3D; i2 + 1;</span><br><span class="line">int32 *digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line"></span><br><span class="line">&#x2F;* Load the same var1digit value into all lanes of 16x4 vector. *&#x2F;</span><br><span class="line">int16x4_t  var1digit_16x4 &#x3D; vdup_n_s16(var1digit);   &#x2F;&#x2F; VDUP.16 d0,r0</span><br><span class="line"></span><br><span class="line">&#x2F;* Parallelize each group of 4 digits *&#x2F;</span><br><span class="line">remainder &#x3D; count%4;</span><br><span class="line">count -&#x3D; remainder;</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i +&#x3D; 4)</span><br><span class="line">&#123;</span><br><span class="line">  &#x2F;*</span><br><span class="line">   \* 1. Load required data into vectors</span><br><span class="line">   \* 2. Do multiply-accumulate-long operation using 16x4 vectors,</span><br><span class="line">   \*  whose output is a 32x4 vector which we need, because digptr[]</span><br><span class="line">   \*  is 32bit.</span><br><span class="line">   \* 3. Store back the result vector into digptr[]</span><br><span class="line">   *&#x2F;</span><br><span class="line"></span><br><span class="line">  &#x2F;* Load 4 var2digits into 16x4 vector and digptr into 32x4 *&#x2F;</span><br><span class="line">  int16x4_t  var2digits_16x4 &#x3D; vld1_s16(&amp;var2digits[i]);</span><br><span class="line">  int32x4_t  dig_32x4 &#x3D; vld1q_s32(&amp;digptr[i]);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Vector multiply-accumulate-long: vmlal_&lt;type&gt;. Vr[i] :&#x3D; Va[i] + Vb[i] * Vc[i] *&#x2F;</span><br><span class="line">  dig_32x4 &#x3D; vmlal_s16(dig_32x4, var1digit_16x4, var2digits_16x4);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Store back the result into &amp;digptr[i] *&#x2F;</span><br><span class="line">  vst1q_s32(&amp;digptr[i], dig_32x4);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;* Do the last remaining digits *&#x2F;</span><br><span class="line">for (; remainder !&#x3D; 0; remainder--, i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>我创建了一个包含高精度的数据的模型，<a href="https://drive.google.com/file/d/1H7U5QMksnFuz39djRAbAcunOO8dGWwh2/view?usp=sharing">如图所示</a>, 并以多组t1.val 和 t2.val来执行如下查询。在没有向量化时，执行时间为0.874毫秒:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.840 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.003..0.004 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.156 ms</span><br><span class="line"> Execution Time: **0.874** ms</span><br><span class="line">(6 rows)</span><br><span class="line"></span><br><span class="line">With the above vectorized code, the same query execution time is now .360 ms, i.e. more than 2x speedup :</span><br><span class="line"></span><br><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>使用上面的向量化代码，相同的查询执行时间现在是0.360 ms，即超过2倍的加速: :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>由于数字的个别位数必须与另一个数字的位数相乘，对于精度较高的数字来说，效果更好。 我创建的模式精度在200-600之间。 但是当我在 ARM64 VM 上的测试时，从20精度开始，它的好处就显现出来了。</p><p><strong>2. 自动向量化</strong></p><p>并不总是需要编写使用 内部函数的代码。通常，如果我们组织并简化代码，今天的编译器，使用适当的编译器选项<a href="https://gcc.gnu.org/projects/tree-ssa/vectorization.html#using">尝试识别代码是否可以被向量化</a>, 并生成适当的汇编指令，以便利用 CPU 体系结构的 SIMD。实际上，在上面的代码中，我将反向 for-loop 简化为使用单个变量递增的正向 for-loop，gcc 编译器能够自动对简化的 for-loop 进行向量化。 以下是一些细节:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">index f3a725271e..4243242ad9 100644</span><br><span class="line">--- a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">+++ b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">@@ -7226,6 +7226,7 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">   int        res_weight;</span><br><span class="line">   int        maxdigits;</span><br><span class="line">   int      *dig;</span><br><span class="line">\+   int      *digptr;</span><br><span class="line">   int        carry;</span><br><span class="line">   int        maxdig;</span><br><span class="line">   int        newdig;</span><br><span class="line">@@ -7362,10 +7363,14 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">       *</span><br><span class="line">       \* As above, digits of var2 can be ignored if they don&#39;t contribute,</span><br><span class="line">       \* so we only include digits for which i1+i2+2 &lt;&#x3D; res_ndigits - 1.</span><br><span class="line">\+      *</span><br><span class="line">\+      * For large precisions, this can become a bottleneck; so keep this for</span><br><span class="line">\+      * loop simple so that it can be auto-vectorized.</span><br><span class="line">       *&#x2F;</span><br><span class="line">\-      for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">\-         i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">\-         dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br><span class="line">\+      i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">\+      digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">\+      for (i &#x3D; 0; i &lt;&#x3D; i2; i++)</span><br><span class="line">\+         digptr[i] +&#x3D; var1digit * var2digits[i];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>通过这个修改，在 mul_var()汇编代码中，我可以看到操作 NEON 向量的乘积指令(这些是 arm64指令) :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">smlal  v1.4s, v2.4h, v3.4h</span><br><span class="line">smlal2 v0.4s, v2.8h, v3.8h</span><br></pre></td></tr></table></figure><p>gcc 编译器选项启用自动向量化是“-ftree-loop-vectorize”. 当使用 gcc -O3时，它始终是开启的。</p><p>虽然有一些例子表明 gcc 能够自动向量化甚至是反向循环，但是在上面的例子中，由于两个递减变量，它不能对原始代码这样做。 这就是为什么我必须将其简化为一个单变量递增的正向循环，这是最简单的方式来规避。</p><p>要检查 gcc 是否能够向量化一段代码，请使用 gcc  -fopt-info-all 选项。输出信息如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numeric.c:7217:3: optimized: loop vectorized using 16 byte vectors</span><br><span class="line">Or in case it can&#39;t vectorize, you would see something like this :</span><br><span class="line">numeric.c:7380:3: missed: couldn&#39;t vectorize loop</span><br><span class="line">numeric.c:7381:15: missed: not vectorized: relevant stmt not supported: _39 &#x3D; *_38;</span><br></pre></td></tr></table></figure><p>用这种自动向量化的方法，我观察到的加速比大约是2.7倍。 这种加速比内部函数方式更快快，可能是因为编译器可能比我使用了更好的汇编向量化指令组合。</p><p><strong>总结</strong></p><p>向量化操作可以在重复操作中获得显著的性能提升。 虽然它很适合柱状数据结构，但是当前 PostgreSQL 代码中的一些代码可能会受益于利用 SIMD 进行这种调整。 尽可能使用编译器的自动向量化。 因为这样的做会使代码更干净，更容易移植。 与方法1相比，我们必须使用特定于 CPU 体系结构的内部函数。 但是选择这个例子是为了解释如何使用内部函数来向量化。 在编译器不能对代码进行向量化的情况下，我们应该使用编译器内部函数。 例如:<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/optimizing-c-code-with-neon-intrinsics/single-page#rgb">这个</a>。</p></div><div id="English" class="tab-content"><p>With the advent of column store databases, there was an urge to make use of SIMD vector processing. It naturally fits into the way table data is arranged. Let’s first briefly check what is SIMD. It stands for Single Instruction Multiple Data. Today, CPU instructions support this kind of mechanism where the same instruction can be executed simultaneously on multiple data elements. E.g. Say, you want to double all the column values. Or remove the red component of the RGB values of pixels of an image. For large data, these operations are CPU bottlenecks. So SIMD cuts the CPU time significantly by operating simultaneously on 2, 4, 8, 16 or 32 (or more) data elements depending on the size of each data element. So suppose we want to do “arr[i] *= 2” for each element of “int32 arr[]”. Normally we would iterate through each of the elements for doing this operation. In the generated assembly code, MUL instruction will be run on each of the elements. With SIMD, we would arrange for loading 4 (or more) adjacent array elements into a 128-bit (or larger) CPU “vector” register, and then arrange for a “vectorized” version of the MUL instruction to be called using this register, and repeat this for each subsequent 4 element array section.</p><p>How do we arrange for generating such vectorized assembly instructions ? Well, one way is to write such an assembly code. But in most of the cases, we won’t need this method, thanks to the below two methods :</p><p><strong>1. Vectorization Intrinsics</strong></p><p>For a programmer, an intrinsic is just like any other function call. Underneath, the compiler replaces it with an appropriate assembly instruction. So instead of having to deal with registers using assembly instruction inside C/C++ code, call the corresponding intrinsic function. Each CPU architecture has it’s own set of intrinsics API, and corresponding header file. As an example, let’s vectorize a snippet of PostgreSQL code using ARM architecture’s SIMD intrinsics, to see how big a difference it makes by vectorizing things. Before that, you might want to quickly go through the <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/introducing-neon-for-armv8-a/single-page#fundamentals">NEON architecture</a> to understand the naming conventions for registers, lanes and vectors. NEON is ARM’s brand name for SIMD architecture. NEON unit is a mandatory part of ARMv8 chip.</p><p>Here is a PostgreSQL code snippet from the <a href="https://doxygen.postgresql.org/backend_2utils_2adt_2numeric_8c.html#a802955bc87af4f3479b776760c12422b">mul_var() function</a> that is used to multiply two PostgreSQL NUMERIC data types. As of this writing, it looks like this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">   i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">  dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br></pre></td></tr></table></figure><p>where, the variables are declared as :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int32 *dig;</span><br><span class="line">int16 var1digit, *var2digits;</span><br></pre></td></tr></table></figure><p>Here, you can see that the loop iterates i2+1 times. On each iteration, both i and i2 are decremented. That means, there is a fixed contiguous section of each of the two arrays where we want to repeatedly do the same arithmetic operation for every array element in this section. The arithmetic being done here is : multiply two int16 variables, and add up that product into an int32 variable. An assembly instruction is available which exactly does that : VMLA. The corresponding intrinsic is : vmlal_s16()</p><p>Let’s first simplify the above backward for-loop into an equivalent forward loop :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">count &#x3D; i2 + 1;</span><br><span class="line">digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>So we want to vectorize the above multiply+accumulate statement. We have this intrinsic :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int16x8_t  vmlaq_s16(int16x8_t a, int16x8_t b, int16x8_t c);</span><br></pre></td></tr></table></figure><p>This does a+(b*c) and returns the result. a, b and c are vectors. The type int16x8_t signifies that the vector is in a 128-bit NEON register having 8 lanes, each lane having 16-bit signed integers. So vmlaq_s16() does the multiply+accumulate operation on all 8 lanes of the 3 vectors in parallel, and returns the 8 result values again in a int16x8_t vector. Each multiple+accumulate is contained in one particular lane of all the 3 vectors.<br>To avoid overflow, as can be seen in the above C snippet, the multiplication is accumulated into a 32-bit integer. So instead of vmlaq_s16(), we have to use an intrinsic that operates on 16-bit values and returns 32bit values :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int32x4_t vmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c);</span><br></pre></td></tr></table></figure><p>Since only 4 32-bit data elements can be accommodated in a 128-bit vector, 4 elements could be parallelized rather than 8.</p><p>As can be seen, all these operations use the 128-bit registers, even though they need not be fully occupied, as in the case with int16x4 vectors. We need to first load the C array element values into these registers, and in the end, store the resultant values back from the registers into the result array elements. We have intrinsics for that also. Although there are intrinsics that operate on a mix of scalar and vectors, the intrinsic used above uses only vectors. So the same var1digit value can be loaded into all 4 lanes of a 16x4 vector.</p><p>With these instrinsics, the final code looks like this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">\#include &lt;arm_neon.h&gt;</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">int i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">int remainder;</span><br><span class="line">int count &#x3D; i2 + 1;</span><br><span class="line">int32 *digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line"></span><br><span class="line">&#x2F;* Load the same var1digit value into all lanes of 16x4 vector. *&#x2F;</span><br><span class="line">int16x4_t  var1digit_16x4 &#x3D; vdup_n_s16(var1digit);   &#x2F;&#x2F; VDUP.16 d0,r0</span><br><span class="line"></span><br><span class="line">&#x2F;* Parallelize each group of 4 digits *&#x2F;</span><br><span class="line">remainder &#x3D; count%4;</span><br><span class="line">count -&#x3D; remainder;</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i +&#x3D; 4)</span><br><span class="line">&#123;</span><br><span class="line">  &#x2F;*</span><br><span class="line">   \* 1. Load required data into vectors</span><br><span class="line">   \* 2. Do multiply-accumulate-long operation using 16x4 vectors,</span><br><span class="line">   \*  whose output is a 32x4 vector which we need, because digptr[]</span><br><span class="line">   \*  is 32bit.</span><br><span class="line">   \* 3. Store back the result vector into digptr[]</span><br><span class="line">   *&#x2F;</span><br><span class="line"></span><br><span class="line">  &#x2F;* Load 4 var2digits into 16x4 vector and digptr into 32x4 *&#x2F;</span><br><span class="line">  int16x4_t  var2digits_16x4 &#x3D; vld1_s16(&amp;var2digits[i]);</span><br><span class="line">  int32x4_t  dig_32x4 &#x3D; vld1q_s32(&amp;digptr[i]);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Vector multiply-accumulate-long: vmlal_&lt;type&gt;. Vr[i] :&#x3D; Va[i] + Vb[i] * Vc[i] *&#x2F;</span><br><span class="line">  dig_32x4 &#x3D; vmlal_s16(dig_32x4, var1digit_16x4, var2digits_16x4);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Store back the result into &amp;digptr[i] *&#x2F;</span><br><span class="line">  vst1q_s32(&amp;digptr[i], dig_32x4);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;* Do the last remaining digits *&#x2F;</span><br><span class="line">for (; remainder !&#x3D; 0; remainder--, i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>I created a schema that contains numerics with large precisions, <a href="https://drive.google.com/file/d/1H7U5QMksnFuz39djRAbAcunOO8dGWwh2/view?usp=sharing">as shown here</a>, and ran the following query that multiplies t1.val and t2.val. With the non-vectorized code, the execution time showed .874 milliseconds :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.840 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.003..0.004 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.156 ms</span><br><span class="line"> Execution Time: **0.874** ms</span><br><span class="line">(6 rows)</span><br><span class="line"></span><br><span class="line">With the above vectorized code, the same query execution time is now .360 ms, i.e. more than 2x speedup :</span><br><span class="line"></span><br><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>Since individual digits of the number have to be multiplied by the digits of the other number, the benefit is more for numerics with large precision. The schema I created has values with precisions in the range of 200-600. But the benefit starts showing up from around 20 precision onwards, with my ARM64 VM.</p><p><strong>2. Auto-vectorization</strong></p><p>It’s not always necessary to write code that uses intrinsics. Often if we arrange/simplify the code, today’s compilers, with appropriate compiler options, <a href="https://gcc.gnu.org/projects/tree-ssa/vectorization.html#using">try to identify if the code can be vectorized</a>, and generate appropriate assembly instructions that leverage the CPU architecture’s SIMD. In fact, above where I simplified the backward for-loop to a forward for-loop that uses a single variable increment, the gcc compiler is able to auto-vectorize the simplified for-loop. Here are the changes again:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">index f3a725271e..4243242ad9 100644</span><br><span class="line">--- a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">+++ b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">@@ -7226,6 +7226,7 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">   int        res_weight;</span><br><span class="line">   int        maxdigits;</span><br><span class="line">   int      *dig;</span><br><span class="line">\+   int      *digptr;</span><br><span class="line">   int        carry;</span><br><span class="line">   int        maxdig;</span><br><span class="line">   int        newdig;</span><br><span class="line">@@ -7362,10 +7363,14 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">       *</span><br><span class="line">       \* As above, digits of var2 can be ignored if they don&#39;t contribute,</span><br><span class="line">       \* so we only include digits for which i1+i2+2 &lt;&#x3D; res_ndigits - 1.</span><br><span class="line">\+      *</span><br><span class="line">\+      * For large precisions, this can become a bottleneck; so keep this for</span><br><span class="line">\+      * loop simple so that it can be auto-vectorized.</span><br><span class="line">       *&#x2F;</span><br><span class="line">\-      for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">\-         i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">\-         dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br><span class="line">\+      i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">\+      digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">\+      for (i &#x3D; 0; i &lt;&#x3D; i2; i++)</span><br><span class="line">\+         digptr[i] +&#x3D; var1digit * var2digits[i];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>With this change, in mul_var() assembly code, I could see the multiply-accumulate instructions that operate on NEON vectors (these are arm64 instructions) :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">smlal  v1.4s, v2.4h, v3.4h</span><br><span class="line">smlal2 v0.4s, v2.8h, v3.8h</span><br></pre></td></tr></table></figure><p>gcc compiler option to enable auto-vectorization is “-ftree-loop-vectorize”. With gcc -O3, it is always enabled.</p><p>Although there are examples where gcc is able to auto-vectorize even backward loops, in the above case, it could not do so for the original code, seemingly because of two decrementing variables. That’s why I had to simplify it to a forward loop with a single variable increment, which is as simple as it gets.</p><p>To check whether gcc has been able to vectorize a particular code, use the gcc -fopt-info-all option. This outputs info such as this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numeric.c:7217:3: optimized: loop vectorized using 16 byte vectors</span><br><span class="line">Or in case it can&#39;t vectorize, you would see something like this :</span><br><span class="line">numeric.c:7380:3: missed: couldn&#39;t vectorize loop</span><br><span class="line">numeric.c:7381:15: missed: not vectorized: relevant stmt not supported: _39 &#x3D; *_38;</span><br></pre></td></tr></table></figure><p>With this auto-vectorization method, the speedup I observed was around 2.7x. This speedup is higher than the intrinsics method, probably because the compiler might have used a better combination of assembly vectorized instructions than I did.</p><p><strong>Conclusion</strong></p><p>Vectorizing operations gives significant returns in repetitive operations. Although it suits well for columnar data, there could be some regions in current PostgreSQL code that might benefit from such tweaks to leverage SIMD. As far as possible, we should arrange for the compiler’s auto-vectorization. Such change is cleaner and clearly portable. Compare this with method 1 where we had to use intrinsics specific to the CPU architecture. But that example was chosen for the sake of explaining how to make use of intrinsics. In cases where it is not possible for the compiler to vectorize the code, we should use compiler intrinsics. E.g. <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/optimizing-c-code-with-neon-intrinsics/single-page#rgb">check this out</a>.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;团队大牛Amit利用SIMD向量化对ARM和X86硬件平台在PostgreSQL上的优化，欢迎品鉴，相当硬核，坐稳了吗？&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>NUMA 智能全局计数器 -- 来自MySQL的灵感</title>
    <link href="https://kunpengcompute.github.io/2020/05/28/numa-zhi-neng-quan-ju-ji-shu-qi-lai-zi-mysql-de-ling-gan/"/>
    <id>https://kunpengcompute.github.io/2020/05/28/numa-zhi-neng-quan-ju-ji-shu-qi-lai-zi-mysql-de-ling-gan/</id>
    <published>2020-05-28T06:49:20.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/">https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/</a></p><p>通过之前在X86, ARM虚机上的调研，与遇到的跨NUMA问题，结合自身在运行benchmark测试中的经验，让应用程序在针对特定的全局数据结构中更好的应用底层硬件，达到极致性能体验。下面来用数据说话。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>在多线程系统中管理全局计数器一直是一项挑战。 它们是限制软件可扩展性的一大难题。NUMA 的引入只是增加了复杂性。 幸运的是，在硬件平台提供的支持下发现了多种选择，以帮助解决 / 缓解其中的一些问题。 在本博客中，我们将讨论如何使全局计数器感知NUMA，做到智能选择NUMA，并且看看每种方法对性能有什么影响。</p><p>注意: 很多这方面的工作都是受 MySQL 代码库的启发，是由于MySQL 代码库不断发展中会尝试解决这个问题。</p><h2 id="全局计数器"><a href="#全局计数器" class="headerlink" title="全局计数器"></a>全局计数器</h2><p>大部分软件(例如: 数据库、web-server等等)都需要维持全局计数器。 由于是全局的，这些计数器只有一个副本，有多个工作线程试图更新它。 当然，这复杂的更新需要进行精细的协调，单从这一点，在性能上它就成为可伸缩性的热点分析对象。</p><p>另一种方式是松散地维护这些计数器（不加任何协调工作），但这意味着它们将表示一个近似的数字（特别是在竞争激烈的系统上）。这样不会对性能有过多影响，有一定的益处。</p><p>让我们看看当前生态系统提供了哪些方法来帮助解决这个问题。</p><h3 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h3><p>为了评估不同的方法，我们将考虑一个通用的设置。In order to evaluate different approaches we will consider a common setup.</p><ul><li>试想一些counter-block，它们位于全局级别，因此所有线程都会时不时地更新它们</li><li>试想一些data-block(实际工作负载发生的地方) ，data-block的一部分操作会更新本地全局计数器。 每个数据块都有自己的本地计数器。 只要访问data-block，就会更新这些本地计数器。 这两个data-block是交错的。 请参阅以下的分布图</li></ul><p><img src="https://mysqlonarm.github.io/images/blog7/numa-counter-mem-allocation.png" alt="img"></p><ul><li>让我们通过一些简单的数值例子来理解这个结构。<ul><li>假设我们有100个全局counter-block，每个data-block有10个counter。</li><li>假设我们有1000个全局data-block，它们与counter-block互相交织在一起。</li><li>这意味着，1-counter-block 计数器块之后是10个数据块，这样组合重复100次。</li><li>这样可以确保在 NUMA 节点上分布完整的内存块，并且在访问计数器和data-block时可以看到对NUMA 产生的影响。</li></ul></li><li>Workload (one-round):<ul><li>将访问 n 个data-block(至少足以使 L2缓存失效)。</li><li>作为data-block访问的一部分，还更新与data-block相关联的本地计数器。 data-block是使用 rand ()函数随机选择的，以确保扩散分布。</li><li>接下来是从counter-block访问和更新全局计数器。 随机选择counter-block，并从中随机选择一个计数器(inc 操作)。 重复 M次。</li></ul></li><li>Workload 循环 K 次(rounds).<ul><li>每个线程执行上述Workload循环(K 次) ，从1-256 / 2048开始使用不同的可伸缩性进行基准测试。</li></ul></li></ul><p>注意: 计数器只是简单的uint64_t 值(目前只使用 inc 操作)。</p><p>如果您有兴趣了解更多关于这方面的信息，您可以随时查看<a href="https://github.com/mysqlonarm/benchmark-suites/tree/master/numa/global-counters">这里</a>的详细代码。</p><h3 id="使用的硬件描述"><a href="#使用的硬件描述" class="headerlink" title="使用的硬件描述"></a>使用的硬件描述</h3><ul><li>x86-vm: 24 vCPU (12 cores with HT), 48 GB memory, 2 NUMA nodes, Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz (Ubuntu-18.04)</li><li>arm-vm: 24 vCPU (24 cores), 48 GB memory, 2 NUMA nodes, Kunpeng 920 2.60 GHz (Ubuntu-18.04)</li></ul><p><strong>用于1-2048伸缩性测试的机器</strong></p><ul><li>arm-bms: 128 physical cores, 1 TB of memory, 4 NUMA nodes, Kunpeng 920 2.60 GHz (CentOS-7.8)</li></ul><p>我们的意图不是比较 x86和 ARM，而是比较 NUMA 对全局计数器的影响。</p><h2 id="测试方案"><a href="#测试方案" class="headerlink" title="测试方案"></a>测试方案</h2><p>作为实验的一部分，我们评估了从基础到高级的多种方法。</p><ul><li><strong>pthread-mutex based:</strong> 保护计数器操作的简单 pthread mutexes</li></ul><ul><li><strong>std::mutex:</strong> C++11 支持的 std::mutexes，和pthread mutexes很像，但是更容易使用。</li></ul><ul><li><strong>std::atomic:</strong> C++11 原子变量。</li></ul><ul><li><strong>fuzzy-counter (来自mysql):</strong> 有 n 个 cacheline 对齐的插槽。 随机选择要更新的一个插槽。 若要计算计数器的总值，需要从所有槽中添加值。 没有用于保护槽操作的互斥对象 / 原子对象。 这意味着数值是近似的，但是当需要想之前计算总是时效果最好。 我们将在结果部分看到一个方差系数。[ref: ib_counter_t. N 通常是核心的数量]</li></ul><ul><li><strong>shard-atomic-counter (来自mysql):</strong> 计数器被分割成 N个分片(如上面的 slot)。 每个处理流程都需要知道更新哪个分片。 为了高效的访问，分片都做了cacheline对齐。 [ ref: Counter: : Shard ]</li></ul><ul><li><strong>shard-atomic-counter (thread-id based):</strong> 计数器被分割成 N 个分片(如上面的槽)。 根据执行线程的线程 id 选择要更新的分片。 为了高效的访问，分片都做了cacheline对齐。[这里 N是活动核心的数量]</li></ul><ul><li><p><strong>shard-atomic-counter (cpu-id based):</strong> 计数器被分割成 N个分片。 根据执行核心的 core-id 选择要更新的分片。 为了高效的访问，分片都做了cacheline对齐。[这里N是活动核心的数量。使用sched_getcpu获得cpu-id]</p></li><li><p><strong>shard-atomic-counter (numa-id based):</strong> 计数器被分成 N 个分片。 根据执行核心的 numa-node-id 选择要更新的碎片。 为了高效的访问，分片都做了cacheline对齐。 [这里 N 是活动节点数。 N比较小，在2 / 4 / 8范围内，不是32 / 64 / 128 /等]</p></li></ul><p><em>值得一提的是Mysql 内部还有另一个计数器结构 ut_lock_free_cnt_t()。 它尝试为各自 NUMA 上的每个计数器(值)分配内存，但是在每个numa_alloc_onnode种，即使是8个字节的小块也会按照系统页面大小的分配( Linux 4KB)。 这是大大的空间浪费。 我尝试过这种方法，但由于超负荷的巨大内存，最终没能成功分配内存。</em></p><p>让我们找出哪种方法在 NUMA 环境中最有效。</p><h3 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>使用上面解释的结构和workflow进行基准测试。 每次运行分配好内存后，然后为每个可伸缩性测试运行 K 轮Workflow。 下面的Timing 包括处理数据和计数器的时间，但大部分时间来自计数器争用(通过压制data-block处理已确认)。</p><ul><li><strong>x86-vm [x轴: threads(1-256), y轴: time秒. 越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环10K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/x86.vm.1-256.png" alt="img"></p><h4 id="点评"><a href="#点评" class="headerlink" title="点评"></a>点评</h4><ul><li>正如预期的那样，<strong>带有 cpu-id</strong> 的 shard-atomic-counter 性能最好。</li><li>值得注意的是，简单的原子操作也是最佳的，并且节省了大量的空间。(没有cacheline对齐)。可能是 VM 影响的</li><li>另一个无法解释的行为: fuzzy counter被认为是最快的，但是测试中它不是最快的(我运行了3次同样的benchmark测试证实了这个现象。 在 ARM 上，它表现如预期的那样，因此不像是在基准测试代码出错导致的，需要再分析一下)。</li></ul><p><em>直线非常接近 / 重叠，因此为了确切的表述，共享一下具体数据。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">305.89</td><td align="left">312.78</td><td align="left">275.21</td><td align="left">306.62</td><td align="left">273.52</td><td align="left">278.14</td><td align="left">263.5</td><td align="left">352.45</td></tr><tr><td align="left">256</td><td align="left">608.21</td><td align="left">625.37</td><td align="left">549.15</td><td align="left">611.97</td><td align="left">546.04</td><td align="left">560.18</td><td align="left">521.25</td><td align="left">705.17</td></tr></tbody></table><hr><ul><li><strong>arm-vm [x轴: threads(1-256), y轴: time秒.  越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环10K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.vm.1-256.png" alt="img"></p><h4 id="点评-1"><a href="#点评-1" class="headerlink" title="点评"></a>点评</h4><ul><li>同样，shard-atomic-counter (这次<strong>使用 thread-id</strong>)的得分高于其他方法。 (原因之一可能是在ARM上消耗巨大的sched_getcpu)。 [对于thread-id，线程开始时，在线程本地存储中缓存该线程的唯一标识符]</li><li>FuzzyCounter正在帮助建立基线(假设没有争用的情况下)。</li><li>老旧的 pthread-mutex 似乎也得到了优化</li><li>令人感兴趣的是，随着可伸缩性的增加，ARM 似乎显示出较低的争用(可能是由于 NUMA 的互连性更好)。</li></ul><p><em>直线线条非常接近，在某些情况下也会重叠，因此为了更好的表述，数据如下。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">265.05</td><td align="left">271.53</td><td align="left">272.06</td><td align="left">241.26</td><td align="left">287.1</td><td align="left">258.9</td><td align="left">337.2</td><td align="left">396.88</td></tr><tr><td align="left">256</td><td align="left">529.74</td><td align="left">546.74</td><td align="left">544.07</td><td align="left">481.71</td><td align="left">574.05</td><td align="left">520</td><td align="left">671.63</td><td align="left">795.92</td></tr></tbody></table><hr><ul><li><strong>arm-bms [x轴: threads(1-2048), y轴: time秒. 越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环1K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.bms.1-2048.png" alt="img"></p><h4 id="点评-2"><a href="#点评-2" class="headerlink" title="点评"></a>点评</h4><ul><li>Fuzzy-Counter 帮助设置基线，但是这次我们看到 shard-atomic-counter (<strong>带有thread-id</strong>)几乎与 Fuzzy-Counter (无争用情况)相当。 似乎这就是预期的最佳数字。</li></ul><p><em>直线非常接近，在某些情况下也会重叠，因此为了更好的表述，参看下列具体数据。 以防你没有注意到测试循环次数已经减少到了1K。 由于跨NUMA访问和增加的可伸缩性，保持循环10K次可能会造成更多的时间消耗，也会有更多的噪音。 (注意: 我们现在裸机有4个 NUMA节点)。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">62.81</td><td align="left">63.9</td><td align="left">66.24</td><td align="left">57.37</td><td align="left">64.24</td><td align="left">54.09</td><td align="left">57.67</td><td align="left">72.08</td></tr><tr><td align="left">256</td><td align="left">115.39</td><td align="left">119.53</td><td align="left">126.52</td><td align="left">102.68</td><td align="left">119.01</td><td align="left">102.13</td><td align="left">106.3</td><td align="left">140.83</td></tr><tr><td align="left">512</td><td align="left">228.2</td><td align="left">234.5</td><td align="left">252</td><td align="left">199.71</td><td align="left">241.69</td><td align="left">205.66</td><td align="left">211.29</td><td align="left">279.81</td></tr><tr><td align="left">1024</td><td align="left">456.53</td><td align="left">470.55</td><td align="left">503.73</td><td align="left">398.61</td><td align="left">484.82</td><td align="left">412.43</td><td align="left">427.52</td><td align="left">559.21</td></tr><tr><td align="left">2048</td><td align="left">913.58</td><td align="left">953.56</td><td align="left">1007.94</td><td align="left">805.35</td><td align="left">960.53</td><td align="left">817.45</td><td align="left">862.94</td><td align="left">1132.56</td></tr></tbody></table><hr><p>让我们看看fuzzy-counter的近似因子，区别不是很大。</p><table><thead><tr><th align="left">threads</th><th align="left">global-counter (expected)</th><th align="left">global-counter (actual)</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">20480000</td><td align="left">20479994</td></tr><tr><td align="left">256</td><td align="left">40960000</td><td align="left">40959987</td></tr><tr><td align="left">512</td><td align="left">81920000</td><td align="left">81919969</td></tr><tr><td align="left">1024</td><td align="left">163840000</td><td align="left">163839945</td></tr><tr><td align="left">2048</td><td align="left">327680000</td><td align="left">327679875</td></tr></tbody></table><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Benchmark测试证明了，对全局计数器使用 CPU/thread 亲和性的性能最好。 当然，x86和 ARM 有不同的优化点，因此可以相应地调整 MySQL。 Fuzzy counter替换为atomic (或shard-atomic)，可以更好的节省空间和提高精度(在 x86平台上)。</p><p><em>如果你有问题 / 疑问，请联系我。</em></p></div><div id="English" class="tab-content"><p>Managing global counters in a multi-threaded system has always been challenging. They pose serious scalability challenges. Introduction of NUMA just increased the complexity. Fortunately multiple options have been discovered with hardware lending support to help solve/ease some of these issues. In this blog we will go over how we can make Global Counter NUMA SMART and also see what performance impact each of this approach has.</p><p>Note: a lot of this work is inspired from MySQL codebase that is continuously trying to evolve to solve this issue.</p><h2 id="Global-Counters"><a href="#Global-Counters" class="headerlink" title="Global Counters"></a>Global Counters</h2><p>Most of the software (for example: database, web-server, etc…) needs to maintain some global counters. Being global, there is one copy of these counters and multiple worker threads try to update it. Of-course this invites a need of coordination while updating these copies and in turn it becomes scalability hotspots.</p><p>Alternative is to loosely maintain these counters (w/o any coordination) but that means they will represent an approximate number (especially on a heavily contended system). But they have their own benefits.</p><p>Let’s see what all approaches the current ecosystem provides to help solve this problem.</p><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>In order to evaluate different approaches we will consider a common setup.</p><ul><li>Let’s consider some global counters that are at global level so all threads update them once in a while.</li><li>Let’s consider some data-blocks (where the real workload happens) and as part of this action global counters are updated. Each data-block has its own local counter(s). These local counters are updated whenever data-block is accessed. Both of these blocks are interleaved. Check the arrangement below.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog7/numa-counter-mem-allocation.png" alt="img"></p><ul><li>Let’s try to understand this structure with some simple numeric examples.<ul><li>Say we have 100 global counter-blocks and each data-block has 10 counters.</li><li>Say we have 1000 global data-blocks that are equally interleaved with each counter block.</li><li>That means, 1-counter-block is followed by 10-data-blocks and this combination repeats 100 times.</li><li>This ensures complete memory blocks are distributed across NUMA nodes and we get to see the effect of NUMA while accessing the counters and data-blocks too.</li></ul></li><li>Workload (one-round):<ul><li>Flow will access N data-blocks (at-least enough to invalidate L2 cache).</li><li>As part of the data-block access, local counter(s) associated with the data-block are also updated. Data blocks are randomly selected using rand() function to ensure spread-across distribution.</li><li>This is followed with the access and update of global counters from the counter-block. Random counter-block is selected and a random counter from the selected counter block is updated (inc operation). This operation is repeated M times.</li></ul></li><li>Workload loop is repeated K times (rounds).<ul><li>Each thread executes the said workload loop (K times). Benchmarking is done with different scalability starting from 1-256/2048.</li></ul></li></ul><p>Note: Counter is simply uint64_t value (currently using inc operation only).</p><p>If you are interested in understanding more about this you can always check out the detailed code <a href="https://github.com/mysqlonarm/benchmark-suites/tree/master/numa/global-counters">here</a>.</p><h3 id="Hardware-used"><a href="#Hardware-used" class="headerlink" title="Hardware used"></a>Hardware used</h3><p><strong>For scaling from 1-256</strong></p><ul><li>x86-vm: 24 vCPU (12 cores with HT), 48 GB memory, 2 NUMA nodes, Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz (Ubuntu-18.04)</li><li>arm-vm: 24 vCPU (24 cores), 48 GB memory, 2 NUMA nodes, Kunpeng 920 2.60 GHz (Ubuntu-18.04)</li></ul><p><strong>For scaling from 1-2048</strong></p><ul><li>arm-bms: 128 physical cores, 1 TB of memory, 4 NUMA nodes, Kunpeng 920 2.60 GHz (CentOS-7.8)</li></ul><p>Idea is not to compare x86 vs ARM but the idea is to compare the effect of NUMA on the global counter.</p><h2 id="Approches"><a href="#Approches" class="headerlink" title="Approches"></a>Approches</h2><p>As part of the experiment we evaluated different approaches right from basic to advanced.</p><ul><li><strong>pthread-mutex based:</strong> Simple pthread mutexes that protects operation on counter</li></ul><ul><li><strong>std::mutex:</strong> C++11 enabled std::mutexes just like pthread mutexes but more easier to use with inherent C++11 support.</li></ul><ul><li><strong>std::atomic:</strong> C++11 atomic variable.</li></ul><ul><li><strong>fuzzy-counter (from mysql):</strong> There are N cacheline aligned slots. Flow randomly selects one of the slots to update. To find out the total value of the counter, add value from all the slots. There are no mutexes/atomic that protect the slot operations. This means value is approximate but works best when the flow needs likely count. We will see a variance factor below in result section. [ref: ib_counter_t. N is typically = number of cores].</li></ul><ul><li><strong>shard-atomic-counter (from mysql):</strong> Counter is split into N shards (like slot above). Each flow tells which shard to update. Shards are cache lines aligned for better access. [ref: Counter::Shard]</li></ul><ul><li><strong>shard-atomic-counter (thread-id based):</strong> Counter is split into N shards (like slot above). Shard to update is selected based on thread-id of executing thread. Shards are cache lines aligned for better access. [here N is number-of-active-cores]</li></ul><ul><li><strong>shard-atomic-counter (cpu-id based):</strong> Counter is split into N shards. Shard to update is selected based on core-id of executing core. Shards are cache lines aligned for better access. [here N is number-of-active-cores. cpu-id obtained using sched_getcpu].</li></ul><ul><li><strong>shard-atomic-counter (numa-id based):</strong> Counter is split into N shards. Shard to update is selected based on numa-node-id of the executing core. Shards are cache lines aligned for better access. [here N is number-of-active-numa-nodes. N is small here in the range of 2/4/8 not like 32/64/128/etc…]</li></ul><p><em>There is another counter structure inside MySQL that is worth mentioning ut_lock_free_cnt_t(). It tries to allocate memory for each counter (value) on respective NUMA but as per the numa_alloc_onnode even a smaller chunk of 8 bytes will cause allocation of system-page size (for Linux 4KB). That is too much space wastage. I tried this approach but eventually failed to allocate memory due to enormous memory over-head.</em></p><p>Idea is to find out which approach works best in the NUMA environment.</p><h3 id="Benchmarking-1"><a href="#Benchmarking-1" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>Benchmarking is done using the structure and workload explained above. Each run allocates memory and then K rounds of workload loop per scalability. Timing below includes time to process data and counter but majority of it is coming from counter contention (confirmed by supressing data-block processing).</p><ul><li><strong>x86-vm [x-axis: threads(1-256), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 10K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/x86.vm.1-256.png" alt="img"></p><h4 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h4><ul><li>As expected, shard-atomic-counter <strong>with cpu-id</strong> performs best. (cpu-id = sched_getcpu).</li><li>Suprisingly, simple atomic is optimal too with significant space saved. (No cacheline alignment). May be VM effect.</li><li>Another unexplained behavior: fuzzy counter which is expected to be fastest is not showing up to be fastest (I re-confirmed this behavior with 3 different runs. On ARM, it performing as expected so less likely something going wrong in the benchmarking code. More analysis to be done).</li></ul><p><em>Lines are pretty close/overlapping, so sharing the numeric numbers for higher-sclalability.</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">305.89</td><td align="left">312.78</td><td align="left">275.21</td><td align="left">306.62</td><td align="left">273.52</td><td align="left">278.14</td><td align="left">263.5</td><td align="left">352.45</td></tr><tr><td align="left">256</td><td align="left">608.21</td><td align="left">625.37</td><td align="left">549.15</td><td align="left">611.97</td><td align="left">546.04</td><td align="left">560.18</td><td align="left">521.25</td><td align="left">705.17</td></tr></tbody></table><hr><ul><li><strong>arm-vm [x-axis: threads(1-256), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 10K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.vm.1-256.png" alt="img"></p><h4 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h4><ul><li>Again, shard-atomic-counter (this time <strong>with thread-id</strong>) scored better than other alternatives. (one of the reason could be sched_getcpu is costly on ARM). [For thread-id, flow cached thread unique identifier during creation, in thread-local storage].</li><li>FuzzyCounter is helping establish baseline (given there is no-contention).</li><li>Good old pthread-mutex seems to be optimized too.</li><li>Intererstingly, ARM seems to be showing lower contention with increase scalability (may be due to better NUMA interconnect).</li></ul><p><em>Lines are pretty close and in some cases overlapping too, so sharing the numeric numbers for higher-sclalability.</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">265.05</td><td align="left">271.53</td><td align="left">272.06</td><td align="left">241.26</td><td align="left">287.1</td><td align="left">258.9</td><td align="left">337.2</td><td align="left">396.88</td></tr><tr><td align="left">256</td><td align="left">529.74</td><td align="left">546.74</td><td align="left">544.07</td><td align="left">481.71</td><td align="left">574.05</td><td align="left">520</td><td align="left">671.63</td><td align="left">795.92</td></tr></tbody></table><hr><ul><li><strong>arm-bms [x-axis: threads(1-2048), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 1K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.bms.1-2048.png" alt="img"></p><h4 id="Comments-2"><a href="#Comments-2" class="headerlink" title="Comments"></a>Comments</h4><ul><li>Fuzzy-Counter help set the baseline but this time we see shard-atomic-counter (<strong>with thread-id</strong>) is almost on-par with Fuzzy-Counter (non-contention use-case). That is like optimal number to expect.</li></ul><p><em>Lines are pretty close and in some cases overlapping too, so sharing the numeric numbers for higher-sclalability. Just incase you have not noticed the rounds has been reduced by 1K. Keeping it 10K increases timing like anything due to cross-numa access and increased scalability. (note: we are now on operating machine with 4 numa nodes).</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">62.81</td><td align="left">63.9</td><td align="left">66.24</td><td align="left">57.37</td><td align="left">64.24</td><td align="left">54.09</td><td align="left">57.67</td><td align="left">72.08</td></tr><tr><td align="left">256</td><td align="left">115.39</td><td align="left">119.53</td><td align="left">126.52</td><td align="left">102.68</td><td align="left">119.01</td><td align="left">102.13</td><td align="left">106.3</td><td align="left">140.83</td></tr><tr><td align="left">512</td><td align="left">228.2</td><td align="left">234.5</td><td align="left">252</td><td align="left">199.71</td><td align="left">241.69</td><td align="left">205.66</td><td align="left">211.29</td><td align="left">279.81</td></tr><tr><td align="left">1024</td><td align="left">456.53</td><td align="left">470.55</td><td align="left">503.73</td><td align="left">398.61</td><td align="left">484.82</td><td align="left">412.43</td><td align="left">427.52</td><td align="left">559.21</td></tr><tr><td align="left">2048</td><td align="left">913.58</td><td align="left">953.56</td><td align="left">1007.94</td><td align="left">805.35</td><td align="left">960.53</td><td align="left">817.45</td><td align="left">862.94</td><td align="left">1132.56</td></tr></tbody></table><hr><p>Let’s see approximation factor for fuzzy-counter. Not that major difference.</p><table><thead><tr><th align="left">threads</th><th align="left">global-counter (expected)</th><th align="left">global-counter (actual)</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">20480000</td><td align="left">20479994</td></tr><tr><td align="left">256</td><td align="left">40960000</td><td align="left">40959987</td></tr><tr><td align="left">512</td><td align="left">81920000</td><td align="left">81919969</td></tr><tr><td align="left">1024</td><td align="left">163840000</td><td align="left">163839945</td></tr><tr><td align="left">2048</td><td align="left">327680000</td><td align="left">327679875</td></tr></tbody></table><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Benchmark study has proved that using CPU/thread affinity for global counters works best. Of-course x86 and ARM has different optimization point so MySQL could be tuned accordingly. Fuzzy counter could be better replaced with atomic (or shard-atomic) given space saved and improved accurancy (on x86).</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/&quot;&gt;https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过之前在X86, ARM虚机上的调研，与遇到的跨NUMA问题，结合自身在运行benchmark测试中的经验，让应用程序在针对特定的全局数据结构中更好的应用底层硬件，达到极致性能体验。下面来用数据说话。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQL ARM和X86性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/05/22/postgresql-arm-he-x86-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/05/22/postgresql-arm-he-x86-xing-neng-bi-pin/</id>
    <published>2020-05-22T09:03:28.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html">https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html</a></p><p>由团队内部PostgreSQL大牛Amit在ARM和X86上针对PostgreSQL的性能比拼测试。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h3 id="PostgreSQL-on-ARM"><a href="#PostgreSQL-on-ARM" class="headerlink" title="PostgreSQL on ARM"></a>PostgreSQL on ARM</h3><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>在我的<a href="https://amitdkhan-pg.blogspot.com/2020/04/arm-points-to-be-noted.html">上个博客</a>, 我写道，如果那些已经运行在X86的应用程序要在不同的架构上运行，比如 ARM，那么这些应用程序可能需要进行一些优化。 让我们来看看它具体指的是什么。</p><p>最近我一直在使用 ARM64机器测试 PostgreSQL RDBMS。几个月前，我甚至不知道它是否可以在 ARM 上编译，因为忽略了一个事实，即我们有一个用于 ARM64的常规构建机器，<a href="https://buildfarm.postgresql.org/cgi-bin/show_history.pl?nm=eelpout&br=HEAD">已经很长时间了</a>. 现在甚至连 PostgreSQL apt 库也开始制作<a href="https://www.df7cb.de/blog/2020/arm64-on-apt.postgresql.org.html">ARM64 PostgreSQL</a> 的包了。但是在我用不同的场景测试了 PostgreSQL-on-ARM 之后，我才真正对它的可靠性有了信心。</p><p>我从read-only pgbench 测试开始，比较了 x86_64和 ARM_64虚机的测试结果。 目的不是比较任何特定的 CPU 实现。 这个想法是为了找出ARM 上的 PostgreSQL 与X86相比表现不尽如人意的场景。</p><p><strong>Test Configuration</strong></p><p>ARM64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 2.6 GHz; available RAM : 11GB<br>x86_64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 3.0 GHz; available RAM : 11GB</p><p>以下是所有测试的共用的配置:</p><p>PostgreSQL parameters changed : shared_buffers = 8GB<br>pgbench scale factor : 30<br>pgbench command :<br>for num in 2 4 6 8 10 12 14<br>do<br>  pgbench [-S] -c $num -j $num -M prepared -T 40<br>done<br>它的意思是: pgbench 运行的并行client数量越来越多，从2个到14个不等。</p><p><strong>Select-only workload</strong></p><p>pgbench -S 选项应用在 read-only workload.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="img"></a></p><p> 在2个线程和4个线程之间，x86的性能比 ARM 高出30% ，而且差距越来越大。 当线程数在4到6之间，曲线变得平坦了一点，再到线程数6到8之间，曲线突然变得陡峭。 然后线程数到达8之后，由于测试机上有8个 cpu，预计它会变平或下降。 但是还有更多的原因。 这里Pgbench client运行在与安装 PostgreSQL server相同的一台机器上。 通过充分利用 cpu，Pgbench client 占用了大约20% 的 cpu。 所以client从6个线程开始对测试产生干扰。 尽管如此，ARM 和 x86的性能差距还是在线程数6到8之间急剧上涨。 我还没有理解为什么会这样，可能与 Linux 调度程序以及 pgbench client和PostgreSQL server之间的交互有关。 注意，x86和 ARM 的曲线形状基本相似。 所以这种行为并不是架构特有的。 不过，这些曲线的一个不同之处在于: ARM 曲线从8个线程开始下降幅度稍大一些。 此外，线程数在6到8之间时，ARM 的处理事务量的增长并不像 x86那样剧烈。 因此，这种情况的最终结果是: 随着 cpu 变得越来越忙，ARM 上的 PostgreSQL 越来越落后于 x86。 让我们看看如果移除 pgbench client带来的干扰会发生什么。</p><p><strong>select exec_query_in_loop(n)</strong></p><p>因此，为了避免由于同一台机器上的pgbench client对PostgreSQL server造成的干扰，我想测试一下它的查询性能。为此，pgbench client运行在另一台机器上，但这可能会产生另一种的噪音: 网络延迟。 所以，我写了一个 PostgreSQL <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">C language user-defined function</a> ，用来循环执行与 pgbench 测试运行的完全相同的 SQL 查询。 使用 pgbench 自定义脚本执行此函数。现在，pgbench client大部分都是空闲的。 另外，这不会占用提交 / 回滚程度时间，因为大部分时间将花费在这个C 函数上。</p><p>pgbench 自定义脚本 : select exec_query_in_loop(n);<br>其中 n 是 pgbench 查询在PostgreSQL server上一次循环执行的次数<br>与pgbench -S选项作用下的普通循环查询：<br>SELECT abalance FROM pgbench_accounts WHERE aid = $1<br>详情参看 <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">exec_query_in_loop</a>()</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>现在，你看到一个非常不同的曲线。 对于这两条曲线，最多为8个线程，事务率与线程数成线性比例。 正如预期的那样，在线程数达到8以后，处理的事务就不会上升了。 而且，即使对 ARM 来说，它也有相同的行为。 与 x86相比，PostgreSQL 在 ARM上从头到尾慢了35% 左右。 考虑到 ARM 处理器的频率是2.6 GHz，而 x86是3.0 GHz，这么一说看起来这性能还不错。 注意，事务率是个位数，因为函数 exec_query_in_loop(n)中是用 n=100000来执行的。</p><p>这个实验还表明，之前使用内置 pgbench 脚本的性能测试结果与 pgbench client干扰有关。 而且，ARM 对于竞争线程的倾斜曲线不是由服务器中的争用引起的。 请注意，事务率是在客户端计算的。 因此，特别是在高争用场景中, 即使查询中的结果已经准备就绪，然而client请求结果、计算时间戳等仍然可能会有一些延迟。</p><p><strong>select exec_query_in_loop(n) - PL/pgSQL function</strong></p><p>在使用用户定义的 c 函数之前，我使用了 <a href="https://drive.google.com/file/d/1-jiYEOIVHdtp8Yfv6QrYhrO06urHILd5/view?usp=sharing">PL/pgSQL function来做同样的事</a>. 我偶然发现了一种不同的表现行为。</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>在这里，无论线程数量如何，ARM 上的 PostgreSQL 都比 x86慢65% 左右。 与之前使用 C 函数的结果相比，由于某种原因，很明显 PL/pgsql 在 ARM 上的执行速度非常慢。 我检查了 perf 输出的报告，在 ARM 和 x86中看到的热点函数大致相同。 但由于某些原因，在 PL/pgsql 函数内执行的任何操作在 ARM 上都比在 x86上慢得多。</p><p>我还没有检查缓存失败，看看缓存失败是否在 ARM 上会更多。 在撰写本文时，我所做的是这样的(在PostgreSQL内部是这样的) : exec_stmt_foreach_a()调用exec_stmt()。 我将 exec_stmt()克隆为 exec_stmt_clone() ，并将 exec_stmt_foreach_a()调用 exec_stmt_clone()。 这加快了整体执行的速度，对于 ARM 来说却加快了20%多 。 到目前为止，这种变化为什么会导致这种行为，对我来说还是一个谜。 可能与程序中某个代码位置有关，这点我还不确定。</p><p><strong>Updates</strong></p><p>默认 pgbench 选项运行与 tpcb类似的内置脚本，该脚本对多个表进行了一些更新操作。</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>与 x86相比，ARM 上的事务处理率大约比X86慢1%-10% 。 这可能是因为大部分时间用于等待锁，而在提交过程中的磁盘写操作。 我使用的磁盘是非 SSD 磁盘。 但总体来看，PostgreSQL在 ARM 上的更新表操作在 ARM 上运行良好。</p><p>接下来，我将测试聚合查询、分页、更多CPU核数 (32 / 64 / 128)、更大的 RAM 和更高的规模因数，以便相对地了解 PostgreSQL 在拥有大量资源的两个平台上的性能扩展情况。</p><p><strong>结论</strong></p><p>我们看到，PostgreSQL RDBMS 在 ARM64上工作得相当稳定。 虽然在比较两个不同平台上的性能很棘手，但是我们仍然可以通过比较两个平台中不同场景中的行为来判断它哪些方面做得不好。</p></div><div id="English" class="tab-content"><h3 id="PostgreSQL-on-ARM-1"><a href="#PostgreSQL-on-ARM-1" class="headerlink" title="PostgreSQL on ARM"></a>PostgreSQL on ARM</h3><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p>In my <a href="https://amitdkhan-pg.blogspot.com/2020/04/arm-points-to-be-noted.html">last blog</a>, I wrote that applications that have been running on x86 might need to undergo some adaptation if they are to be run on a different architecture such as ARM. Let’s see what it means exactly.</p><p>Recently I have been playing around with PostgreSQL RDBMS using an ARM64 machine. A few months back, I even didn’t know whether it can be compiled on ARM, being oblivious of the fact that we already have a regular build farm member for ARM64 for <a href="https://buildfarm.postgresql.org/cgi-bin/show_history.pl?nm=eelpout&br=HEAD">quite a while</a>. And now even the PostgreSQL apt repository has started making PostgreSQL packages <a href="https://www.df7cb.de/blog/2020/arm64-on-apt.postgresql.org.html">available for ARM64</a> architecture. But the real confidence on the reliability of PostgreSQL-on-ARM came after I tested it with different kinds of scenarios.</p><p>I started with read-only pgbench tests and compared the results on the x86_64 and the ARM64 VMs available to me. The aim was not to compare any specific CPU implementation. The idea was to find out scenarios where PostgreSQL on ARM does not perform in one scenario as good as it performs in other scenarios, when compared to x86.</p><p><strong>Test Configuration</strong></p><p>ARM64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 2.6 GHz; available RAM : 11GB<br>x86_64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 3.0 GHz; available RAM : 11GB</p><p>Following was common for all tests :</p><p>PostgreSQL parameters changed : shared_buffers = 8GB<br>pgbench scale factor : 30<br>pgbench command :<br>for num in 2 4 6 8 10 12 14<br>do<br>  pgbench [-S] -c $num -j $num -M prepared -T 40<br>done<br>What it means is : pgbench is run with increasing number of parallel clients, starting from 2 to 14.</p><p><strong>Select-only workload</strong></p><p>pgbench -S option is used for read-only workload.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="img"></a></p><p>Between 2 and 4 threads, the x86 performance is 30% more than ARM, and the difference rises more and more. Between 4 and 6, the curves flatten a bit, and between 6 and 8, the curves suddenly become steep. After 8, it was expected to flatten out or dip, because the machines had 8 CPUs. But there is more to it. The pgbench clients were running on the same machines where servers were installed. And with fully utilized CPUs, the clients took around 20% of the CPUs. So they start to interfere from 6 threads onward. In spite of that, there is a steep rise between 6 and 8, for both ARM and x86. This is not yet understood by me, but possibly it has something to do with the Linux scheduler, and the interaction between the pgbench clients and the servers. Note that, the curve shape is mostly similar on both x86 and ARM. So this behaviour is not specific to architectures. One difference in the curves, though, is : the ARM curve has a bit bigger dip from 8 threads onward. Also, betweeen 6 and 8, the sudden jump in transactions is not that steep for ARM compared to x86. So the end result in this scenario is : As the CPUs become more and more busy, PostgreSQL on ARM lags behind x86 more and more. Let’s see what happens if we remove the interference created by pgbench clients.</p><p><strong>select exec_query_in_loop(n)</strong></p><p>So, to get rid of the noise occurring because of both client and server on the same machines, I arranged for testing exactly what I intended to test: query performance. For this, pgbench clients can run on different machines, but that might create a different noise: network latency. So instead, I wrote a PostgreSQL <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">C language user-defined function</a> that keeps on executing in a loop the same exact SQL query that is run by this pgbench test. Execute this function using the pgbench custom script. Now, pgbench clients would be mostly idle. Also, this won’t take into account the commit/rollback time, because most of the time will be spent inside the C function.</p><p>pgbench custom script : select exec_query_in_loop(n);<br>where n is the number of times the pgbench query will be executed on the server in a loop.<br>The loop query is the query that gets normally executed with pgbench -S option:<br>SELECT abalance FROM pgbench_accounts WHERE aid = $1<br>Check details in <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">exec_query_in_loop</a>()</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Now, you see a very different curve. For both curves, upto 8 threads, transactions rate is linearly proportional to number of threads. After 8, as expected, the transactions rate doesn’t rise. And it has not dipped, even for ARM. PostgreSQL is consistently around 35% slower on x86 compared to ARM. This sounds not that bad when we consider that the ARM CPU frequency is 2.6 GHz whereas x86 is 3.0 Gz. Note that the transaction rate is single digit, because the function exec_query_in_loop(n) is executed with n=100000.</p><p>This experiment also shows that the previous results using built-in pgbench script have to do with pgbench client interference. And that, the dip in curve for ARM for contended threads is not caused by the contention in the server. Note that, the transactions rates are calculated at client side. So even when a query is ready for the results, there may be some delay in the client requesting the results , calculating the timestamp, etc, especially in high contention scenarios.</p><p><strong>select exec_query_in_loop(n) - PLpgSQL function</strong></p><p>Before using the user-defined C function, I had earlier used a <a href="https://drive.google.com/file/d/1-jiYEOIVHdtp8Yfv6QrYhrO06urHILd5/view?usp=sharing">PL/pgSQL function to do the same work</a>. There, I stumbled across a different kind of performance behaviour.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Here, PostgreSQL on ARM is around 65% slower than on x86, regardless of number of threads. Comparing with the previous results that used a C function, it is clear that PL/pgSQL execution is remarkably slower on ARM, for some reason. I checked the perf report, but more or less the same hotspot functions are seen in both ARM and x86. But for some reason, anything executed inside PL/pgSQL function becomes much slower on ARM than on x86.</p><p>I am yet to check the cache misses to see if those are more on ARM. As of this writing, what I did was this (some PostgreSQL-internals here) : exec_stmt_foreach_a() calls exec_stmt(). I cloned exec_stmt() to exec_stmt_clone(), and made exec_stmt_foreach_a() call exec_stmt_clone() instead. This sped up the overall execution, but it sped up 20% more for ARM. Why just this change caused this behaviour is kind of a mystery to me as of now. May be it has to do with the location of a function in the program; not sure.</p><p><strong>Updates</strong></p><p>The default pgbench option runs the tpcb-like built-in script, which has some updates on multiple tables.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Here, the transaction rate is only around 1-10% percent less on ARM compared to x86. This is probably because major portion of the time goes in waiting for locks, and in disk writes during commits. And the disks I used are non-SSD disks. But overall it looks like, updates on PostgreSQL are working good on ARM.</p><p>Next thing, I am going to test with aggregate queries, partitions, high number of CPUs (32/64/128), larger RAM and higher scale factor, to relatively see how PostgreSQL scales on the two platforms with large resources.</p><p><strong>Conclusion</strong></p><p>We saw that PostgreSQL RDBMS works quite robustly on ARM64. While it is tricky to compare the performance on two different platforms, we could still identify which areas it is not doing good by comparing patterns of behaviour in different scenarios in the two platforms.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由团队内部PostgreSQL大牛Amit在ARM和X86上针对PostgreSQL的性能比拼测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让压缩库ZSTD在aarch64更顺滑</title>
    <link href="https://kunpengcompute.github.io/2020/05/20/rang-ya-suo-ku-zstd-zai-aarch64-geng-shun-hua/"/>
    <id>https://kunpengcompute.github.io/2020/05/20/rang-ya-suo-ku-zstd-zai-aarch64-geng-shun-hua/</id>
    <published>2020-05-20T08:46:05.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>作者: 姜逸坤 曹亚珍</p><p>Facebook的ZSTD压缩库从1.0版本发布的那天起，就引起了业界的关注，对比业界常用的压缩库lz4、zilib、xz，ZSTD更注重速度和压缩比的均衡，对比zlib来看，更是在保证压缩比的情况下，较zlib压缩性能提升6倍左右，解压性能提升2倍左右。</p><p>我们团队也在2020年年初时，对ZSTD压缩库进行了性能优化，最终优化已推入到Facebook的上游社区中，本文将详细的介绍我们进行的优化。</p><a id="more"></a><h2 id="1-利用neon指令集对数据复制优化。"><a href="#1-利用neon指令集对数据复制优化。" class="headerlink" title="1. 利用neon指令集对数据复制优化。"></a>1. 利用neon指令集对数据复制优化。</h2><p>完整的Patch链接：<a href="https://github.com/facebook/zstd/pull/2041">facebook/zstd#2041</a></p><h3 id="优化思路："><a href="#优化思路：" class="headerlink" title="优化思路："></a>优化思路：</h3><p>aarch64提供了一系列的neon指令，本次优化则利用了VLD和VST指令，借助neon寄存器进行读写加速，<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0489c/CJAJIIGG.html">ARM的官方文档</a>是这样描述这两个指令的：</p><blockquote><p>VLDn and VSTn (single n-element structure to one lane)</p><ul><li>Vector Load single n-element structure to one lane. It loads one n-element structure from memory into one or more NEON registers. Elements of the register that are not loaded are unaltered.</li><li>Vector Store single n-element structure to one lane. It stores one n-element structure into memory from one or more NEON registers.</li></ul></blockquote><p>来自ARM的官方文档<a href="https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/coding-for-neon---part-1-load-and-stores">Coding for Neon - Part 1: Load and Stores</a>中，写的非常详细，引用一张图来描述neon寄存器和memory加载和存储的方式，核心思想就是：<strong>利用neon寄存器作为暂存的中转站，加速数据处理</strong>：<br><img src="https://user-images.githubusercontent.com/1736354/82516998-bc78a900-9b4e-11ea-8183-1200678566e8.png" alt="image"></p><p>我们以u8的复制为例，总结下本次我们在ZSTD具体的优化实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ZSTD_copy8</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __aarch64__</span></span><br><span class="line">    vst1_u8((<span class="keyword">uint8_t</span>*)dst, vld1_u8((<span class="keyword">const</span> <span class="keyword">uint8_t</span>*)src));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">memcpy</span>(dst, src, <span class="number">8</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心步骤包含两步：</p><ol><li>将src利用vld1加载到neon寄存器。</li><li>使用vst1将neon寄存器的值store到dst的memory中。</li></ol><p>这样便利用neon完成了对u8的memcpy的优化，对于此类优化，有兴趣的可以阅读<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.faqs/ka13544.html">What is the fastest way to copy memory on a Cortex-A8?</a>，了解在Cortext-A8的架构下，如何快速的进行memory copy。</p><h3 id="性能测试："><a href="#性能测试：" class="headerlink" title="性能测试："></a>性能测试：</h3><p>完成neon优化后，我们对压缩和解压缩都进行了测试，最终，在压缩场景获得了大概1+%的提升：</p><table><thead><tr><th>Average gains(level 1~19)</th><th>gcc9.2.0</th><th>clang9.0.0</th></tr></thead><tbody><tr><td>Compression</td><td>1.67%</td><td>1.23%</td></tr><tr><td>Decompression</td><td>0.02%</td><td>0.36%</td></tr></tbody></table><h2 id="2-使用prefetch机制加速数据读取。"><a href="#2-使用prefetch机制加速数据读取。" class="headerlink" title="2. 使用prefetch机制加速数据读取。"></a>2. 使用prefetch机制加速数据读取。</h2><p>完整的Patch链接：<a href="https://github.com/facebook/zstd/pull/2040">facebook/zstd#2040</a></p><h3 id="优化思路：-1"><a href="#优化思路：-1" class="headerlink" title="优化思路："></a>优化思路：</h3><p>Prefetch的中文是预取，原理是通过将数据预取到cache中，加速数据的访问。一个比较常见的场景就是在循环中，我们可以通过显示的调用，充分的预取未来将会访问的数据或指令便能快速从Cache中加载到处理器内部进行运算或者执行。</p><p>在Jeff Dean的一次经典的talk–<a href="http://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf">Software Engineering Advice from<br>Building Large-Scale Distributed Systems</a>中，提到了cache和memory的速度差异，大致如下图所示：<br><img src="https://user-images.githubusercontent.com/1736354/82518863-dddb9400-9b52-11ea-937c-55766b53f3a1.png" alt="image"></p><p>可以看到，从cache中拿数据，将比直接从memory拿数据性能提升几十甚至上百倍，因此，我们也在本次的优化中，为aarch64加入的<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0802b/PRFM_imm.html">预取指令</a>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PREFETCH_L1(ptr)  __asm__ __volatile__(<span class="meta-string">"prfm pldl1keep, %0"</span> ::<span class="meta-string">"Q"</span>(*(ptr)))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PREFETCH_L2(ptr)  __asm__ __volatile__(<span class="meta-string">"prfm pldl2keep, %0"</span> ::<span class="meta-string">"Q"</span>(*(ptr)))</span></span><br></pre></td></tr></table></figure><p>同时，将预取加速加入到了ZSTD_compressBlock_fast_generic和ZSTD_compressBlock_doubleFast_generic的主循环中，在数据访问前，预先先将数据加载到cache中，从而加速后续访问对数据读取。</p><h3 id="性能测试：-1"><a href="#性能测试：-1" class="headerlink" title="性能测试："></a>性能测试：</h3><p>我们仅对压缩进行了优化，因此，也仅对压缩进行了测试，测试结果可以看出，速度在aarch64架构下获得了1.5-3+%的提升：</p><table><thead><tr><th>Average gains(level 1~19)</th><th>gcc9.2.0</th><th>clang9.0.0</th></tr></thead><tbody><tr><td>level 1~2</td><td>3.10%</td><td>3.69%</td></tr><tr><td>level 3~4</td><td>2.49%</td><td>1.51%</td></tr></tbody></table><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>在Facebook的ZSTD中，我们使用了neon指令集对memcpy的过程进行了加速，同时，也利用了prefetch机制，加速了循环时数据的访问。</p><p>希望本篇文章，能够对大家带来一些性能优化的启发。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: 姜逸坤 曹亚珍&lt;/p&gt;
&lt;p&gt;Facebook的ZSTD压缩库从1.0版本发布的那天起，就引起了业界的关注，对比业界常用的压缩库lz4、zilib、xz，ZSTD更注重速度和压缩比的均衡，对比zlib来看，更是在保证压缩比的情况下，较zlib压缩性能提升6倍左右，解压性能提升2倍左右。&lt;/p&gt;
&lt;p&gt;我们团队也在2020年年初时，对ZSTD压缩库进行了性能优化，最终优化已推入到Facebook的上游社区中，本文将详细的介绍我们进行的优化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Memcached  x86_64 VS arm64 性能对比</title>
    <link href="https://kunpengcompute.github.io/2020/05/20/memcached-x86-64-vs-arm64-xing-neng-dui-bi/"/>
    <id>https://kunpengcompute.github.io/2020/05/20/memcached-x86-64-vs-arm64-xing-neng-dui-bi/</id>
    <published>2020-05-20T03:26:40.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者: <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接:  <a href="https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8">https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8</a></p><p>Tomcat PMC Martin Grigorov带来的另一篇ARM64 VS X86性能对比文章。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>上周，我<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">分享</a>了在 x86_64和 ARM64 CPU 架构上测试 Apache Tomcat 的结果。 在这篇文章中，我将测试 <a href="https://memcached.org/">Memcached</a>。</p><p>什么是 Memcached？</p><p>摘自 <a href="https://en.wikipedia.org/wiki/Memcached">Wikipedia</a>: Memcached 是一个通用的分布式<a href="https://en.wikipedia.org/wiki/Memory_caching">内存缓存</a>系统。 它通常用于加速动态数据库驱动的网站，方法是在 <a href="https://en.wikipedia.org/wiki/Random-access_memory">RAM</a> 中缓存数据和<a href="https://en.wikipedia.org/wiki/Object_(computer_science)">对象</a>，以减少必须读取外部数据源(如数据库或 API)的次数。</p><p>与 Apache Tomcat 不同的是，Apache Tomcat 是用 Java 编写的，因此是多平台通用的。 而Memcached 是用 c 编写的，需要为不同的 CPU 体系构建它。 正如在其硬件 Wiki <a href="https://github.com/memcached/memcached/wiki/Hardware">页面</a> ARM64中所说的那样，它是官方支持的体系结构之一，并且有一个 BuildBot <a href="https://build.memcached.org/#/builders/6">构建器</a>来测试所有的代码更改！ 如果您遇到任何问题，只要在项目的<a href="https://github.com/memcached/memcached/issues">问题跟踪工具</a>中报告它！ 项目的维护者 Dormando 会非常友好和积极响应！</p><p>在我第一次尝试为 Memcached 找到一个好的负载测试工具时，我无意中发现了 <a href="https://github.com/RedisLabs/memtier_benchmark">RedisLabs Memtier Benchmark</a> 工具。 在 Apache Tomcat 的<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">文章</a>中提到的同一个 vm 上运行它，结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          985.28          ---          ---     20.02700        67.22 </span><br><span class="line">Gets         9842.00         0.00      9842.00     20.01900       248.83 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10827.28         0.00      9842.00     20.02000       316.05</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          931.04          ---          ---     20.06800        63.52 </span><br><span class="line">Gets         9300.21         0.00      9300.21     20.32600       235.13 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10231.26         0.00      9300.21     20.30200       298.66</span><br></pre></td></tr></table></figure><p>上面我们可以看到，Memcached 服务运行在 ARM64虚拟机会稍微快一点！</p><p><strong>注意</strong>: Memcached 服务器运行的默认设置(最大1024连接，4线程和64M 内存) ，即没有指定自定义值。</p><p>对于二进制协议，数字几乎是一样的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Binary protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.68          ---          ---     23.46500        63.90 </span><br><span class="line">Gets         8287.69         0.00      8287.69     23.56100       314.75 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9117.37         0.00      8287.69     23.55200       378.65</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.32          ---          ---     23.63600        63.87 </span><br><span class="line">Gets         8284.10         0.00      8284.10     23.58600       314.61 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9113.42         0.00      8284.10     23.59100       378.48</span><br></pre></td></tr></table></figure><p>在我与 Memcached 社区<a href="https://groups.google.com/d/msg/memcached/8hT2BT9cgEc/Ldm8Q42xAgAJ">分享</a>了这些结果之后，社区建议我使用 <a href="https://github.com/memcached/mc-crusher">MC Crusher</a> 工具代替。 实际上，结果比之相关的数据要好得多:</p><p><code>ASCII protocol GET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401295-ffbe1380-9a8b-11ea-928f-aa62068b3967.png" alt="image"></p><p><code>ASCII protocol SET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401314-0d739900-9a8c-11ea-9f96-ccb54c081475.png" alt="image"></p><p>在第一个图表中，你可以看到在 x86_64和 ARM64上，每秒大约有150万次 get 操作！</p><p>在第二张图表中，在 ARM64上每秒运行90万次，在 x86_64上每秒运行84万次。</p><p><strong>注意</strong>: 由于 mc-crusher 工具不提供任何统计数据。因而我使用 Memcached 的统计命令，以获得执行的操作的数量。</p><p>下面是用于负载测试的设置:</p><ol><li>服务器的启动方式如下:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ memcached -t 16 -c 256 -m 2048</span><br></pre></td></tr></table></figure><p>即16线程，最多256个并发连接和2Gb 内存。</p><ol start="2"><li><p>MC Crusher<br> a. GET配置</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_get,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,pipelines&#x3D;10</span><br><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;10,key_prefix&#x3D;foobar,pipelines&#x3D;4,stop_after&#x3D;200000,usleep&#x3D;1000,value_size&#x3D;10</span><br></pre></td></tr></table></figure><p> b. SET配置</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,value_size&#x3D;2,value&#x3D;hi,pipelines&#x3D;10</span><br></pre></td></tr></table></figure></li></ol><p>注意: 在 GET 操作的图表中，你可以看到在2020年5月13日，这个数字从每秒950K 次左右上升到每秒160万次左右。 在那一天，我升级了我用作客户机的 VM，也就是我运行负载测试工具(mc-crusher)的地方，因为我注意到在测试运行期间，当客户机本身超载时会出现峰值。</p><p>我们再一次看到，ARM64服务器可以快到和 x86_64一样！</p><p>如果你对如何改善这个Memcached 测试或如何衡量一些其他方面有任何的想法，随意与我分享您的意见！</p><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>Last week I’ve <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">shared</a> with you the results of load testing Apache Tomcat on x86_64 and ARM64 CPU architecture. In this article I will test <a href="https://memcached.org/">Memcached</a>.</p><p>What is Memcached ?</p><p>From <a href="https://en.wikipedia.org/wiki/Memcached">Wikipedia</a>: Memcached is a general-purpose distributed <a href="https://en.wikipedia.org/wiki/Memory_caching">memory-caching</a> system. It is often used to speed up dynamic database-driven websites by caching data and <a href="https://en.wikipedia.org/wiki/Object_(computer_science)">objects</a> in <a href="https://en.wikipedia.org/wiki/Random-access_memory">RAM</a> to reduce the number of times an external data source (such as a database or API) must be read.</p><p>In contrast to Apache Tomcat which is written in Java and thus is multi-platform Memcached is written in C and one needs to build it especially for the your CPU architecture. As stated at its Hardware Wiki <a href="https://github.com/memcached/memcached/wiki/Hardware">page</a> ARM64 is one of the officially supported architectures and there is a BuildBot <a href="https://build.memcached.org/#/builders/6">builder</a> testing all code changes! If you face any issue just report it at the project’s <a href="https://github.com/memcached/memcached/issues">issue tracker</a>! Dormando, the project maintainer, is very friendly and responsive!</p><p>In my first attempt to find a good load testing tool for Memcached I stumbled upon <a href="https://github.com/RedisLabs/memtier_benchmark">RedisLabs Memtier Benchmark</a> tool. Running it on the same VMs as in the <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">article</a> for Apache Tomcat and the results were:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          985.28          ---          ---     20.02700        67.22 </span><br><span class="line">Gets         9842.00         0.00      9842.00     20.01900       248.83 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10827.28         0.00      9842.00     20.02000       316.05</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          931.04          ---          ---     20.06800        63.52 </span><br><span class="line">Gets         9300.21         0.00      9300.21     20.32600       235.13 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10231.26         0.00      9300.21     20.30200       298.66</span><br></pre></td></tr></table></figure><p>Above we see that the Memcached server running on the ARM64 VM was slightly faster!</p><p><strong>Note</strong>: the Memcached server was running with default settings (maximum 1024 connections, 4 threads and 64M memory), i.e. without specifying custom values.</p><p>For binary protocol the numbers are almost the same:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.68          ---          ---     23.46500        63.90 </span><br><span class="line">Gets         8287.69         0.00      8287.69     23.56100       314.75 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9117.37         0.00      8287.69     23.55200       378.65</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.32          ---          ---     23.63600        63.87 </span><br><span class="line">Gets         8284.10         0.00      8284.10     23.58600       314.61 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9113.42         0.00      8284.10     23.59100       378.48</span><br></pre></td></tr></table></figure><p>After <a href="https://groups.google.com/d/msg/memcached/8hT2BT9cgEc/Ldm8Q42xAgAJ">sharing</a> these results with Memcached community it was recommended to me to use <a href="https://github.com/memcached/mc-crusher">MC Crusher</a> tool instead. And indeed the numbers are much better with it:</p><p><code>ASCII protocol GET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401295-ffbe1380-9a8b-11ea-928f-aa62068b3967.png" alt="image"></p><p><code>ASCII protocol SET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401314-0d739900-9a8c-11ea-9f96-ccb54c081475.png" alt="image"></p><p>In the first chart you may see that both on x86_64 and ARM64 it makes around 1.5 million get operations per second!</p><p>On the second chart it makes a little bit more than 900 thousand set operations per second on ARM64 and around 840 thousand ops per second on x86_64.</p><p><strong>Note</strong>: Since mc-crusher tool does not provide any statistics from its execution I used Memcached’s stats command to get the number of executed operations.</p><p>Here are the settings used for the load test:</p><ol><li>The servers are started with:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ memcached -t 16 -c 256 -m 2048</span><br></pre></td></tr></table></figure><p>i.e. with 16 threads, maximum of 256 simultaneous connections and 2Gb memory.</p><ol start="2"><li><p>MC Crusher<br> a. GET config</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_get,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,pipelines&#x3D;10</span><br><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;10,key_prefix&#x3D;foobar,pipelines&#x3D;4,stop_after&#x3D;200000,usleep&#x3D;1000,value_size&#x3D;10</span><br></pre></td></tr></table></figure><p> b. SET config</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,value_size&#x3D;2,value&#x3D;hi,pipelines&#x3D;10</span><br></pre></td></tr></table></figure></li></ol><p>Note: In the chart for the GET operation you see that the number rises at May 13th 2020 from around 950K operations per second to around 1.6 million ops/s. At that day I’ve upgraded the VM that I use as a client, i.e. where I run the load testing tools (mc-crusher) because I’ve noticed that during the test run there were spikes when the client itself was overloaded.</p><p>Once again we saw that ARM64 on the server could be as fast as x86_64!</p><p>If you have ideas how to improve this test or how to measure some other aspect of Memcached feel free to share it with me in the comments!</p><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者: &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接:  &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8&quot;&gt;https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomcat PMC Martin Grigorov带来的另一篇ARM64 VS X86性能对比文章。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>Mysql社区ARM优化汇总</title>
    <link href="https://kunpengcompute.github.io/2020/05/13/mysql-she-qu-arm-you-hua-hui-zong/"/>
    <id>https://kunpengcompute.github.io/2020/05/13/mysql-she-qu-arm-you-hua-hui-zong/</id>
    <published>2020-05-13T03:24:14.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Community-Contributions-So-Far/">https://mysqlonarm.github.io/Community-Contributions-So-Far/</a></p><p>社区拥有来自不同组织的开发人员为 MySQL 提供了一些很好的补丁。但是这些补丁中的大多数都在等待Oracle的接受。 这篇博客的目的是分析这些补丁以及它们的利弊。希望这将有助于 Mysql / Oracle 接受这些期待已久的补丁。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h2 id="社区Patches"><a href="#社区Patches" class="headerlink" title="社区Patches"></a>社区Patches</h2><h3 id="1-校验和优化"><a href="#1-校验和优化" class="headerlink" title="1. 校验和优化"></a>1. 校验和优化</h3><ul><li>Mysql 使用的校验和有两种: crc32c 和 crc32。因为它们使用不同的多项式而导致它们的之间的不同。<ul><li>crc32c 被用来在 MySQL Innodb中计算页面校验和</li><li>crc32在 MySQL 中用于表校验和、 binlog-checksum 等…</li></ul></li></ul><h4 id="crc32c"><a href="#crc32c" class="headerlink" title="crc32c"></a>crc32c</h4><ul><li>页面校验和是在读/写每个页面时进行的，所以crc32c可以在perl报告中快速的展示出来。确保在使用优化版本后，它能够提高整个系统的性能。</li><li>crc32c 通常是由硬件完成的功能，例如在 x86(SSE)和 ARM (ACLE)。 Innodb 目前使用基于硬件的 x86实现，但还没有使用 ARM (ACLE)实现。 必要的补丁修复有助于解决上述问题。</li><li>最新的补丁(bug # 85819)还有助于在使用 crypto (PMULL)处理指令时来进行进一步优化.</li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=79144">bug#79144</a> No hardware CRC32 implementation for AArch64<br><a href="https://bugs.mysql.com/bug.php?id=85819">bug#85819</a> Optimize AARCH64 CRC32c implementation</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs update-non-index and gets the crc32 as top mysqld function].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+   10.43%          8027  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    3.23%          2486  mysqld   mysqld               [.] ut_crc32_sw                                                                                 </span><br><span class="line">+    2.33%          1797  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.73%          1330  mysqld   libc-2.27.so         [.] malloc                                                                                    </span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">  Overhead       Samples  Command  Shared Object        Symbol                                                                                          </span><br><span class="line">+   10.60%          8133  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    2.37%          1816  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.78%          1366  mysqld   libc-2.27.so         [.] malloc                                                                                      </span><br><span class="line">....</span><br><span class="line">     0.44%           338  mysqld   mysqld               [.] ut_crc32_aarch64</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 明显可以考到节省了大约3%的吞吐量。另外，在更加广泛的测试中，我们可以看到crc32有助于提高所有类型测试场景下的吞吐量。</p><h4 id="crc32"><a href="#crc32" class="headerlink" title="crc32"></a>crc32</h4><p>为了计算表校验和，MySQL 使用基于 zlib 的 crc32(软实现)。 据我所知，x86不支持 crc32计算的硬件优化版本，但幸运的是 ARM (ACLE)支持。 同时Binlog-checksum 也使用相同的代码 / 处理流程。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=99118">bug#99118</a> ARM CRC32 intrinsic call to accelerate table-checksum (not crc32c but crc32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs checksum on all tables and update-non-index].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+   49.46%         13480  mysqld   mysqld               [.] crc32_z</span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.40%           311  mysqld   mysqld               [.] crc32_z                                                                                     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+    8.15%           988  mysqld   mysqld               [.] aarch64_crc32_checksum                                                                      </span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.07%            56  mysqld   mysqld               [.] aarch64_crc32_checksum</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 这个补丁在两个方面都有提升。超级加速表校验和(平均提高50%) ，并且在 binlog 校验和中也是。</p><h3 id="2-my-convert-in-turn-copy-and-convert-在ARM平台表现不好"><a href="#2-my-convert-in-turn-copy-and-convert-在ARM平台表现不好" class="headerlink" title="2. my_convert (in turn copy_and_convert) 在ARM平台表现不好:"></a>2. my_convert (in turn copy_and_convert) 在ARM平台表现不好:</h3><ul><li>my_convert用作发送结果的一部分步骤中，主要是用于在字符集之间进行转换。</li><li>给定转换的数据量，这个函数会出现在 perf top-list 中</li><li>现有的实现对于 x86使用4字节的复制，但对于 ARM 则退回到单字节复制。 通过对 x86-64和 aarch64使用8个字节的复制，然后再到现有逻辑的尾部处理过程，可以总体改进这一点。 这个简单的补丁可以帮助节省大量的时钟周期并提高系统性能。</li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=98737">bug#98737</a> my_convert routine is suboptimal in case of non-x86 platforms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs oltp-read-write on all tables].</span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+    0.79%          1114  mysqld   mysqld               [.] my_convert</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">     0.22%           299  mysqld   mysqld               [.] my_convert</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 这个补丁提高了吞吐量。</p><h3 id="3-为原子变量优化内存屏障"><a href="#3-为原子变量优化内存屏障" class="headerlink" title="3. 为原子变量优化内存屏障:"></a>3. 为原子变量优化内存屏障:</h3><ul><li>Mysql / innodb 有很多变量，它使用 gcc 内置的原子函数(sync add and fetch 或 atomic add fetch).</li><li>虽然在x86的强内存模型，它们表现很好，但是大多数这些计数器函数中都是使用顺序内存排序(缺省)来实现的。</li><li>因为Arm 的弱内存模型，因此不推荐使用这种顺序内存排序(缺省的)。</li><li>社区多个补丁来帮助修改这些代码片段。它们有助于实现两个目标:<ul><li>切换到使用 c + + 11原子函数(MySQL现已支持)。</li><li>切换到使用松散的内存顺序(vs 顺序)。</li></ul></li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=97228">bug#97228</a> rwlock: refine lock-&gt;lock_word with C11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97230">bug#97230</a> rwlock: refine lock-&gt;waiters with C++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97703">bug#97703</a> innobase/dict: refine dict_temp_file_num with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97704">bug#97704</a> innobase/srv: refine srv0conc with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97765">bug#97765</a> innobase/os: refine os_total_large_mem_allocated with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97766">bug#97766</a> innobase/os_once: optimize os_once with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97767">bug#97767</a> innobase/dict: refine zip_pad_info-&gt;pad with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=99432">bug#99432</a> Improving memory barrier during rseg allocation</p><p><strong>结论:</strong> 由于它的影响分布非常广，所以很难用perf来进行判断。另外，部分修复仅仅是为了改进语义，而不是为了性能因素。</p><h3 id="4-为全局计数器带来核心亲和性调度"><a href="#4-为全局计数器带来核心亲和性调度" class="headerlink" title="4. 为全局计数器带来核心亲和性调度:"></a>4. 为全局计数器带来核心亲和性调度:</h3><p>Arm 以拥有大量的核心(和 numa 套接字)而闻名，为了从多核中获得最大的吞吐量，确保全局计数器的可编程性是非常重要的。拥有一个分布式计数器并将计数器的递增部分尽量与线程核心靠近，应该可以避免跨numa延迟。</p><p>MySQL通过调用call sched_getcpu来获取计数器插槽，但是这个逻辑由于另一个bug的修复而改变了(这对于上述问题来说当然是有意义的) ， 这个bug修复影响了正常的全局计数器。下面的补丁提议纠正这一点，并使用sched_getcpu(核心亲和性)来实现全局计数器。</p><p><em>不幸的是，在 ARM 上，这个补丁由于使用了 sched_getcpu 而产生了开销，但是在x86上在使用VDSO时进行了优化。</em></p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=79455">bug#79455</a> Restore get_sched_indexer_t in 5.7</p><h3 id="5-当前UT-RELAX-CPU-在ARM平台上的可伸缩性问题"><a href="#5-当前UT-RELAX-CPU-在ARM平台上的可伸缩性问题" class="headerlink" title="5. 当前UT_RELAX_CPU () 在ARM平台上的可伸缩性问题:"></a>5. 当前UT_RELAX_CPU () 在ARM平台上的可伸缩性问题:</h3><p>Innodb 使用自制的 spin-wait 来实现 rw-locks 和 mutexes。 无论何时需要休眠(或让我纠正称它为PAUSE) ，在 x86 MySQL 上支持PAUSE指令。 ARM不支持 PAUSE 指令，因此流程中使用编译器屏障，但是该指令未能引入所需的延迟。 修补程序建议使用Compare-And-Exchange，这应该有助于引入类似的延迟(如PAUSE)。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=87933">bug#87933</a> Scalibility issue on Arm platform with the current UT_RELAX_CPU () code.</p><p>基于内部评估，我们得不到补丁所带来的吞吐量提升，因此目前没有将其纳入我们贡献到社区的内容。</p><h3 id="6-在ARM上应用更宽的cacheline来填充"><a href="#6-在ARM上应用更宽的cacheline来填充" class="headerlink" title="6. 在ARM上应用更宽的cacheline来填充:"></a>6. 在ARM上应用更宽的cacheline来填充:</h3><p>大多数 ARM 处理器计划拥有更宽的cacheline size。 补丁提出基于 ARM 处理器使用更大的cachelilne，并填充以避免false sharing问题。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=98499">bug#98499</a> Improvement about CPU cache line size</p><h3 id="7-其他开源贡献"><a href="#7-其他开源贡献" class="headerlink" title="7. 其他开源贡献"></a>7. 其他开源贡献</h3><p>除了上面列出的6个大类，在其他领域也有很多的贡献。 但是大多数都没有相关的代码提交，或者这个想法已经作为另一个重大改进被放在 MySQL 中(不是针对 ARM 的工作) ，又或者是这个idea不太可能对性能产生影响。</p><h2 id="社区补丁对性能的影响"><a href="#社区补丁对性能的影响" class="headerlink" title="社区补丁对性能的影响"></a>社区补丁对性能的影响</h2><p>基于上面收集的内容，我们分析了引入社区补丁后对性能的影响，下面的表显示了如果我们合入这些补丁，吞吐量将如何提高。 结果限制在较大的可伸缩性(256线程)，因为它显示了主要的影响，我们已经全面运行了测试用例，确定补丁有助于提高总吞吐量(即使对于单线程而言)。</p><table><thead><tr><th></th><th>point select</th><th>read only</th><th>read write</th><th>update index</th><th>update non index</th></tr></thead><tbody><tr><td>without-patch</td><td>218447</td><td>145755</td><td>5646</td><td>22200</td><td>22601</td></tr><tr><td>with-patch</td><td>224355</td><td>149718</td><td>5829</td><td>23070</td><td>23292</td></tr><tr><td>%</td><td>2.7</td><td>2.72</td><td>3.24</td><td>3.92</td><td>3.06</td></tr></tbody></table><p><em>使用 mysql-8.0.20进行评估。配置参看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>. 处理器: ARM Kunpeng 920 24vCPU/48GB</em></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>社区提供的补丁确实有助于优化 ARM 上的 MySQL，但影响程度有限，并且需要覆盖很多领域才能看到 MySQL 在 ARM 上加速的程度。 如果你有好的想法，请联系我或者直接在社区进行交流。 ARM MySQL 社区可以尽情针对这一问题爆发头脑风暴，并尝试实现修复这个问题的idea。</p><p><em>如果你有问题 / 疑问，请让我知道，我会尽力回答。</em></p></div><div id="English" class="tab-content"><h2 id="Community-Patches"><a href="#Community-Patches" class="headerlink" title="Community Patches"></a>Community Patches</h2><h3 id="1-Optimizing-checksum"><a href="#1-Optimizing-checksum" class="headerlink" title="1. Optimizing checksum"></a>1. Optimizing checksum</h3><ul><li>MySQL uses 2 types of checksum: crc32c and crc32. They both are different since both uses different polynomials.<ul><li>crc32c is used in MySQL by InnoDB to calculate page-checksum.</li><li>crc32 is used in MySQL for table checksum, binlog-checksum, etc…</li></ul></li></ul><h4 id="crc32c-1"><a href="#crc32c-1" class="headerlink" title="crc32c"></a>crc32c</h4><ul><li>Page checksum is calculated during each page read/write so crc32c can quickly show up as one of the top functions in perf report. Ensuring use of optimized versions of it could help improve the overall throughput of the system.</li><li>crc32c has been implemented as a hardware function on both x86 (SSE) and ARM (ACLE). InnoDB currently uses hardware based implementation for x86 but not yet for ARM (ACLE). Patch helps address the said issue.</li><li>Latest patch (bug#85819) also helps further optimize it using crypto (PMULL) processing instruction.</li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=79144" title="suggest use of crc32c">bug#79144</a> No hardware CRC32 implementation for AArch64<br><a href="https://bugs.mysql.com/bug.php?id=85819">bug#85819</a> Optimize AARCH64 CRC32c implementation</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs update-non-index and gets the crc32 as top mysqld function].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+   10.43%          8027  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    3.23%          2486  mysqld   mysqld               [.] ut_crc32_sw                                                                                 </span><br><span class="line">+    2.33%          1797  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.73%          1330  mysqld   libc-2.27.so         [.] malloc                                                                                    </span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">  Overhead       Samples  Command  Shared Object        Symbol                                                                                          </span><br><span class="line">+   10.60%          8133  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    2.37%          1816  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.78%          1366  mysqld   libc-2.27.so         [.] malloc                                                                                      </span><br><span class="line">....</span><br><span class="line">     0.44%           338  mysqld   mysqld               [.] ut_crc32_aarch64</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> Clearly a saving of around 3% in overall throughput can be seen. Also, as part of wider testing we see crc32c helps in overall throughput gain for all kind of test-cases.</p><h4 id="crc32-1"><a href="#crc32-1" class="headerlink" title="crc32"></a>crc32</h4><p>For calculating table checksum MySQL uses zlib-based crc32. As per my knowledge, x86 doesn’t support hardware optimized versions for crc32 calculation but fortunately ARM (ACLE) supports it. The same code/flow path is also used for binlog-checksum.</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=99118">bug#99118</a> ARM CRC32 intrinsic call to accelerate table-checksum (not crc32c but crc32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs checksum on all tables and update-non-index].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+   49.46%         13480  mysqld   mysqld               [.] crc32_z</span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.40%           311  mysqld   mysqld               [.] crc32_z                                                                                     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+    8.15%           988  mysqld   mysqld               [.] aarch64_crc32_checksum                                                                      </span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.07%            56  mysqld   mysqld               [.] aarch64_crc32_checksum</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> This patch helps on both front. Super-accelerate table checksum (average improvement of 50%) and also marginally helps in binlog-checksum.</p><h3 id="2-my-convert-in-turn-copy-and-convert-routine-is-suboptimal-for-ARM"><a href="#2-my-convert-in-turn-copy-and-convert-routine-is-suboptimal-for-ARM" class="headerlink" title="2. my_convert (in turn copy_and_convert) routine is suboptimal for ARM:"></a>2. my_convert (in turn copy_and_convert) routine is suboptimal for ARM:</h3><ul><li>my_convert is used as part of the send result for converting among charsets.</li><li>Given the amount of the data that is converted this function gets spotted in perf top-list.</li><li>Existing implementation uses 4 bytes copying for x86 but falls back to byte copy for ARM. This could be overall improved by using 8 bytes copying for x86-64 and aarch64 and then falling back for trailing things to existing logic. Patch for this simple operation help save significant cycles and help improve performance.</li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=98737">bug#98737</a> my_convert routine is suboptimal in case of non-x86 platforms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs oltp-read-write on all tables].</span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+    0.79%          1114  mysqld   mysqld               [.] my_convert</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">     0.22%           299  mysqld   mysqld               [.] my_convert</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> Patch can help improve overall throuhgput.</p><h3 id="3-Improving-memory-barrier-for-atomic-variables"><a href="#3-Improving-memory-barrier-for-atomic-variables" class="headerlink" title="3. Improving memory barrier for atomic variables:"></a>3. Improving memory barrier for atomic variables:</h3><ul><li>MySQL/InnoDB has a lot of variables for which it uses gcc inbuilt atomic functions (__sync_add_and_fetch or __atomic_add_fetch).</li><li>While this is all good x86 being a strong memory model most of these counter functions were implemented to use sequential memory ordering (default).</li><li>ARM has a relaxed memory model so using sequential memory ordering (default one) is not recommended.</li><li>Multiple patches were submitted to help revamp the said snippets. Patches help achieve 2 things:<ul><li>Switch to use C++11 atomics. (Now that MySQL supports it).</li><li>Switch to use relaxed memory order (vs sequential).</li></ul></li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=97228">bug#97228</a> rwlock: refine lock-&gt;lock_word with C11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97230">bug#97230</a> rwlock: refine lock-&gt;waiters with C++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97703">bug#97703</a> innobase/dict: refine dict_temp_file_num with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97704">bug#97704</a> innobase/srv: refine srv0conc with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97765">bug#97765</a> innobase/os: refine os_total_large_mem_allocated with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97766">bug#97766</a> innobase/os_once: optimize os_once with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97767">bug#97767</a> innobase/dict: refine zip_pad_info-&gt;pad with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=99432">bug#99432</a> Improving memory barrier during rseg allocation</p><p><strong>Conclusion:</strong> Impact is wide spread so difficult to judge using perf. Also, some of the fixes help improve semantics and may not be for performance reason as such.</p><h3 id="4-Restore-core-affinity-scheduler-for-global-counter"><a href="#4-Restore-core-affinity-scheduler-for-global-counter" class="headerlink" title="4. Restore core affinity scheduler for global counter:"></a>4. Restore core affinity scheduler for global counter:</h3><p>ARM is known for its large number of cores (and numa sockets) and to harvest the max throughput from multi-cores it is important to ensure that global counters are programmed accordingly. Having a distributed counter and incrementing part of the counter closer to the thread core should avoid cross-numa latency.</p><p>MySQL use to call sched_getcpu for getting the counter slots but this logic was changed as part of the different bug fix (that surely made sense for the said issue) but it also affected the normal global counters. Patch proposes to correct this and use sched_getcpu (core affinity) based counter for global counters.</p><p><em>On ARM this patch unfortunately is running into overhead resulting from use of sched_getcpu which is optimized on x86 using VDSO.</em></p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=79455">bug#79455</a> Restore get_sched_indexer_t in 5.7</p><h3 id="5-Scalability-issue-on-ARM-platform-with-the-current-UT-RELAX-CPU-code"><a href="#5-Scalability-issue-on-ARM-platform-with-the-current-UT-RELAX-CPU-code" class="headerlink" title="5. Scalability issue on ARM platform with the current UT_RELAX_CPU () code:"></a>5. Scalability issue on ARM platform with the current UT_RELAX_CPU () code:</h3><p>InnoDB uses home-grown spin-wait implementation for rw-locks and mutexes. Whenever there is a need to sleep (or let me correctly say PAUSE) then on x86 MySQL uses supported PAUSE instruction. ARM doesn’t have support for PAUSE instruction so the flow uses a compiler barrier but this statement fails to introduce the needed delay. Patch suggest use of Compare-And-Exchange that should help introduce comparable delay (like PAUSE).</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=87933">bug#87933</a> Scalibility issue on Arm platform with the current UT_RELAX_CPU () code.</p><p>Based on internal evaluation we couldn’t get the patch to help improve on throughput so have not-considered it as part of our community-patch branch for now.</p><h3 id="6-Using-wider-cacheline-padding-for-ARM"><a href="#6-Using-wider-cacheline-padding-for-ARM" class="headerlink" title="6. Using wider cacheline padding for ARM:"></a>6. Using wider cacheline padding for ARM:</h3><p>Most of the ARM processors are scheduled to have a wider cache line. Patch proposes use of a wider cache line padding for ARM based processors to avoid false sharing.</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=98499">bug#98499</a> Improvement about CPU cache line size</p><h3 id="7-Other-open-contributions"><a href="#7-Other-open-contributions" class="headerlink" title="7. Other open contributions"></a>7. Other open contributions</h3><p>Besides the 6 main categories listed above there are more contributions in other areas too. But most of them didn’t have a patch associated or the said idea has been folded in MySQL as part of another major revamp (not specific to ARM work) or the idea is less likely to have a performance impact. So for now we were not able to consider these set of patches.</p><h2 id="Performance-impact-of-Community-Patches"><a href="#Performance-impact-of-Community-Patches" class="headerlink" title="Performance impact of Community Patches"></a>Performance impact of Community Patches</h2><p>Based on the inputs collected above we have analyzed performance impact of community patches and below table help shows how the throughput would improve if the said patches are accepted. Limiting results for higher scalability (256 threads) where it shows major effect but we have run test-case across the board and patches helps improve overall throughput (even for single threaded).</p><table><thead><tr><th></th><th>point select</th><th>read only</th><th>read write</th><th>update index</th><th>update non index</th></tr></thead><tbody><tr><td>without-patch</td><td>218447</td><td>145755</td><td>5646</td><td>22200</td><td>22601</td></tr><tr><td>with-patch</td><td>224355</td><td>149718</td><td>5829</td><td>23070</td><td>23292</td></tr><tr><td>%</td><td>2.7</td><td>2.72</td><td>3.24</td><td>3.92</td><td>3.06</td></tr></tbody></table><p><em>Evaluated using mysql-8.0.20. For configuration check <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>. Processor: ARM Kunpeng 920 24vCPU/48GB</em></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Patches contributed by community surely helps in optimizing MySQL on ARM but the impact is still limited and lot of ground to cover to make MySQL accelerate on ARM. If you have good ideas on how things could be pushed further then let’s connect. ARM MySQL community can help brainstorm the idea and aid/help in materializing it to a contribution.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Community-Contributions-So-Far/&quot;&gt;https://mysqlonarm.github.io/Community-Contributions-So-Far/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;社区拥有来自不同组织的开发人员为 MySQL 提供了一些很好的补丁。但是这些补丁中的大多数都在等待Oracle的接受。 这篇博客的目的是分析这些补丁以及它们的利弊。希望这将有助于 Mysql / Oracle 接受这些期待已久的补丁。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Apache Tomcat X86 vs ARM64 性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/05/13/apache-tomcat-x86-vs-arm64-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/05/13/apache-tomcat-x86-vs-arm64-xing-neng-bi-pin/</id>
    <published>2020-05-13T01:28:58.000Z</published>
    <updated>2020-09-11T01:53:43.713Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者:  <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接:  <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6</a></p><p>Tomcat PMC Martin Grigorov带来的Tomcat X86 VS ARM64性能测试。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>大多数软件开发人员通常不会考虑他们的软件将在何种 CPU 架构上运行。 尽管没有官方的统计数据，但根据我的经验，大多数桌面和后端应用软件都运行在 x86_64架构(英特尔和 AMD 处理器)上，大多数移动和物联网设备都运行在 ARM 架构上。 开发人员使用一些高级编程语言为各自的 CPU 架构编写软件，并不考虑在运行时执行何种汇编指令。 而这正是高级编程语言的目的—- 让编译器处理低级硬件指令，并简化我们的任务，使其只专注于高级业务相关问题。</p><p>生活简单而美好，但有时候，笔记本电脑和台式机硬件及软件制造业的<a href="https://www.apple.com/">巨头</a>会说，我们的软件必须在不同的架构上运行——先是从 <a href="https://en.wikipedia.org/wiki/Apple%27s_transition_to_Intel_processors#Timeline">PowerPC 到英特尔</a>，现在从英特尔到 ARM64(消息来源: <a href="https://www.bloomberg.com/news/articles/2020-04-23/apple-aims-to-sell-macs-with-its-own-chips-starting-in-2021">Bloomberg</a> &amp; <a href="https://appleinsider.com/articles/20/02/25/why-apple-will-move-macs-to-arm-and-what-consumers-get">AppleInsider</a>)。 由于电力消耗较低，甚至一些较大的云供应商也开始提供 ARM64虚拟机(如<a href="https://aws.amazon.com/ec2/graviton/">亚马逊 AWS</a>、<a href="https://www.huaweicloud.com/en-us/product/ecs.html">华为云</a>、 <a href="https://www.linaro.cloud/">Linaro</a>)。 但还有以下不确定性:</p><ul><li>我的软件能在新的 CPU 架构上运行吗?</li><li>我需要做出什么样的改变才能让它发挥作用</li><li>它会像以前一样表现出色吗</li></ul><p>为了能够回答这些问题，你必须撸起袖子进行测试！</p><p>您可以在任何云供应商上部署软件。 有些还提供免费试用期！ 或者如果你的预算很少，你可以试试 <a href="https://www.raspberrypi.org/">RaspberryPi</a>。</p><p>根据您编写软件所使用的编程语言，您可能需要进行一些更改，或者根本不需要更改！ 如果你使用一个直译语言文件(例如 Python，Perl，Ruby，JVM，…) ，那么解释器已经支持 ARM64的可能性相当高，你可以不做任何改变就继续使用它！ 但是，如果你的软件需要被编译，那么你需要调整你的工具链，并确保有 ARM64二进制文件为你所有的依赖！ 根据您的软件开发堆栈，您的修改量可能会有所不同！</p><p>一旦我们的软件在新架构上运行良好，我们将能够检查它是否像以前那样执行良好。 最近一些用户在 Apache Tomcat 邮件列表中询问是否支持 ARM64架构。 因为 Apache Tomcat 大部分代码是用 Java 编写的，所以它可以基本的运行在ARM64上。 如果您需要使用 <code>libtcnative</code> 和 / 或 <code>mod_jk</code>，那么您需要自己在 ARM64上构建它们。 Apache Tomcat 团队使用 TravisCI 在 ARM64上测试 <a href="https://travis-ci.org/github/apache/tomcat">Java</a> 和 <a href="https://travis-ci.org/github/martin-g/tomcat-connectors">C</a> 代码，目前还没有已知的问题！</p><p>为了比较某些软件的两个版本的性能，通常您将在同一个硬件上运行测试，但在这种情况下，由于我们使用不同的 CPU 架构，这是不可能的。 在我的测试中，我使用了两个具有类似规范的 vm：</p><ul><li><p>X86_64处理器是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">Core(s) per socket:  4</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           GenuineIntel</span><br><span class="line">CPU family:          6</span><br><span class="line">Model:               85</span><br><span class="line">Model name:          Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:            7</span><br><span class="line">CPU MHz:             3000.000</span><br><span class="line">BogoMIPS:            6000.00</span><br><span class="line">Hypervisor vendor:   KVM</span><br><span class="line">Virtualization type: full</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            1024K</span><br><span class="line">L3 cache:            30976K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure></li><li><p>Arm64处理器是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        aarch64</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  1</span><br><span class="line">Core(s) per socket:  8</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           0x48</span><br><span class="line">Model:               0</span><br><span class="line">Stepping:            0x1</span><br><span class="line">BogoMIPS:            200.00</span><br><span class="line">L1d cache:           64K</span><br><span class="line">L1i cache:           64K</span><br><span class="line">L2 cache:            512K</span><br><span class="line">L3 cache:            32768K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure></li></ul><p>两个虚拟机具有相同数量的 RAM、磁盘和网络连接。</p><p>测试应用程序基于 Spring Boot (2.2.7) ，运行嵌入式 Apache Tomcat 9.0.x + OpenSSL 1.1.1h-dev 和 Apache Apr 1.7.x。 每晚构建，并且有一个单独的 REST 客户端，该客户端公开一个用于创建实体的 PUT Endpoint、一个用于读取它的 GET Endpoint、一个用于更新它的 POST Endpoint和一个用于删除它的 DELETE Endpoint。 它使用 <a href="https://memcached.org/">Memcached</a> 作为数据库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">package info.mgsolutions.testbed.rest;</span><br><span class="line"></span><br><span class="line">import info.mgsolutions.testbed.domain.Error;</span><br><span class="line">import info.mgsolutions.testbed.domain.Person;</span><br><span class="line">import info.mgsolutions.testbed.domain.Response;</span><br><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line">import net.rubyeye.xmemcached.MemcachedClient;</span><br><span class="line">import net.rubyeye.xmemcached.exception.MemcachedException;</span><br><span class="line">import net.rubyeye.xmemcached.transcoders.SerializingTranscoder;</span><br><span class="line">import org.springframework.http.HttpStatus;</span><br><span class="line">import org.springframework.http.MediaType;</span><br><span class="line">import org.springframework.http.ResponseEntity;</span><br><span class="line">import org.springframework.http.server.ServletServerHttpRequest;</span><br><span class="line">import org.springframework.web.bind.annotation.DeleteMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PutMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line">import org.springframework.web.bind.annotation.RestController;</span><br><span class="line">import org.springframework.web.util.UriComponents;</span><br><span class="line">import org.springframework.web.util.UriComponentsBuilder;</span><br><span class="line"></span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line">import java.util.Base64;</span><br><span class="line">import java.util.concurrent.TimeoutException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * A REST endpoint that uses Memcached to get its data.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@RestController</span><br><span class="line">@RequestMapping(&quot;testbed&#x2F;memcached&quot;)</span><br><span class="line">@Slf4j</span><br><span class="line">public class MemcachedTestController &#123;</span><br><span class="line"></span><br><span class="line">public static final int TTL_IN_SECONDS &#x3D; 1000;</span><br><span class="line"></span><br><span class="line">private final SerializingTranscoder coder &#x3D; new SerializingTranscoder();</span><br><span class="line">private final MemcachedClient client;</span><br><span class="line"></span><br><span class="line">public MemcachedTestController(MemcachedClient client) &#123;</span><br><span class="line">this.client &#x3D; client;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PutMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Response&gt; create(@RequestBody Person person,</span><br><span class="line">                                       HttpServletRequest servletRequest) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(person.name);</span><br><span class="line">Person existing &#x3D; client.get(base64Name);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Create: Person with name &#123;&#125; already exists!&quot;, person.name);</span><br><span class="line">Error error &#x3D; new Error(&quot;Person with name &quot; + person.name + &quot; already exists!&quot;);</span><br><span class="line">return ResponseEntity.status(HttpStatus.NOT_ACCEPTABLE)</span><br><span class="line">                     .body(error);</span><br><span class="line">&#125;</span><br><span class="line">log.info(&quot;Create: Going to create &#39;&#123;&#125;&#39;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line"></span><br><span class="line">ServletServerHttpRequest request &#x3D; new ServletServerHttpRequest(servletRequest);</span><br><span class="line">final UriComponents uriComponents &#x3D; UriComponentsBuilder.fromHttpRequest(request).build();</span><br><span class="line">final URI uri &#x3D; uriComponents.encode(StandardCharsets.UTF_8).toUri();</span><br><span class="line">return ResponseEntity.created(uri).contentType(MediaType.APPLICATION_JSON).body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@GetMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.ALL_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; get(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">Person person &#x3D; (Person) client.get(base64(name), coder);</span><br><span class="line">if (person &#x3D;&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Get: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Get: Found person with name: &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.ok().body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PostMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; update(@RequestBody Person person) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line">final String name &#x3D; person.name;</span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person existing &#x3D; (Person) client.get(base64Name, coder);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Update: Going to update: &#123;&#125;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Update: Cannot find a person with name &#123;&#125;!&quot;, person.name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@DeleteMapping(value &#x3D;&quot;&quot;, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; delete(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person person &#x3D; client.get(base64Name);</span><br><span class="line">if (person !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Delete: Going to delete: &#123;&#125;&quot;, person);</span><br><span class="line">client.delete(base64Name);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Delete: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private String base64(String name) &#123;</span><br><span class="line">return Base64.getEncoder().encodeToString(name.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于负载测试，我使用了 <a href="https://jmeter.apache.org/">Apache JMeter 5.2.1</a>和基于主干代码的 <a href="https://github.com/wg/wrk">wrk</a>。 Jmeter 用于一个真实的情况场景，有1000个并发用户，在 HTTP 请求之间有一个适应期和一段思考时间。 然后用 wrk 测试最大吞吐量。</p><p>使用以下参数执行 JMeter:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">jmeter.sh \</span><br><span class="line">--testfile JMeter_plan.jmx \</span><br><span class="line">--logfile $RESULTS_FILE \</span><br><span class="line">--reportoutputfolder $RESULTS_FOLDER \</span><br><span class="line">--reportatendofloadtests \</span><br><span class="line">--nongui \</span><br><span class="line">--forceDeleteResultFile \</span><br><span class="line">--jmeterproperty httpclient4.validate_after_inactivity&#x3D;4900 \</span><br><span class="line">--jmeterproperty httpclient4.time_to_live&#x3D;120000 \</span><br><span class="line">-Jhost&#x3D;$JMETER_HOST \</span><br><span class="line">-Jport&#x3D;$JMETER_PORT \</span><br><span class="line">-Jprotocol&#x3D;$JMETER_PROTOCOL \</span><br><span class="line">-JresourceFolder&#x3D;$JMETER_RESOURCE_FOLDER \</span><br><span class="line">-Jusers&#x3D;1000 \</span><br><span class="line">-JrampUpSecs&#x3D;5 \</span><br><span class="line">-Jloops&#x3D;10 \</span><br><span class="line">-JrequestPath&#x3D;&#x2F;testbed&#x2F;memcached</span><br></pre></td></tr></table></figure><p>重用 HTTPS 连接需要 <code>httpclient4. * *</code> 属性，否则 Keep-Alive 不会有效。</p><p>Jmeter 和 wrk 的结果与存储在 Elasticsearch 的 Logstash 一起解析，并由 Kibana 进行可视化。</p><p>Jmeter 的响应时间:<br><img src="https://user-images.githubusercontent.com/10891919/81760860-f6b4cb80-94fa-11ea-9d31-27fb43687cc5.png" alt="image"></p><p>正如你所看到的，在5月8日之前 HTTPS 的结果并不是很好。 没有重用 HTTPS 连接，每个请求都进行了 TLS 握手，尽管请求头“ Connection: keep-alive”。 因为 wrk 没有这样的问题，我在 JMeter 邮件列表中询问过，他们给了我上面提到的 httpclient4参数。 (谢谢你，菲利普 · 穆瓦德!) . 不管有没有 HttpClient 的调整，我们看到 x8664和 arm64的响应时间非常相似。 太棒了！</p><p>对于 wrk 的吞吐量测试，我使用以下参数运行它:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wrk -c96 -t8 -d30s -s &#x2F;scripts&#x2F;wrk-report-to-csv.lua $HOST:$PORT</span><br></pre></td></tr></table></figure><p>例如，8个线程将使用96个 HTTP (s)连接访问服务器30秒。</p><p>为了收集 CSV 文件中的摘要，我使用了这个自定义 Lua 脚本:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">-- Initialize the pseudo random number generator</span><br><span class="line">-- Resource: http:&#x2F;&#x2F;lua-users.org&#x2F;wiki&#x2F;MathLibraryTutorial</span><br><span class="line">math.randomseed(os.time())</span><br><span class="line">math.random(); math.random(); math.random()</span><br><span class="line"></span><br><span class="line">local _request &#x3D; &#123;&#125;</span><br><span class="line">method &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line">-- Load URL config from the file</span><br><span class="line">function load_request_objects_from_file(csvFile)</span><br><span class="line">  local data &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for line in io.lines(csvFile) do</span><br><span class="line">    local idx &#x3D; string.find(line, &quot;:&quot;)</span><br><span class="line">    local key &#x3D; string.sub(line, 0, idx-1)</span><br><span class="line">    local value &#x3D; string.sub(line, idx+1, string.len(line))</span><br><span class="line">    data[key] &#x3D; value</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  return data</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function trim(s)</span><br><span class="line">  return s:match &quot;^%s*(.-)%s*$&quot;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function readMethod()</span><br><span class="line">  local method &#x3D; &#39;&#39;</span><br><span class="line">  local f &#x3D; io.open(&#39;&#x2F;data&#x2F;method.txt&#39;,&quot;r&quot;)</span><br><span class="line">  if f ~&#x3D; nil then</span><br><span class="line">    method &#x3D; f:read(&quot;*all&quot;)</span><br><span class="line">    io.close(f)</span><br><span class="line">    return trim(method)</span><br><span class="line">  else</span><br><span class="line">    print(&#39;Cannot read the method name from &#x2F;data&#x2F;method.txt&#39;)</span><br><span class="line">    os.exit(123)</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function init(args)</span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  -- Load request config from file</span><br><span class="line">  _request &#x3D; load_request_objects_from_file(&quot;&#x2F;data&#x2F;&quot; .. method ..&quot;.conf&quot;)</span><br><span class="line"></span><br><span class="line">  --for i, val in pairs(_request) do</span><br><span class="line">  --  print(&#39;Request:\t&#39;, i, val)</span><br><span class="line">  --end</span><br><span class="line"></span><br><span class="line">  if _request[method] ~&#x3D; nil then</span><br><span class="line">    print(&quot;multiplerequests: No requests found.&quot;)</span><br><span class="line">    os.exit()</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  print(&quot;multiplerequests: Found a &quot; .. _request.method .. &quot; request&quot;)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">request &#x3D; function()</span><br><span class="line">  local request_object &#x3D; _request</span><br><span class="line"></span><br><span class="line">  -- Return the request object with the current URL path</span><br><span class="line">  local headers &#x3D;  &#123;&#125;</span><br><span class="line">  headers[&quot;Content-type&quot;] &#x3D; &quot;application&#x2F;json&quot;</span><br><span class="line"></span><br><span class="line">  local url &#x3D; wrk.format(request_object.method, request_object.path, headers, request_object.body)</span><br><span class="line">  return url</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function done(summary, latency, reqs)</span><br><span class="line"></span><br><span class="line">  local date_table &#x3D; os.date(&quot;*t&quot;)</span><br><span class="line">  local ms &#x3D; string.match(tostring(os.clock()), &quot;%d%.(%d+)&quot;) &#x2F; 1000</span><br><span class="line">  local hour, minute, second &#x3D; date_table.hour, date_table.min, date_table.sec</span><br><span class="line">  local year, month, day &#x3D; date_table.year, date_table.month, date_table.day</span><br><span class="line">  local timeStamp &#x3D; string.format(&quot;%04d-%02d-%02dT%02d:%02d:%02d.%03d&quot;, year, month, day, hour, minute, second, ms)</span><br><span class="line">  print(&quot;Timestamp: &quot; .. timeStamp)</span><br><span class="line"></span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  file &#x3D; io.open(&#39;&#x2F;results&#x2F;today&#x2F;&#39; .. method .. &#39;.csv&#39;, &#39;w&#39;)</span><br><span class="line">  io.output(file)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(&quot;timeStamp,&quot;)</span><br><span class="line">  io.write(&quot;duration_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;num_requests,&quot;)</span><br><span class="line">  io.write(&quot;total_bytes,&quot;)</span><br><span class="line">  io.write(&quot;connect_errors,&quot;)</span><br><span class="line">  io.write(&quot;read_errors,&quot;)</span><br><span class="line">  io.write(&quot;write_errors,&quot;)</span><br><span class="line">  io.write(&quot;error_status_codes,&quot;)</span><br><span class="line">  io.write(&quot;timeouts,&quot;)</span><br><span class="line">  io.write(&quot;requests_per_sec,&quot;)</span><br><span class="line">  io.write(&quot;bytes_per_sec,&quot;)</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(&quot;lat_min_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_max_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_mean_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_stdev_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_90_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_95_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_99_microseconds\n&quot;)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(string.format(&quot;%s,&quot;, timeStamp))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.duration))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.requests))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.bytes))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.connect))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.read))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.write))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.status))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.timeout))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.requests&#x2F;(summary.duration &#x2F; 1000 &#x2F; 1000)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.bytes&#x2F;summary.duration))</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.min))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.max))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.mean))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.stdev))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(90.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(95.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f\n&quot;,  latency:percentile(99.0)))</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>结果显示，x86_64上的 Tomcat 比 arm64快两倍:<br><img src="https://user-images.githubusercontent.com/10891919/81760902-16e48a80-94fb-11ea-84a9-ea88b895d06e.png" alt="image"></p><p>我将试图找出这种差异的原因，并在后续的帖子中与你分享。 如果你有什么想法和建议，我很乐意试试！</p><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>The majority of the software developers usually do not think about the CPU architecture their software will run on. I do not have official statistics but in my experience most of the software for desktop and backend applications run on x86_64 architecture (Intel and AMD processors) and most of the mobile and IoT devices run on ARM architecture. The developers write their software for the respective CPU architecture using some high level programming language and do not think what kind of Assembly instructions are being executed at runtime. And this is the purpose of the high level programming languages — to let the compiler deal with the low level hardware instructions and simplify our task to focus only on the high level business related problems.</p><p>Life is simple and beautiful but there are times when a <a href="https://www.apple.com/">big player</a> in the laptop and desktop hardware and software manufacturing comes and says that our software will have to run on a different architecture — first from <a href="https://en.wikipedia.org/wiki/Apple%27s_transition_to_Intel_processors#Timeline">PowerPC to Intel</a> and now from Intel to ARM64 (sources: <a href="https://www.bloomberg.com/news/articles/2020-04-23/apple-aims-to-sell-macs-with-its-own-chips-starting-in-2021">Bloomberg</a> &amp; <a href="https://appleinsider.com/articles/20/02/25/why-apple-will-move-macs-to-arm-and-what-consumers-get">AppleInsider</a>). Due to the lower consumption of electricity even several of the bigger cloud providers started providing ARM64 virtual machines (e.g. <a href="https://aws.amazon.com/ec2/graviton/">Amazon AWS</a>, <a href="https://www.huaweicloud.com/en-us/product/ecs.html">HuaweiCloud</a>, <a href="https://www.linaro.cloud/">Linaro</a>). And here comes the uncertainty —</p><ul><li>Will my software run on the new CPU architecture ?!</li><li>What kind of changes I will have to do to make it work ?!</li><li>Will it perform as good as before ?!</li></ul><p>To be able to answer these questions you will have to roll up your sleeves and test!</p><p>You can deploy your software on any of the cloud providers. Some of them give free trial period! Or if you are on a low budget you can experiment on <a href="https://www.raspberrypi.org/">RaspberryPi</a>.</p><p>Depending on what programming language you use to write your software you might need to do some changes or not at all! If you use an interpreted language (e.g. Python, Perl, Ruby, JVM, …) then the chances the interpreter already supports ARM64 are pretty high and you are good to go without any changes! But if your software needs to be compiled then you will need to adapt your toolchain and make sure that there are ARM64 binaries for all your dependencies! Depending on your software development stack your mileage may vary!</p><p>Once our software runs fine on the new architecture we will be able to check whether it performs as good as before. Recently some users have asked in Apache Tomcat mailing lists whether ARM64 architecture is supported. Since Apache Tomcat is written mostly in Java it “Just Works”. If you need to use libtcnative and/or mod_jk then you will need to build them yourself on ARM64. Apache Tomcat team uses TravisCI to test both <a href="https://travis-ci.org/github/apache/tomcat">Java</a> and <a href="https://travis-ci.org/github/martin-g/tomcat-connectors">C</a> code on ARM64 and there are no known issues at the moment!</p><p>To compare the performance of two versions of some software usually you will run it on the same hardware but in this case since we use different CPU architectures this makes it impossible. For my tests I have used two VMs with similar specifications:</p><ul><li><p>The x86_64 processor is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">Core(s) per socket:  4</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           GenuineIntel</span><br><span class="line">CPU family:          6</span><br><span class="line">Model:               85</span><br><span class="line">Model name:          Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:            7</span><br><span class="line">CPU MHz:             3000.000</span><br><span class="line">BogoMIPS:            6000.00</span><br><span class="line">Hypervisor vendor:   KVM</span><br><span class="line">Virtualization type: full</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            1024K</span><br><span class="line">L3 cache:            30976K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure></li><li><p>The ARM64 processor is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        aarch64</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  1</span><br><span class="line">Core(s) per socket:  8</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           0x48</span><br><span class="line">Model:               0</span><br><span class="line">Stepping:            0x1</span><br><span class="line">BogoMIPS:            200.00</span><br><span class="line">L1d cache:           64K</span><br><span class="line">L1i cache:           64K</span><br><span class="line">L2 cache:            512K</span><br><span class="line">L3 cache:            32768K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure></li></ul><p>Both VMs have same amount of RAM, disk and network connectivity.</p><p>The test application is based on Spring Boot (2.2.7) running an embedded Apache Tomcat 9.0.x nightly builds and has a single REST controller that exposes a PUT endpoint for creating an entity, a GET endpoint to read it, a POST endpoint to update it and a DELETE endpoint to remove it. It uses <a href="https://memcached.org/">Memcached</a> as a database.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">package info.mgsolutions.testbed.rest;</span><br><span class="line"></span><br><span class="line">import info.mgsolutions.testbed.domain.Error;</span><br><span class="line">import info.mgsolutions.testbed.domain.Person;</span><br><span class="line">import info.mgsolutions.testbed.domain.Response;</span><br><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line">import net.rubyeye.xmemcached.MemcachedClient;</span><br><span class="line">import net.rubyeye.xmemcached.exception.MemcachedException;</span><br><span class="line">import net.rubyeye.xmemcached.transcoders.SerializingTranscoder;</span><br><span class="line">import org.springframework.http.HttpStatus;</span><br><span class="line">import org.springframework.http.MediaType;</span><br><span class="line">import org.springframework.http.ResponseEntity;</span><br><span class="line">import org.springframework.http.server.ServletServerHttpRequest;</span><br><span class="line">import org.springframework.web.bind.annotation.DeleteMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PutMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line">import org.springframework.web.bind.annotation.RestController;</span><br><span class="line">import org.springframework.web.util.UriComponents;</span><br><span class="line">import org.springframework.web.util.UriComponentsBuilder;</span><br><span class="line"></span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line">import java.util.Base64;</span><br><span class="line">import java.util.concurrent.TimeoutException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * A REST endpoint that uses Memcached to get its data.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@RestController</span><br><span class="line">@RequestMapping(&quot;testbed&#x2F;memcached&quot;)</span><br><span class="line">@Slf4j</span><br><span class="line">public class MemcachedTestController &#123;</span><br><span class="line"></span><br><span class="line">public static final int TTL_IN_SECONDS &#x3D; 1000;</span><br><span class="line"></span><br><span class="line">private final SerializingTranscoder coder &#x3D; new SerializingTranscoder();</span><br><span class="line">private final MemcachedClient client;</span><br><span class="line"></span><br><span class="line">public MemcachedTestController(MemcachedClient client) &#123;</span><br><span class="line">this.client &#x3D; client;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PutMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Response&gt; create(@RequestBody Person person,</span><br><span class="line">                                       HttpServletRequest servletRequest) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(person.name);</span><br><span class="line">Person existing &#x3D; client.get(base64Name);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Create: Person with name &#123;&#125; already exists!&quot;, person.name);</span><br><span class="line">Error error &#x3D; new Error(&quot;Person with name &quot; + person.name + &quot; already exists!&quot;);</span><br><span class="line">return ResponseEntity.status(HttpStatus.NOT_ACCEPTABLE)</span><br><span class="line">                     .body(error);</span><br><span class="line">&#125;</span><br><span class="line">log.info(&quot;Create: Going to create &#39;&#123;&#125;&#39;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line"></span><br><span class="line">ServletServerHttpRequest request &#x3D; new ServletServerHttpRequest(servletRequest);</span><br><span class="line">final UriComponents uriComponents &#x3D; UriComponentsBuilder.fromHttpRequest(request).build();</span><br><span class="line">final URI uri &#x3D; uriComponents.encode(StandardCharsets.UTF_8).toUri();</span><br><span class="line">return ResponseEntity.created(uri).contentType(MediaType.APPLICATION_JSON).body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@GetMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.ALL_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; get(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">Person person &#x3D; (Person) client.get(base64(name), coder);</span><br><span class="line">if (person &#x3D;&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Get: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Get: Found person with name: &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.ok().body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PostMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; update(@RequestBody Person person) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line">final String name &#x3D; person.name;</span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person existing &#x3D; (Person) client.get(base64Name, coder);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Update: Going to update: &#123;&#125;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Update: Cannot find a person with name &#123;&#125;!&quot;, person.name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@DeleteMapping(value &#x3D;&quot;&quot;, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; delete(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person person &#x3D; client.get(base64Name);</span><br><span class="line">if (person !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Delete: Going to delete: &#123;&#125;&quot;, person);</span><br><span class="line">client.delete(base64Name);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Delete: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private String base64(String name) &#123;</span><br><span class="line">return Base64.getEncoder().encodeToString(name.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For load testing I have used <a href="https://jmeter.apache.org/">Apache JMeter</a> 5.2.1 and <a href="https://github.com/wg/wrk">wrk</a> from its master branch. JMeter is used for a real case scenario with 1000 simultaneous users, ramp-up period and think time between the HTTP requests. And wrk is used to test the maximal throughput.<br>JMeter is executed with these arguments:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">jmeter.sh \</span><br><span class="line">--testfile JMeter_plan.jmx \</span><br><span class="line">--logfile $RESULTS_FILE \</span><br><span class="line">--reportoutputfolder $RESULTS_FOLDER \</span><br><span class="line">--reportatendofloadtests \</span><br><span class="line">--nongui \</span><br><span class="line">--forceDeleteResultFile \</span><br><span class="line">--jmeterproperty httpclient4.validate_after_inactivity&#x3D;4900 \</span><br><span class="line">--jmeterproperty httpclient4.time_to_live&#x3D;120000 \</span><br><span class="line">-Jhost&#x3D;$JMETER_HOST \</span><br><span class="line">-Jport&#x3D;$JMETER_PORT \</span><br><span class="line">-Jprotocol&#x3D;$JMETER_PROTOCOL \</span><br><span class="line">-JresourceFolder&#x3D;$JMETER_RESOURCE_FOLDER \</span><br><span class="line">-Jusers&#x3D;1000 \</span><br><span class="line">-JrampUpSecs&#x3D;5 \</span><br><span class="line">-Jloops&#x3D;10 \</span><br><span class="line">-JrequestPath&#x3D;&#x2F;testbed&#x2F;memcached</span><br></pre></td></tr></table></figure><p>The httpclient4.** properties are needed to reuse the HTTPS connections, otherwise Keep-Alive was not effective.</p><p>The results from both JMeter and wrk are parsed with Logstash, stored in Elasticsearch and visualized by Kibana.</p><p>JMeter’s response times:<br><img src="https://user-images.githubusercontent.com/10891919/81760860-f6b4cb80-94fa-11ea-9d31-27fb43687cc5.png" alt="image"></p><p>As you can see the results for HTTPS were not very good before May 8th. The HTTPS connections were not reused and TLS handshake has been done for each request, despite request header “Connection: keep-alive”. Since there was no such issue with wrk I’ve asked at JMeter mailing lists and they gave me the httpclient4 arguments above. (Thank you, Philippe Mouawad!). With or without the HttpClient tweak we see that the response times are very similar for x86_64 and arm64.</p><p>For the throughput test with wrk I have run it with these parameters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wrk -c96 -t8 -d30s -s &#x2F;scripts&#x2F;wrk-report-to-csv.lua $HOST:$PORT</span><br></pre></td></tr></table></figure><p>i.e. 8 threads will hit the server for 30 seconds using 96 HTTP(S) connections.</p><p>To collect the summary in a CSV file I used this custom Lua script:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">-- Initialize the pseudo random number generator</span><br><span class="line">-- Resource: http:&#x2F;&#x2F;lua-users.org&#x2F;wiki&#x2F;MathLibraryTutorial</span><br><span class="line">math.randomseed(os.time())</span><br><span class="line">math.random(); math.random(); math.random()</span><br><span class="line"></span><br><span class="line">local _request &#x3D; &#123;&#125;</span><br><span class="line">method &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line">-- Load URL config from the file</span><br><span class="line">function load_request_objects_from_file(csvFile)</span><br><span class="line">  local data &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for line in io.lines(csvFile) do</span><br><span class="line">    local idx &#x3D; string.find(line, &quot;:&quot;)</span><br><span class="line">    local key &#x3D; string.sub(line, 0, idx-1)</span><br><span class="line">    local value &#x3D; string.sub(line, idx+1, string.len(line))</span><br><span class="line">    data[key] &#x3D; value</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  return data</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function trim(s)</span><br><span class="line">  return s:match &quot;^%s*(.-)%s*$&quot;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function readMethod()</span><br><span class="line">  local method &#x3D; &#39;&#39;</span><br><span class="line">  local f &#x3D; io.open(&#39;&#x2F;data&#x2F;method.txt&#39;,&quot;r&quot;)</span><br><span class="line">  if f ~&#x3D; nil then</span><br><span class="line">    method &#x3D; f:read(&quot;*all&quot;)</span><br><span class="line">    io.close(f)</span><br><span class="line">    return trim(method)</span><br><span class="line">  else</span><br><span class="line">    print(&#39;Cannot read the method name from &#x2F;data&#x2F;method.txt&#39;)</span><br><span class="line">    os.exit(123)</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function init(args)</span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  -- Load request config from file</span><br><span class="line">  _request &#x3D; load_request_objects_from_file(&quot;&#x2F;data&#x2F;&quot; .. method ..&quot;.conf&quot;)</span><br><span class="line"></span><br><span class="line">  --for i, val in pairs(_request) do</span><br><span class="line">  --  print(&#39;Request:\t&#39;, i, val)</span><br><span class="line">  --end</span><br><span class="line"></span><br><span class="line">  if _request[method] ~&#x3D; nil then</span><br><span class="line">    print(&quot;multiplerequests: No requests found.&quot;)</span><br><span class="line">    os.exit()</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  print(&quot;multiplerequests: Found a &quot; .. _request.method .. &quot; request&quot;)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">request &#x3D; function()</span><br><span class="line">  local request_object &#x3D; _request</span><br><span class="line"></span><br><span class="line">  -- Return the request object with the current URL path</span><br><span class="line">  local headers &#x3D;  &#123;&#125;</span><br><span class="line">  headers[&quot;Content-type&quot;] &#x3D; &quot;application&#x2F;json&quot;</span><br><span class="line"></span><br><span class="line">  local url &#x3D; wrk.format(request_object.method, request_object.path, headers, request_object.body)</span><br><span class="line">  return url</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function done(summary, latency, reqs)</span><br><span class="line"></span><br><span class="line">  local date_table &#x3D; os.date(&quot;*t&quot;)</span><br><span class="line">  local ms &#x3D; string.match(tostring(os.clock()), &quot;%d%.(%d+)&quot;) &#x2F; 1000</span><br><span class="line">  local hour, minute, second &#x3D; date_table.hour, date_table.min, date_table.sec</span><br><span class="line">  local year, month, day &#x3D; date_table.year, date_table.month, date_table.day</span><br><span class="line">  local timeStamp &#x3D; string.format(&quot;%04d-%02d-%02dT%02d:%02d:%02d.%03d&quot;, year, month, day, hour, minute, second, ms)</span><br><span class="line">  print(&quot;Timestamp: &quot; .. timeStamp)</span><br><span class="line"></span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  file &#x3D; io.open(&#39;&#x2F;results&#x2F;today&#x2F;&#39; .. method .. &#39;.csv&#39;, &#39;w&#39;)</span><br><span class="line">  io.output(file)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(&quot;timeStamp,&quot;)</span><br><span class="line">  io.write(&quot;duration_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;num_requests,&quot;)</span><br><span class="line">  io.write(&quot;total_bytes,&quot;)</span><br><span class="line">  io.write(&quot;connect_errors,&quot;)</span><br><span class="line">  io.write(&quot;read_errors,&quot;)</span><br><span class="line">  io.write(&quot;write_errors,&quot;)</span><br><span class="line">  io.write(&quot;error_status_codes,&quot;)</span><br><span class="line">  io.write(&quot;timeouts,&quot;)</span><br><span class="line">  io.write(&quot;requests_per_sec,&quot;)</span><br><span class="line">  io.write(&quot;bytes_per_sec,&quot;)</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(&quot;lat_min_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_max_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_mean_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_stdev_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_90_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_95_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_99_microseconds\n&quot;)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(string.format(&quot;%s,&quot;, timeStamp))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.duration))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.requests))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.bytes))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.connect))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.read))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.write))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.status))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.timeout))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.requests&#x2F;(summary.duration &#x2F; 1000 &#x2F; 1000)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.bytes&#x2F;summary.duration))</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.min))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.max))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.mean))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.stdev))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(90.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(95.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f\n&quot;,  latency:percentile(99.0)))</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>the results show that Tomcat on x86_64 is twice faster than on arm64:<br><img src="https://user-images.githubusercontent.com/10891919/81760902-16e48a80-94fb-11ea-84a9-ea88b895d06e.png" alt="image"></p><p>I will try to find out </p><p>what is the reason for this difference and share it with you in a follow up post. If you have any ideas I would be happy to test them!</p><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者:  &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接:  &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6&quot;&gt;https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomcat PMC Martin Grigorov带来的Tomcat X86 VS ARM64性能测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>跑benchmark？当心你的芯</title>
    <link href="https://kunpengcompute.github.io/2020/04/28/pao-benchmark-dang-xin-ni-de-xin/"/>
    <id>https://kunpengcompute.github.io/2020/04/28/pao-benchmark-dang-xin-ni-de-xin/</id>
    <published>2020-04-28T12:32:23.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/">https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/</a></p><p>最近，我们在运行基准测试时发现 MySQL 吞吐量的抖动。 即使对于普通用户来说也是如此，但是还有很多其他事情需要注意(尤其是 IO 性能瓶颈) ，以至于我们今天计划讨论的一些方面可能会被暂时省略。 在本文中，我们将讨论可能影响 MySQL 性能的一个原因。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h2 id="在启用-NUMA-的-vm-machine-上的线程调度"><a href="#在启用-NUMA-的-vm-machine-上的线程调度" class="headerlink" title="在启用 NUMA 的 vm / machine 上的线程调度"></a><strong>在启用 NUMA 的 vm / machine 上的线程调度</strong></h2><p>Numa 通常是从内存分配的角度来看待的，但本文试图探讨在不同的 vCPUs上启动线程会如何大幅度地影响性能。 在我们的实验中，我们已经看到性能上升到66% 。</p><p>Mysql 有一个名为 <code>innodb_numa_interleave</code>的选项，如果启用，它将尝试在 NUMA 节点之间统一分配缓冲池。 这很好，但是工作线程呢。 这些工作线程是否在 NUMA 节点上分配过于一致？ 跨 NUMA 访问成本较高，因此最好让工作线程更接近数据，但鉴于这些工作线程的通用性，它们不应该被均匀分布。</p><p>假设我在一台24个 vCPU 机器上启动12个工作线程，这台机器上有2个 NUMA 节点，那么统一的分布预计会有6个工作线程绑定到 NUMA-node-0的 vCPU，剩下6个工作线程绑定到 NUMA-node-1的 vCPU。</p><p>操作系统(Linux)调度程序不是这样工作的。 它将尝试从一个 NUMA节点耗尽 vCPU，然后才会继续到另一个NUMA节点获取。</p><p>当工作线程(可伸缩性) &lt; 核心数，所有这些都会大大影响性能。 甚至还会看到同一测试用例的不同性能，这些都是因为CPU跨NUMA的高访问成本, OS层面调度的不均衡, 以及核心切换导致的。</p><h2 id="实验开始"><a href="#实验开始" class="headerlink" title="实验开始:"></a>实验开始:</h2><p>现在，让我们尝试看看Mysql吞吐量是如何如何由于工作线程所在的位置而更改的。</p><p>我使用同一台机器来运行client(sysbench)和server，因此client也会占用几个核心。 我们还考虑了客户端线程的位置，因为它是运行基准测试时的一个重要方面(除非你计划使用一些专用机器)。</p><ul><li>24 vCPU/48 GB VM，2 个NUMA nodes.<ul><li>NUMA-1: 0-11 vCPU/24GB</li><li>NUMA-2: 12-23 vCPU/24GB</li></ul></li><li>x86 (Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz) VM中每个物理内核有2个线程，所以24个 vCPU就是12个物理内核。 因此，我们还将探索两个工作线程位于不同的 vCPU 但处在相同物理核心的情况。</li><li>Test-Case: oltp-point-select(2 threads). 故意将其限制在两个线程内，以保持其他内核处于开放状态，从而允许操作系统执行内核切换(这会导致它独特的效果)。此外，所有测试数据都在内存中，使用 point-select 意味着没有执行 IO操作，因此 IO 瓶颈或后台线程大多处于空闲状态。 所有迭代的时间是60秒。</li><li>为了让测试更加灵活，vCPUs/cores 使用 numactl (vs taskset)绑定到 sysbench 和 mysqld。</li><li>对于服务器配置请看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>.<br>Data-Size: 34G and BP: 36G. 在内存中生成测试数据并平均分布50% 的数据到 numa-0，剩下50% 的 numa-1。 Sysbench 使用range-type=uniform，这会让其触及大多数测试表的不同部分。</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: (0, 1, 12, 13)</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td><strong>35188, 37426, 35140, 37640 37625, 35574, 35709, 37680</strong></td></tr></tbody></table><p>很显然 tps 在波动。 我做了进一步的研究，操作系统会一直继续做核心切换，导致 TPS 的波动(7% 的范围对于像这样的小测试场景来说太高了)。 此外，操作系统也同样一直在切换客户端线程核心。</p><p>这样的景象促使我对服务器核心绑定进行了更多的探索。</p><h3 id="Client和Server线程所处的位置"><a href="#Client和Server线程所处的位置" class="headerlink" title="Client和Server线程所处的位置"></a><strong>Client和Server线程所处的位置</strong></h3><table><thead><tr><th></th><th>Server-Threads: (Numa Node: 0, Physical Core: 2-5, vCPU: 4-11)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, 3, vCPU: 4, 6)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, vCPU: 4, 5)</th></tr></thead><tbody><tr><td>Client Threads</td><td></td><td></td><td></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0,1)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在同一物理核心上 <strong>TPS: 39570, 38656, 39633</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心上 <strong>TPS: 39395, 39481, 39814</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心上. <strong>TPS: 39889, 40270, 40457</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0,1, vCPU: 0,2)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在不同物理核心上 <strong>TPS: 39890, 38698, 40005</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads on 在不同物理核心上. <strong>TPS: 40068, 40309, 39961</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在不同物理核心上. <strong>TPS: 40680, 40571, 40481</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 37642, 39730, 35984</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 40426, 40063, 40200</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 40292, 40158, 40125</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12,13)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) -Client threads 在同一物理核心上 <strong>TPS: 34224, 34463, 34295</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心上 <strong>TPS: 34518, 34418, 34436</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心上 <strong>TPS: 34282, 34512, 34583</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6,7, vCPU: 12,14)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) -Client threads 处在不同的物理核心上 <strong>TPS: 34462, 34127, 34620</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 处在不同的物理核心上. <strong>TPS: 34438, 34379, 34419</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads(可能性小)执行核心切换 (在同一物理内核上) - Client threads 处在不同的物理核心上. <strong>TPS: 34804,34453,34729</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads处在相同的物理核心和vCPU上 <strong>TPS: 34989, 35162, 35245</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 处在相同的物理核心和vCPU上 <strong>TPS: 35503, 35455, 35632</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads处在相同的物理核心和vCPU上 <strong>TPS: 35572, 35481, 35692</strong></td></tr></tbody></table><h3 id="观察结果"><a href="#观察结果" class="headerlink" title="观察结果:"></a><strong>观察结果</strong>:</h3><ul><li>限制服务器线程的核心有助于稳定性能(减少抖动)。 操作系统核心交换成本很高(如果具有不同的可伸缩性，这可能不太可行，但是是一个很好理解的点)。</li><li>将客户端线程移动到不同的 NUMA 对性能有很大影响(40K-34K)。 我没有想到会这样，因为真正的工作是由服务器工作线程完成的，所以移动客户端线程不应该影响服务器性能到这个程度(17%)。</li></ul><p>因此，从实验中我们了解到，如果客户机和服务器线程位于相同的 NUMA 上，并且使用一定工具减少OS核心交换(直到真的需要扩展核心数或者整体性能) ，则有助于实现最佳性能。</p><p>但是等等！ 我们的目标是在 NUMA 节点上平衡客户端和服务器线程的分布，以获得最佳性能。</p><h3 id="跨-NUMA-平衡客户端和服务器的线程"><a href="#跨-NUMA-平衡客户端和服务器的线程" class="headerlink" title="跨 NUMA 平衡客户端和服务器的线程"></a><strong>跨 NUMA 平衡客户端和服务器的线程</strong></h3><p>让我们应用上面获得的知识和数据来平衡 NUMA配置</p><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: 0, 1, 12, 13</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td>35188, 37426, 35140, 37640, 37625, 35574, 35709, 37680</td><td>多核心交换</td></tr><tr><td>Client thread bounded to specific vCPU across NUMA (0,12)</td><td>Server Thread bounded to specific vCPU across NUMA (4,16)</td><td>30001, 36160, 24403, 24354 37708, 24478, 36323, 24579</td><td>限制核心交换</td></tr></tbody></table><p>Oops，结果比预期的还要糟糕。波动增加了。让我们来看看到底出了什么问题</p><ul><li>24K: OS 选择了threads倾斜的分布，其中 NUMA-x 运行客户端线程，而 NUMA-y 运行两个服务器线程</li><li>37K: OS 选择在每个 NUMA 运行1个客户端和1个服务器线程的情况下很好地平衡了线程分布</li></ul><p><em>(所有其他数字都是排列组合测试得到的)</em></p><p>让我们尝试一个可能的提示。 平衡 NUMA。你可以在这里<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing">here</a>了解更多</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;numa_balancing</span><br></pre></td></tr></table></figure><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPUs across NUMA (0,12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>33628, 34190, 35380, 37572</td><td>限制核心交换 + 禁用NUMA平衡。抖动仍然存在，但肯定比上面提到的24K 情况要好</td></tr></tbody></table><ul><li>如果我们将客户端线程绑定到特定的 NUMA上的核心并平衡服务器线程会怎样？</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPU NUMA (0)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>36742, 36326, 36701, 36570</td><td>限制核心交换 + 禁用NUMA平衡。 看起来很平均。</td></tr><tr><td>Client thread bounded to specific vCPU NUMA (12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>35440, 35667, 35748, 35578</td><td>限制核心交换 + 禁用NUMA平衡。看起来很平均。</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的多个实验，我们看到了给定的测试用例如何因为运行客户机和服务器线程的位置和方式得到了从24K 到40K 不等的不同性能数据。</p><p>如果你的基准测试真的只关心较低的可伸缩性，那么你应该注意核心分配。</p><p>常用的降噪策略有运行测试 n 次的平均值、 n 次的中位数、 n 次的最优值等。但是如果方差是很大的话，没有一个是最有效的。 我倾向于使用平均 n 次最小时间运行测试的策略，因此概率上的数据趋于稳定。我不确定这是否是最好的方法，但似乎它有助于将噪音到一定的程度。 较小的样本(n 值较小)会增加噪声，所以我建议 n =9至少每次运行(60 + 10(tc-warmup))秒，所以630秒的测试用例运行时间足以减少抖动。</p><p>如果你有更好的方案，请与社区分享。</p><p>另外，随着 NUMA 节点的增加和 ARM 上核心的增加，情况变得更加复杂。如果有人研究过它，我会很乐意去理解它。</p><p><em>如果你有疑问，请让我知道，我会尽力回答。</em></p></div><div id="English" class="tab-content"><h2 id="Scheduling-threads-on-NUMA-enabled-VM-Machine"><a href="#Scheduling-threads-on-NUMA-enabled-VM-Machine" class="headerlink" title="Scheduling threads on NUMA enabled VM/Machine"></a>Scheduling threads on NUMA enabled VM/Machine</h2><p>NUMA is often looked upon from a memory allocation perspective but the article tries to explore how booting thread on different vCPUs can affect performance in a big way. During our experiment we have seen performance swing upto 66%.</p><p>MySQL has an option named <code>innodb_numa_interleave</code> that if enabled will try to uniformly allocate the buffer pool across the NUMA node. This is good but what about the worker threads. Are these worker threads too uniformly allocated across the NUMA node? Cross NUMA access is costlier and so having a worker thread closer to the data is always prefered but given the generic nature of these worker threads shouldn’t they be uniformly distributed.</p><p>Say I am booting 12 worker threads on a 24 vCPU machine with 2 NUMA nodes then uniform distribution would expect 6 worker threads bound to vCPUs from NUMA-node-0 and remaining 6 to vCPUs from NUMA-node-1.</p><p>The OS (Linux) scheduler doesn’t work it that way. It would try to exhaust vCPUs from one of the NUMA-nodes and then proceed to another.</p><p>All this could affect performance big-way till your worker threads (scalability) &lt; number-of-cores. Even beyond this point one may see varying performances for the same test-case due to core-switches.</p><h2 id="Understanding-the-setup"><a href="#Understanding-the-setup" class="headerlink" title="Understanding the setup:"></a>Understanding the setup:</h2><p>Let’s now try to see how the throughput can change based on where the worker threads are located.</p><p>I am using the same machine to run client (sysbench) and server so few cores are reserved for clients too. We also consider the position of client threads as it is an important aspect while running benchmark (unless you plan to use some dedicated machine for it).</p><ul><li>24 vCPU/48 GB VM with 2 NUMA nodes.<ul><li>NUMA-1: 0-11 vCPU/24GB</li><li>NUMA-2: 12-23 vCPU/24GB</li></ul></li><li>x86 (Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz) VM has 2 threads per physical core so 24 vCPU = 12 physical cores. So we would also explore scenarios when both worker threads are located on different vCPU but have the same physical core.</li><li>Test-Case: oltp-point-select with 2 threads. Purposely limiting it to 2 threads to keep other cores open to allow the OS to do core-switches (this has its own sweet effect). Also, all data is in-memory and using point-select means no IO being done so IO bottlenecks or background threads are mostly idle. All iterations are timed for 60 seconds.</li><li>vCPUs/cores are bound to sysbench and mysqld using numactl (vs taskset) given the flexibility it provides.</li><li>For server configuration please check <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>.<br>Data-Size: 34G and BP: 36G. Complete data in memory and equally distributed so 50% of data on numa-0 and remaining 50% of numa-1. Sysbench uses range-type=uniform that should touch most of the varied parts of the table.</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: (0, 1, 12, 13)</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td><strong>35188, 37426, 35140, 37640 37625, 35574, 35709, 37680</strong></td></tr></tbody></table><p>Naturally the tps is fluctuating. Some closer look revealed that OS continues to do core-switch that causes TPS to fluctuate (range of 7% is too high for small test-case like this). Also, OS continues to switch client threads cores too.</p><p>That prompted me to explore more about server core binding. As part of completeness I also explored client thread positioning.</p><h3 id="Position-of-Client-and-Server-Threads"><a href="#Position-of-Client-and-Server-Threads" class="headerlink" title="Position of Client and Server Threads"></a>Position of Client and Server Threads</h3><table><thead><tr><th></th><th>Server-Threads: (Numa Node: 0, Physical Core: 2-5, vCPU: 4-11)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, 3, vCPU: 4, 6)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, vCPU: 4, 5)</th></tr></thead><tbody><tr><td>Client Threads</td><td></td><td></td><td></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0,1)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on the same physical core <strong>TPS: 39570, 38656, 39633</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on the same physical core. <strong>TPS: 39395, 39481, 39814</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on the same physical core. <strong>TPS: 39889, 40270, 40457</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0,1, vCPU: 0,2)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on different physical core <strong>TPS: 39890, 38698, 40005</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on different physical core. <strong>TPS: 40068, 40309, 39961</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different same physical core. <strong>TPS: 40680, 40571, 40481</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on same physical core and same vCPU <strong>TPS: 37642, 39730, 35984</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on same physical core and same vCPU <strong>TPS: 40426, 40063, 40200</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on same physical core and same vCPU <strong>TPS: 40292, 40158, 40125</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12,13)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) -Client threads on the same physical core <strong>TPS: 34224, 34463, 34295</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on the same physical core. <strong>TPS: 34518, 34418, 34436</strong></td><td>- Client+Server threads on different NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on the same physical core. <strong>TPS: 34282, 34512, 34583</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6,7, vCPU: 12,14)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) -Client threads on different physical core <strong>TPS: 34462, 34127, 34620</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on different physical core. <strong>TPS: 34438, 34379, 34419</strong></td><td>- Client+Server threads on different NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different same physical core. <strong>TPS: 34804,34453,34729</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on same physical core and same vCPU <strong>TPS: 34989, 35162, 35245</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on same physical core and same vCPU <strong>TPS: 35503, 35455, 35632</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different physical core and same vCPU <strong>TPS: 35572, 35481, 35692</strong></td></tr></tbody></table><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations:"></a>Observations:</h3><ul><li>Limiting cores for server threads helps stabilize the performance (reduce jitter). OS-core switches are costly (with varying scalability this may not be feasible but a good parameter to understand).</li><li>Moving client thread to different NUMA affects performance in a big way (40K -&gt; 34K). I was not expecting this since the real work is done by server worker threads so moving client threads should not affect server performance to this level (17%).</li></ul><p>So from the experiment we learned that client and server threads if co-located on the same NUMA and technique to reduce core-switch (till it is really needed with increased scalability) helps achieve optimal performance.</p><p>But wait! Our goal is to have balance distribution of client and server threads across the NUMA node to get optimal performance.</p><h3 id="Balance-Client-and-Server-Threads-across-NUMA"><a href="#Balance-Client-and-Server-Threads-across-NUMA" class="headerlink" title="Balance Client and Server Threads across NUMA"></a>Balance Client and Server Threads across NUMA</h3><p>Let’s apply the knowledge gained above to balance numa configuration</p><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: 0, 1, 12, 13</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td>35188, 37426, 35140, 37640, 37625, 35574, 35709, 37680</td><td>Lot of core switches</td></tr><tr><td>Client thread bounded to specific vCPU across NUMA (0,12)</td><td>Server Thread bounded to specific vCPU across NUMA (4,16)</td><td>30001, 36160, 24403, 24354 37708, 24478, 36323, 24579</td><td>Limit core switches</td></tr></tbody></table><p>Oops it turned out to be worse than expected. Fluctuation increased. Let’s understand what went wrong</p><ul><li>24K: OS opted for skewed distribution with NUMA-x running both client threads and NUMA-y running both server threads.</li><li>37K: OS opted for well balance distribution with each NUMA running 1 client and 1 server thread.</li></ul><p><em>(All other numbers are mix of combinations)</em></p><p>Let’s try a possible hint. NUMA balancing. You can read more about it <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing">here</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;numa_balancing</span><br></pre></td></tr></table></figure><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPUs across NUMA (0,12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>33628, 34190, 35380, 37572</td><td>Limit core switches + NUMA balancing disabled. Jitter is still there but surely better than 24K case above.</td></tr></tbody></table><ul><li>What if we bind client thread to specific numa cores and balance server threads</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPU NUMA (0)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>36742, 36326, 36701, 36570</td><td>Limiting core switches + NUMA balancing disabled. Looks well balanced now.</td></tr><tr><td>Client thread bounded to specific vCPU NUMA (12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>35440, 35667, 35748, 35578</td><td>Limit core switches + NUMA balancing disabled. Looks well balanced now.</td></tr></tbody></table><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Through multiple experiments above we saw how the given test-case can help produce different results ranging from 24K -&gt; 40K based on where and how you run client and server threads.</p><p>If your benchmark really cares about the lower scalability then you should watch out for the core allocation.</p><p>Usual strategies to reduce noise are average of N runs, median of N runs, best of N runs, etc… But if the variance is that high none of them will work best. I tend to use strategy of averaging N smaller time runs so with probabilty things could stablize. Not sure if this is best approach but seems like it help reduce the noise to quite some level. Lesser sample (smaller value of N) would increase noise so I would recommend N = 9 at-least with each run of (60+10 (tc-warmup)) secs so 630 seconds run of test-case is good enough to reduce the jitter.</p><p>If you have better alternative, please help share it with community.</p><p>BTW, story is more complex with increasing NUMA nodes and more cores on ARM. Topic of future. If anyone has studied it, would love to understand it.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/&quot;&gt;https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近，我们在运行基准测试时发现 MySQL 吞吐量的抖动。 即使对于普通用户来说也是如此，但是还有很多其他事情需要注意(尤其是 IO 性能瓶颈) ，以至于我们今天计划讨论的一些方面可能会被暂时省略。 在本文中，我们将讨论可能影响 MySQL 性能的一个原因。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM, 真的来了！</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-zhen-de-lai-liao/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-zhen-de-lai-liao/</id>
    <published>2020-04-24T09:10:49.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Why-ARM/">https://mysqlonarm.github.io/Why-ARM/</a></p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>ARM处理器无处不在。 很有可能你们中的一些人在阅读这篇博客时，使用的是 基于ARM 驱动的设备。 电话，物联网设备，家用电器，医疗设备，都是由 ARM 处理器驱动的。 ARM处理器以节能著称，因此大多数需要较长充电周期且处理器能力较低的设备一直在使用ARM芯片。</p><p>但在过去的几年里，这种情况已经发生了变化。 越来越多的 ARM 处理器被用于数据库服务器、 web 服务器、应用服务器、大数据场景等高端应用。 它们已经作为服务器芯片进入了数据中心。 在云上运行应用程序时，ARM芯片被视为一种具有成本效益的最佳选择。</p><h2 id="ARM-生态之进化"><a href="#ARM-生态之进化" class="headerlink" title="ARM 生态之进化"></a>ARM 生态之进化</h2><p>几年前，很难想象 ARM 会被用于运行在一些高端的服务器级的应用程序。 我可以想出两个主要原因:</p><ul><li>Arm 最适合小型手持设备</li><li>Arm 生态受到它所支持的特定产品的限制</li></ul><p>一些主要的操作系统提供商对 ARM 生态系统提供了支持，包括 RedHat (CentOS)、 Ubuntu、 Debian 和 Windows。 这使得主流软件可以轻松地移植到 ARM。 社区推动ARM以确保大多数主流软件都可以在 ARM架构上使用。 例如IDE、 DB-server、 Hadoop 及其衍生软件，包括 Apache 基金会、 CI/CD软件、容器、虚拟化软件等等。 .</p><p>授权其他厂商和开发自己的 ARM 处理器的 ARM 商业模式进一步推动了其受欢迎的程度，吸引了更多的芯片设计师参与其中，相互合作和创新优化。</p><p>随着亚马逊等主要云供应商开始提供基于 ARM 处理器的 EC2实例(目前只允许邀请) ，这意味着现在每个人都可以启动一个 ARM 实例，并开始在 ARM 上开发 / 移植他们想要的软件。 这非常有助于ARM生态的进一步发展。</p><h2 id="那…还缺什么"><a href="#那…还缺什么" class="headerlink" title="那…还缺什么?"></a>那…还缺什么?</h2><p>虽然大部分主流软件已经移植到 ARM 上，但它们还没有针对 ARM 进行优化。 Arm 具有弱内存模型，可以在更小的空间内安置更多的内核，底层指令的不同(软件会用这些指令)等等。 </p><p>这是 ARM 生态第二阶段的开始，社区 / 开发人员 / 用户开始从“在 ARM 上运行软件”导向“在 ARM 上优化软件”。 我认为当用户开始认真思考 ARM 并开始在 ARM 上优化他们的软件时，这是 ARM 社区乃至ARM生态的一个重大胜利(大大的里程碑)。</p><p>这(尤其是优化)是一个永无止境的过程，我看的第一个目标是至少与 x86的部分性能相当。 我故意说“ 部分性能” ，因为每个架构都有自己的 USP，所以如果你将一个企业级应用程序移植到 ARM，并且你可以以 50% 的成本(运营成本 + 初始投资)外加x86芯片的75% 性能的ARM芯片提供给客户 ，我认为这对大多数客户来说仍然是极具吸引力的(特尤其是应用程序为可伸缩的场景)。 当然，这并不意味着所有的应用程序在 ARM 上运行的速度都比较低下，事实上，有些应用程序在 ARM 上运行的速度会比 x86快，而且由于ARM生态的优化阶段在未来几年才逐步开始，经过各个玩家的努力，可以推断许多应用程序在 ARM 上运行的速度将比其他架构更快、更猛。</p><h2 id="ARM节能环保-！"><a href="#ARM节能环保-！" class="headerlink" title="ARM节能环保 ！"></a>ARM节能环保 ！</h2><p>ARM无处不在，尤其是数据中心运营商(无论大小)最关心的问题。 与其他架构相比，ARM 可以节省大约50% 的电力。 这有助于支持绿色环保倡议。</p><h2 id="ARM是下一代处理器！"><a href="#ARM是下一代处理器！" class="headerlink" title="ARM是下一代处理器！"></a>ARM是下一代处理器！</h2><p>有趣的是，我为什么这样称呼ARM。 下一代的芯片正在积极地使用 Andrino、 Raspberry Pi、 Odroid、 Banana Pi、 Asus tinker board 等工具包来构建下一代系统。 这些芯片将定义下一代计算机。 你看到它们正在使用 ARM，并且已经围绕着 ARM 成长起来，变得更加通用，在未来的几年里，将会有一大批 ARM 用户 / 开发者簇拥在一起，共建ARM生态。</p><p>一旦大量的ARM用户/开发者的支持到位，现阶段围绕 ARM正在紧锣密鼓进行的所有基础工作和有助于用户的事物将会迎来新的面貌。</p><h2 id="ARM上台式机-笔记本！"><a href="#ARM上台式机-笔记本！" class="headerlink" title="ARM上台式机 / 笔记本！"></a>ARM上台式机 / 笔记本！</h2><p>如果我们开始看到基于 ARM 的办公桌面 / PC工作站 / 笔记本电脑(已经有了！)被大家普遍使用时，ARM挤占台式机/笔记本市场也就不足为奇了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Arm 生态系统看起来非常迷人，其中充满了新的挑战和机遇。 畅想一下近十年，从微型可穿戴设备到高端电影体验，从自动驾驶助力车 / 汽车到大型喷气式飞机 / 太空飞行器，都会被基于 ARM 的处理器主宰，它将无处不在。 现阶段据估计，每人有35个基于 ARM的设备。 这就是机会的海洋，不是吗?!</p><p><em>如果您有想法，请随时发送邮件交流</em></p></div><div id="English" class="tab-content"><p>ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.</p><p>But this has changed in the past few years. More and more ARM processors are being used for high-end applications like database server, web server, application server, big data use-cases. They have already made their way to the data-centers as a server class machines. They are being looked upon as a cost effective option while running applications in cloud.</p><h2 id="ARM-ecosystem-evolution"><a href="#ARM-ecosystem-evolution" class="headerlink" title="ARM ecosystem evolution"></a><span style="color:#4885ed">ARM ecosystem evolution</span></h2><p>Few years back it was difficult to imagine that ARM would be used for running some high-end server class applications. There were 2 major reasons that I could think off:</p><ul><li>ARM were best suited for small handheld devices.</li><li>ARM ecosystem was limited around the specific product it supported.</li></ul><p>ARM ecosystem has really picked up well after some major OS providers added support for it including RedHat (CentOS), Ubuntu, Debian, Windows. This eased out porting of the major softwares to ARM. ARM community gave it a push to make sure most of the standard softwares are available on ARM viz. IDE, DB-server, Hadoop and all its variants from Apache Foundation, CI/CD software, Container, Virtualization, etc…</p><p>The ARM model that allows other vendors to license and develop their own ARM processors further helped fueled its popularity with more chip designers joining, collaborating and innovating.</p><p>Break-through came with major cloud providers like Amazon started providing ec2 instances (currently invitation only) based on ARM processors this means now everyone can boot an ARM instance and start developing/porting their software on ARM. This helped further grow the ecosystem.</p><h2 id="What-was-missing"><a href="#What-was-missing" class="headerlink" title="What was missing?"></a><span style="color:#4885ed">What was missing?</span></h2><p>Though most of these software have been ported to ARM they were not yet optimized for ARM. ARM has a weak memory model, can fit more cores in smaller space, difference in low-level instruction (for software that uses them), etc..</p><p>This was the start of the 2nd phase of ARM where the community/developer/user started moving from “running software on arm” -&gt; “optimizing software on arm”. I think this was a major win for the arm community when users started to think ARM seriously and started spending efforts on optimizing their software on ARM.</p><p>This (especially optimization) is a never ending process but I see first goal is to at-least be on par with x86. I purposely say “onpar” because each of architecture has its own USP so say if you port an enterprise class application to ARM and you can offer it to customer @ 50% of the cost (operating cost + initial investment) for 75% of the performance of x86 I think that would be still be attractive fit for most of the customers (especially given application are horizontally scalable). Of-course that doesn’t mean all applications run on ARM at reduced speed, in fact there are applications that run on ARM faster than x86 and since the optimization phase has just started in next few years a lot of applications would be running on ARM faster than other architectures.</p><h2 id="Go-Green"><a href="#Go-Green" class="headerlink" title="Go Green"></a><span style="color:#4885ed">Go Green</span></h2><p>It is everywhere and especially a matter of major concern for data-center operators (small or big). ARM being power efficient can save approximately 50% of the power compared to other architecture. This makes it help support Go-Green initiative.</p><h2 id="ARM-is-Next-Gen-processor"><a href="#ARM-is-Next-Gen-processor" class="headerlink" title="ARM is Next-Gen processor"></a><span style="color:#4885ed">ARM is Next-Gen processor</span></h2><p>It is interesting why I referred to it this way. Next generation kids are actively using kits like Andrino, Raspberry Pi, Odroid, Banana Pi, Asus tinker board, etc…. to build some of the next-gen system. These kids will be defining the next generation of computing. Given they started with ARM their social community has grown around ARM in the next few years there would be an army of ARM users/developers with a very active community.</p><p>All the groundwork and good things that are being built at this stage around ARM will be pushed to the next level once this workforce becomes active.</p><h2 id="ARM-in-Desktop-Laptop"><a href="#ARM-in-Desktop-Laptop" class="headerlink" title="ARM in Desktop/Laptop"></a><span style="color:#4885ed">ARM in Desktop/Laptop</span></h2><p>This is catching up fast and no wonder if we start seeing ARM based Desktop/PC workstation/Laptop (there are already few) commonly being used.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>The ARM Ecosystem looks a lot more fascinating and full of new challenges and opportunities. Current decade will be ruled by ARM based processors and it will be everywhere from tiny wearable devices to high-end movie experience, from auto-driving cycle/car to jumbo jet/space-craft. It is estimated that there would be 35 active ARM power devices per person. That’s Ocean of Opportunity.</p><p><em>If you have any comments feel free to drop an email (check about section)</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Why-ARM/&quot;&gt;https://mysqlonarm.github.io/Why-ARM/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM: 需要了解的背景和问题</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-xu-yao-liao-jie-de-bei-jing-he-wen-ti/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-xu-yao-liao-jie-de-bei-jing-he-wen-ti/</id>
    <published>2020-04-24T07:21:27.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者： bzhaoopenstack<br>作者： Amit Dattatray Khandekar<br>原文链接：<a href="https://amitdkhan-pg.blogspot.com/">https://amitdkhan-pg.blogspot.com/</a></p><p>Arm 的故事始于1993年，当时苹果与 ARM (当时是 Acorn RISC Machines)组建了一家合资企业，推出了“ Apple Newton”掌上电脑。 这个故事今天还在继续，有消息称苹果将把他们的 Mac换成 ARM 处理器，在这里 ARM 仍然以节能处理器的角色出现。 这是它在智能手机中如此受欢迎，也是它进入智能汽车、无人机和其他物联网设备的主要原因，在这些设备中，电池寿命保持和硬件尽量减少热量的产生是至关重要的。 今天，甚至是数据中心也可以在 ARM 上运行。 由于市场的巨大变更，在此，我考虑抛出关于主角ARM的一些观点，这些观点对于刚刚开始接触 ARM 生态系统的用户和软件开发人员是十分有益的..</p><a id="more"></a><p>Arm 能耗较低的原因与 其所基于的 RISC的硬件架构有关。 RISC 指令非常简单，每个指令执行只需要一个CPU时钟周期; 因此它们需要更少的晶体管，更少的功率，从而产生更少的热量。</p><p>OK，是什么促使ARM 处理器开始进入数据中心市场呢？ 毕竟，移动电话和数据中心没有任何共同点。 不是吗？</p><p>好吧，两者都消耗电力，并且都需要在拟定的成本下表现良好。 尽管数据中心与手机相比是巨大的，同时它们对 CPU需求和 使用量也是巨大的。 对于两个场景功率效率非常重要，能达到市场需求的性能成本也是一样。 </p><p><strong>分而治之</strong></p><p>那么，我们就用更多的，又便宜的 ARM 处理器来替换现有的数据中心中昂贵的处理器不就好了，这样总的 CPU 功耗不就等于现有的功耗了吗？ 是的，这确实有用。 假设有4个 cpu 承载16个并行进程，那么最好的办法是将4个高性能CPU替换为8到16个性能较差的 cpu ，这样总吞吐量可能会更高。</p><p>但是，当有一个数据库长查询，需要消耗高性能 CPU 的能力时？ 即便在这个时候，这个数据库查询请求也可以使用多个 较低性能的cpu 来并行查询。 这里我们看到，即使是软件也需要适应这种范式转换: 就是尽可能地将任务划分为多个并行任务。 我们需要理解这样一个事实: 重要的不仅仅是单个 CPU 的能力，而是所有 CPU 的总能力。</p><p><strong>big.LITTLE架构</strong></p><p>在 ARM 的big.LITTLE架构中，在同一个 SoC 中可以有两个或多个具有不同性能的核。 如果其中一个处理的工作负载发生变化，如果另一个核更适合已变化的工作负载的话，那么它可以动态地接管工作负载。 这种方式避免消耗不必要的电力和热量的产生，因为ARM定位是低功耗处理器，成本成本！！ 在 linux 内核中，这种调度已经得到了支持，特别是针对 big.LITTLE架构。</p><p><strong>ARM的商业许可模式</strong></p><p>正如我们许多人知道的那样，ARM 公司本身并不生产芯片，而是设计芯片。 它的客户购买了基于 ARM 架构设计的芯片制造许可证。 目前为止，有两种这样的许可证。</p><p>一种是核心许可证。 当一家公司购买核心许可证时，它必须使用 ARM 的内部核心设计制造完整的 CPU 核心，而不能做任何更改。 Arm 授权的核心系列名为 Cortex-A * * 。 例如，在高通的 Snapdragon 855芯片组中，所有的 CPU 核心都基于 Cortex-A 系列; 这意味着它们使用了 ARM 的核心许可证。</p><p>另一个是 ARM 架构许可证。 当一家公司购买这个许可证而不是核心许可证时，它必须设计自己的核心，但核心的设计必须与 ARM 指令集兼容。 这样的核心通常被称为定制核心，因为它们有自己的微结构，而不是由 ARM 设计的。 这为某些大公司根据自己的需要生产核心提供了灵活的条件。 高通(Qualcomm)、华为(huawei)、苹果(Apple)和三星(Samsung)等公司都制造了这种定制的核心。</p><p>这种许可模式的优点在于: 现成的核心设计可供任何人使用(当然需要购买许可证)。 因此，有许多厂商都制造了兼容的芯片。 从而进一步推动了芯片创新和竞争。</p><p><strong>基于ARM的应用程序</strong></p><p>移动设备的应用程序已经基于ARM 处理器上编写了。 但是在服务器上运行的软件呢？ 好吧，Linux 内核支持 ARM，所以像 Ubuntu，CentOS 和 Debian 这样的操作系统已经正式支持 ARM 镜像。 另外，如果你使用的是 Ubuntu，在Ubuntu 官方库中几乎所有常用的 x86依赖库都已经能安装在 ARM 上了，至少对于 ARMv8来说已经做了该做的porting。 我能够安装 PostgreSQL 数据库包，并且一直在高竞争的情况下运行 pgbench，并且运行得很好。 (可能在以后的博客中，我将进一步阐述 PostgreSQL)此外，gcc / g + + 等编译器已经针对 ARM 架构进行了调优，因此大多数硬件专用编译器都已完成了ARM架构相关的优化。</p><p>但是，当涉及到那些运行在数据服务器的软件时，可能需要进行大量的适配工作才能获得比较nice的性能。 例如，在做代码同步的时候，应用程序必须特别注意ARM 的弱内存模型。 其次，应用程序应该利用内置的 ARM 硬件方面的能力，比如 NEON (这是 ARM 对 SIMD 的品牌名称) 指令，以便在多数据流上并行操作等等。</p><p>对此，目前已经有很多开发者和团队正投身于大量的研究和分析，以优化整个 ARM 生态系统中的软件。 同时我们已经看到了这个ARM生态系统正在逐渐过渡、完善和适应市场。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者： bzhaoopenstack&lt;br&gt;作者： Amit Dattatray Khandekar&lt;br&gt;原文链接：&lt;a href=&quot;https://amitdkhan-pg.blogspot.com/&quot;&gt;https://amitdkhan-pg.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Arm 的故事始于1993年，当时苹果与 ARM (当时是 Acorn RISC Machines)组建了一家合资企业，推出了“ Apple Newton”掌上电脑。 这个故事今天还在继续，有消息称苹果将把他们的 Mac换成 ARM 处理器，在这里 ARM 仍然以节能处理器的角色出现。 这是它在智能手机中如此受欢迎，也是它进入智能汽车、无人机和其他物联网设备的主要原因，在这些设备中，电池寿命保持和硬件尽量减少热量的产生是至关重要的。 今天，甚至是数据中心也可以在 ARM 上运行。 由于市场的巨大变更，在此，我考虑抛出关于主角ARM的一些观点，这些观点对于刚刚开始接触 ARM 生态系统的用户和软件开发人员是十分有益的..&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM: Points to be noted</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-points-to-be-noted/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-points-to-be-noted/</id>
    <published>2020-04-24T07:19:47.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者： Amit Dattatray Khandekar<br>原文链接：<a href="https://amitdkhan-pg.blogspot.com/">https://amitdkhan-pg.blogspot.com/</a></p><p>  The story of ARM began in 1993 with a joint venture of Apple with ARM (then Acorn RISC Machines) to launch the “Apple Newton” handheld PC. And the story continues today with news that Apple is going to switch their MACs to ARM processors.  What has not changed in the story is ARM’s reputation as a power-efficient processor. This is the primary reason why it is so popular in smarthphones, and why it has made its way into smart cars, drones and other internet-of-things devices where it is crucial to preserve battery life and minimize heat generation. Today even data centers can run on ARM. Due to such widespread market disruption happening, I thought about putting some specific points which I think are good-to-know for users and software developers who have just begun using the ARM ecosystem …</p><a id="more"></a><p>The reason why ARM power consumption is less has to do with the inherent nature of RISC architecture on which ARM is based. RISC instructions are so simple that each of them requires only one clock cycle to execute; so they require less transistors, and hence less power is required and less heat is generated.</p><p>Ok, but then why ARM processors started making their way into data centers? After all, mobile phones and data centers don’t have anything in common. Or do they?</p><p>Well, both consume power, and both need to perform well for a given price. Even though data centers are huge as compared to the size of a mobile phone, their CPU usage is also huge. So power efficiency is equally important. And so is the price for a given performance.</p><p><strong>Divide and conquer</strong></p><p>So, just replace the existing expensive processors with more number of cheaper ARM processors, so that the total CPU power will be equal to the existing power ? Yes, this does work. Suppose, there are 4 CPUs serving 16 parallel processes, it’s better for them to be instead served by 8 or 16 lower performing CPUs.  Overall throughput will likely be higher.</p><p>But what if there is a single long database query which needs high CPU power ? Even here, the database query can make use of multiple CPUs to run a parallelized query.  Here we see that even the software needs to adapt to this paradigm shift: divide the task into number of parallel tasks wherever possible. We need to understand the fact that more than the power of a single CPU, what counts is the total power of all the CPUs.</p><p>Another thing is that, the worloads are not always high. For instance, cloud service workloads are always mixed, frequently with numerous small tasks, where again a server with large number of low power CPUs fits well.</p><p><strong>big.LITTLE</strong></p><p>In the ARM’s big.LITTLE architecture, there can be two or more cores of different performance capacity in the same SoC. And if the workload processed by one of them changes, the other one can take over that workload on the fly if it is more suitable for the changed workload. This way unnecessary power usage and heat generation is prevented because the low-power processor type gets chosen. There has been support for doing such scheduling particularly for big.LITTLE in the linux kernel.</p><p><strong>ARM’s licensing model</strong></p><p>As many of you might know, ARM does not manufacture chips; it designs them. And it’s clients buy its license to manufacture chips based on ARM’s design. Now, there are two kinds of licenses.</p><p>One is the core license.  When a company buys the core license, it has to manufacture the complete CPU core using ARM’s in-house core design without modifying it. The ARM’s family of core designs that it licenses, are named Cortex-A**. E.g. in Qualcomm’s Snapdragon 855 chipset, all CPU cores are based on Cortex-A series; it means they used the ARM core license.</p><p>The other is the ARM architecture license. When a company buys this license and not the core license, it has to design it’s own core, but the core design has to be compatible with the ARM instruction set. Such cores are often called custom cores, because they have their own micro-architecture that is not designed by ARM. This provides flexibility to the big companies to build cores as per their own needs. Companies like Qualcomm, Huawei, Apple and Samsung have built such custom cores.</p><p>The beauty of this licensing model is : the ready-made core design is available to just anybody (of course a license has to be bought). And hence there are a number of vendors who all have manufactured compatibile chips. This drives innovation and competition.</p><p><strong>Applications</strong></p><p>Applications for mobile devices were already written from scratch on ARM processers. But what about the software running on servers ? Well, Linux kernel has support for ARM, so OSes like Ubuntu, CentOS and Debian already have officially supported ARM images. Furthermore, if you are running on, say Ubuntu, almost all the usual x86 packages that are present in the Ubuntu repository are already there for ARM as well, at least for ARMv8. I was able to install the PostgreSQL database package, and have been running pgbench with high contention, and it runs just fine. (Probably in later blogs, I will elaborate on PostgreSQL further) Also, the compilers like gcc/g++ are already tuned for ARM architecture, so most of the hardware-specific compiler optimizations are transparently done for ARM.</p><p>But when it comes to running software meant for data servers, a lot of adaptation might be required to have a reasonable performance. For instance, applications have to be aware of the implications of the ARM’s weak memory model, especially for code synchronizatoin. Secondly, they should leverage in-built ARM capabilities like NEON (which is the ARM’s brand name for SIMD) to parallelize same operation on multiple data; and so on.</p><p>A lot of research and analysis is going on to optimize sofware running in the ARM ecosystem as a whole. But we are already seeing a gradual transition and adaptation to this ecosystem.  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者： Amit Dattatray Khandekar&lt;br&gt;原文链接：&lt;a href=&quot;https://amitdkhan-pg.blogspot.com/&quot;&gt;https://amitdkhan-pg.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  The story of ARM began in 1993 with a joint venture of Apple with ARM (then Acorn RISC Machines) to launch the “Apple Newton” handheld PC. And the story continues today with news that Apple is going to switch their MACs to ARM processors.  What has not changed in the story is ARM’s reputation as a power-efficient processor. This is the primary reason why it is so popular in smarthphones, and why it has made its way into smart cars, drones and other internet-of-things devices where it is crucial to preserve battery life and minimize heat generation. Today even data centers can run on ARM. Due to such widespread market disruption happening, I thought about putting some specific points which I think are good-to-know for users and software developers who have just begun using the ARM ecosystem …&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM vs X86 Hadoop 性能大比拼</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-vs-x86-hadoop-xing-neng-da-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-vs-x86-hadoop-xing-neng-da-bi-pin/</id>
    <published>2020-04-24T02:59:28.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>参与者：<a href="https://github.com/ayushtkn">Ayush Saxena</a>、<a href="https://github.com/ZhengZhenyu">郑振宇</a>、<a href="https://github.com/liusheng">刘胜</a></p><p><em>背景：2019年，本团队在 Apache Hadoop 开源社区上游完成了 ARM 平台适配并使能了 Hadoop ARM CI 以便保证 Hadoop 在后续的版本开发中均保证 ARM 平台适配，并且团队成员正在推动 Hadoop 开源社区发布 ARM 平台的软件包，以便用户可以直接下载使用。在 ARM 的基本适配工作完成后，性能调优成了我们下一步的工作重心。</em></p><p>近期，华为计算开源生态部技术团队进行了 Apache Hadoop 在 ARM 及 X86 平台上的初步性能比拼测试及简要分析，可以为关心 ARM 数据中心大数据场景的读者提供一些参考。</p><a id="more"></a><h2 id="测试环境及工具"><a href="#测试环境及工具" class="headerlink" title="测试环境及工具"></a>测试环境及工具</h2><p>本次测试使用华为云，我们在华为云上搭建了两套三节点测试集群（一套ARM，一套X86），个节点均为8vCPU 16GB RAM，但在CPU指标上，由于硬件限制，X86集群使用 <code>Intel Xeon Gold 6266C</code> 主频为  <code>3.0 GHz</code> 而 ARM 节点使用 <code>Huawei Kunpeng 920</code> 主频为 <code>2.6 GHz</code>，<strong>但从成本角度来看，ARM 节点的单价仅为 X86 节点单价的 70%</strong>，在参考性能比拼结果时，需要考虑上述指标。硬盘方面，我们使用了IOPS上限4,200，IOPS突发上限5,000的500G硬盘作为系统盘；网络方面，为虚拟机配置了5Mbit/s的带宽。</p><p>本次测试分别对上述Hadoop集群进行了业界常用的<code>TeraSort</code>测试，结合Hadoop支持的各种压缩算法，对50GB数据进行<code>TeraSort</code>对比测试，并对测试结果进行简要分析。</p><h2 id="测试数据及简要分析"><a href="#测试数据及简要分析" class="headerlink" title="测试数据及简要分析"></a>测试数据及简要分析</h2><p>首先，需要再次明确的是，本次测试所使用的CPU在主频方面并不相同，大约有13%的性能差距(2.6G Hz vs 3.0GHz)，我们可以对性能指标乘以一定的系数来进行平衡，但由于ARMv8以及X86指令集自身的不同，简单的系数并不能完全反映出实际情况。这里我们先通过简单系数的方式来进行初步的理论分析，来看一看Apache Hadoop在ARM运行在ARM数据中心上是否与运行在X86数据中心上有可比性，为读者提供一定的参考；后面我们也会继续探索更为合理和精确的对比方式。</p><p><img src="https://user-images.githubusercontent.com/10849016/80173098-5d904480-8621-11ea-9d4b-f894c5a4c1d9.png" alt="image"></p><p>从上面得测试数据可以发现，ARM平台上的表现整体比X86稍差，性能差距大约在10%-20%之间；将我们上面提到的CPU主频大约有13%的性能差距考虑在内，ARM平台上的整体表现与X86平台上的性能表现之间的差距大约在10%以内。</p><p>针对这一情况，我们进行了更进一步的分析，分析结果表明，ARM平台在<strong>Mapper</strong>阶段（包括QuickSort和压缩）表现的较X86平台要差；而在<strong>Reducer</strong>阶段（包括MergeSort和解压缩）ARM的表现较X86平台要更好；通过优化<strong>Mapper</strong>阶段的实现，ARM平台的性能有可能超越X86平台。在<strong>压缩</strong>环节，ARM耗时较X86落后的比较多，同时<code>ZSTD</code>是当前在ARM平台上表现的最好的压缩算法，与X86之间的性能差距最小。</p><p>除了Hadoop本身代码上的区别，我们还发现在ARM平台和X86平台上启动JVM的性能有所差距，这里我们单独编写了一份脚本来进行测试，在这个脚本中我们将启动<code>Mapper</code>和<code>Reducer</code>各1500个，然后退出，用于观察启动JVM的性能差距：</p><p><img src="https://user-images.githubusercontent.com/10849016/80352005-d0115680-88a5-11ea-81b8-d6ec532fb4b0.png" alt="image"></p><p>从结果可以看出，在这项对比中，ARM与X86相比也有15%-18%的性能差距，同样，CPU主频是否对该时间有影响并且有多大的影响也是需要进行进一步分析的。但是，从这个测试我们可以得到一个结论，那就是如果可以重用JVM，那么可以一定程度上缩小ARM和X86之间的性能差距。</p><h2 id="总结及后续工作"><a href="#总结及后续工作" class="headerlink" title="总结及后续工作"></a>总结及后续工作</h2><ol><li>总体来说，Apache Hadoop运行在ARM数据中心上与运行在X86数据中心上在性能上约有10%到20%的性能差距，将CPU主频差别考虑进去后，ARM与X86的整体性能差距基本在10%以内。</li><li>ARM在Mapper阶段性能较X86差，但是在Reducer阶段较X86性能好；更进一步来看，ARM在Mapper阶段的数据压缩阶段性能与X86相比差距较为明显，其中ZSTD的差距最小。</li><li>ARM和X86目前在启动JVM方面约有15%-18%的性能差距，如果可以在MR中重用JVM，那么可以一定程度上缩小ARM和X86之间的性能差距。</li><li>从性价比角度考虑，当前测试所使用的ARM集群成本仅为X86集群成本的70%，但性能差距只有10%-20%。可以预见，同等成本的ARM集群将能够提供与X86集群同等性能、甚至更好的性能，因此ARM数据中心在大数据领域还是十分有实用价值的。</li></ol><p>后续，我们将持续进行更为细致的测试，并且根据测试结果推动社区上游进行有针对性的改进，逐步缩小ARM与X86之间的性能差距。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参与者：&lt;a href=&quot;https://github.com/ayushtkn&quot;&gt;Ayush Saxena&lt;/a&gt;、&lt;a href=&quot;https://github.com/ZhengZhenyu&quot;&gt;郑振宇&lt;/a&gt;、&lt;a href=&quot;https://github.com/liusheng&quot;&gt;刘胜&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;背景：2019年，本团队在 Apache Hadoop 开源社区上游完成了 ARM 平台适配并使能了 Hadoop ARM CI 以便保证 Hadoop 在后续的版本开发中均保证 ARM 平台适配，并且团队成员正在推动 Hadoop 开源社区发布 ARM 平台的软件包，以便用户可以直接下载使用。在 ARM 的基本适配工作完成后，性能调优成了我们下一步的工作重心。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;近期，华为计算开源生态部技术团队进行了 Apache Hadoop 在 ARM 及 X86 平台上的初步性能比拼测试及简要分析，可以为关心 ARM 数据中心大数据场景的读者提供一些参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Building Linux packages for different CPU architectures with Docker and QEMU</title>
    <link href="https://kunpengcompute.github.io/2020/04/23/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu/"/>
    <id>https://kunpengcompute.github.io/2020/04/23/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu/</id>
    <published>2020-04-23T03:00:37.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Martin Grigorov</p><p>原文链接: <a href="https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5">https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5</a></p><p>Many Linux open source projects provide only source code releases. To be able to use them the users need to download the source code and to build it, usually by executing steps like: <code>./configure</code>,<code>make</code> and <code>make install</code>.</p><p>Some users prefer this way because they have the chance to configure the software by passing specific arguments to the <code>./configure</code> script. It is also the preferred way from security point of view — the person responsible for managing the system is certain that this is the original version of the source code and no one added anything on top.</p><a id="more"></a><p>Still many users prefer to download a binary package and install it, or to use the package management software of their favorite Linux distribution, e.g. yum for RedHat/CentOS/Fedora or apt for Debian/Ubuntu flavors. Here the benefit is that the dependencies are installed automatically for you.</p><p>Some of the open source projects provide binary packages for download themselves. Others delegate the packaging task to their community or to the Linux distributions to package the software following the best practices for the specific package type. They do this for different reasons but most often because:</p><p>1) it is an extra burden — the software developers do not want to deal with “bureaucracy” different their domain of expertise</p><p>2) there are many Linux distributions with their specific packaging types, e.g. .deb, .rpm, .apk, etc. One needs to read a lot of documentation to understand each of them</p><p>3) another reason is because one may need specific hardware to be able to build a package for not so common CPU architectures. Usually developers work on Intel or AMD based computers known as x86_64 CPU architecture. But if your software needs to run on mobile phones or tables and Internet of Things (IoT) devices then you need to produce a binary for ARM architecture, also known as AARCH. ARMv7 and before is 32-bit. ARMv8, also known as aarch64, is 64-bit. Lately even more and more cloud providers recommend ARM64 CPUs because they have similar performance to the x86_64 ones but consume less electricity, so they are cheaper to rent and environment friendlier.</p><p>In the rest of this article I’m going to show you how to build and package your software for ARM on x86_64 computer by using Docker and QEMU.</p><h2 id="What-is-Docker"><a href="#What-is-Docker" class="headerlink" title="What is Docker ?"></a>What is Docker ?</h2><p>From Wikipedia: Docker is a set of platform as a service (PaaS) products that uses OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than virtual machines.</p><h2 id="What-is-QEMU"><a href="#What-is-QEMU" class="headerlink" title="What is QEMU ?"></a>What is QEMU ?</h2><p>From Wikipedia: QEMU (short for Quick EMUlator) is a free and open-source emulator that performs hardware virtualization. QEMU is a hosted virtual machine monitor: it emulates the machine’s processor through dynamic binary translation and provides a set of different hardware and device models for the machine, enabling it to run a variety of guest operating systems. It also can be used with KVM to run virtual machines at near-native speed (by taking advantage of hardware extensions such as Intel VT-x). QEMU can also do emulation for user-level processes, allowing applications compiled for one architecture to run on another.</p><p>Most of the cloud based Continuous Integration (CI) providers (e.g. TravisCI, CircleCI, DroneCI, Github Actions, and more) use Docker to provide you with a throw-away Docker container (a Linux instance) which you can modify the way you need, for example by installing required dependencies of your software or by even changing kernel settings, and then to build/test/package your software. Once your CI job finishes the docker container is discarded and the resources freed for the new CI job. The jobs are fully isolated from each other and this makes your build reproducible because they always start from the same state and there is nothing left from a previous job.</p><h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>The process of building your software consists of two main steps:</p><h3 id="1-register-QEMU-binfmt"><a href="#1-register-QEMU-binfmt" class="headerlink" title="1) register QEMU/binfmt"></a>1) register QEMU/binfmt</h3><p>If you try to run a Docker container that is built for a different CPU architecture than the host’s it will fail with this error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm arm64v8&#x2F;centos:8 uname -m</span><br><span class="line">standard_init_linux.go:211: exec user process caused “exec format error”</span><br></pre></td></tr></table></figure><p>To be able to run such foreign architectures one may use QEMU! Someone made the installation step as simple as executing:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm — privileged multiarch&#x2F;qemu-user-static — credential yes — persistent yes</span><br></pre></td></tr></table></figure><p>What this does is:</p><p> 1.2) run a Docker container that modifies the host. If it is executed inside a Docker container then it will modify the outer container.<br> 1.3) The — privileged argument gives permissions to the Docker container to modify the host. In case it is run in a CI server then the host is the outer Docker container, the one allocated for our CI job.<br> 1.4) The — credential yes argument is needed only if you need to use sudo later in step 2).<br> 1.5) The — persistent yes argument tells it to load the interpreter when binfmt is configured and remains in memory. All future uses clone the interpreter from memory.</p><p>If the execution of the command above is successful you should see output similar to the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-alpha-static as binfmt interpreter for alpha</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static as binfmt interpreter for arm</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-armeb-static as binfmt interpreter for armeb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc-static as binfmt interpreter for sparc</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc32plus-static as binfmt interpreter for sparc32plus</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc64-static as binfmt interpreter for sparc64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc-static as binfmt interpreter for ppc</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc64-static as binfmt interpreter for ppc64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc64le-static as binfmt interpreter for ppc64le</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-m68k-static as binfmt interpreter for m68k</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips-static as binfmt interpreter for mips</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsel-static as binfmt interpreter for mipsel</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsn32-static as binfmt interpreter for mipsn32</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsn32el-static as binfmt interpreter for mipsn32el</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips64-static as binfmt interpreter for mips64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips64el-static as binfmt interpreter for mips64el</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sh4-static as binfmt interpreter for sh4</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sh4eb-static as binfmt interpreter for sh4eb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-s390x-static as binfmt interpreter for s390x</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-aarch64-static as binfmt interpreter for aarch64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-aarch64_be-static as binfmt interpreter for aarch64_be</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-hppa-static as binfmt interpreter for hppa</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-riscv32-static as binfmt interpreter for riscv32</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-riscv64-static as binfmt interpreter for riscv64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-xtensa-static as binfmt interpreter for xtensa</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-xtensaeb-static as binfmt interpreter for xtensaeb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-microblaze-static as binfmt interpreter for microblaze</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-microblazeel-static as binfmt interpreter for microblazeel</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-or1k-static as binfmt interpreter for or1k</span><br></pre></td></tr></table></figure><p>Now if we try to run the foreign Docker image it will succeed:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm arm64v8&#x2F;centos:8 uname -m</span><br><span class="line">aarch64</span><br></pre></td></tr></table></figure><p>The above tells us that uname -m executed inside arm64v8/centos:8 container returns that the CPU architecture is aarch64!</p><p>If you want to understand how QEMU/binfmt works you can read its documentation but it is not required to know more for the purpose of this article.</p><h3 id="2-Build-your-software"><a href="#2-Build-your-software" class="headerlink" title="2) Build your software"></a>2) Build your software</h3><p>All we need to do now is to run the usual build steps (e.g. ./configure, make, make test, etc.) inside the foreign Docker container.</p><p>2.1) Create a Dockerfile that uses as a base image any image with foreign architecture, like arm64v8/centos:8 above.<br>2.2) run the scripts</p><p>One can use Docker’s RUN commands, e.g.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN make</span><br><span class="line">RUN make test</span><br></pre></td></tr></table></figure><p>but this may get wild if you need to execute many steps!</p><p>I prefer to put all these commands in a Shell script, copy it to the custom Docker image and finally execute it.</p><p>The script may look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env bash</span><br><span class="line">apt install -y dependency1 dependency2 dependencyN</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make test</span><br></pre></td></tr></table></figure><p>The Dockerfile will look something like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM arm64v8&#x2F;centos:8</span><br><span class="line">ADD build-test-and-package.sh &#x2F;usr&#x2F;bin</span><br><span class="line">CMD [“build-test-and-package.sh”]</span><br></pre></td></tr></table></figure><h3 id="2-3-Build-the-custom-image"><a href="#2-3-Build-the-custom-image" class="headerlink" title="2.3) Build the custom image"></a>2.3) Build the custom image</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t my-arm-centos:8 .</span><br></pre></td></tr></table></figure><h3 id="2-4-Run-it"><a href="#2-4-Run-it" class="headerlink" title="2.4) Run it"></a>2.4) Run it</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run — rm -it -v $(pwd):&#x2F;my-software my-arm-centos:8</span><br></pre></td></tr></table></figure><p>Here we mount the current folder into /my-software folder inside the Docker container. build-test-and-package.sh needs to know this location to cd into it!</p><p>Any result files, like the binary packages, could be saved in this folder or another mounted folder so that they can be consumed at the end of the CI workflow, e.g. to store them as artifacts of the build and copy them to AWS S3 or elsewhere.</p><h2 id="In-action"><a href="#In-action" class="headerlink" title="In action"></a>In action</h2><p>You can see all this in action at Varnish Cache GitHub repository.</p><p>It is a Pull Request suggesting to build, test and package Varnish Cache for CentOS 7 &amp; 8, Ubuntu 16.04 &amp; 18.04, Debian 8, 9 &amp; 10, and Alpine 3, for both x86_64 and aarch64.</p><p>Expending it for more distros, versions or CPU architectures is as easy as adding an additional CircleCI job with the proper parameters.</p><p>The build graph looks like:<br><img src="https://user-images.githubusercontent.com/1736354/80073339-33377c00-857a-11ea-869e-4abbc3949086.png" alt="graph"></p><p>The dist and tar_pkg_tools jobs run first in parallel. The dist job packages the source distribution and tar_pkg_tools gets the packaging recipes for RPM, DEB and APK from here. Those are stored in CircleCI’s workflow workspace and made available for any following jobs.</p><p>Once both of them finish the next jobs that run in parallel are the distcheck and the package jobs. distcheck jobs build Varnish Cache on different distros and the package jobs build the respective binary packages for each arch/distro/version triple.</p><p>If everything is successful finally the collect_packages job exports all binary packages as CircleCI artifacts which are later copied to Package Cloud.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Using stable tools like Docker and QEMU makes it easier to build and test our software for different CPU architectures.</p><p>There are few drawbacks though:</p><p>1) it is an emulation of the foreign CPU architecture, so it is slower than doing it on a real hardware<br><strong>Note:</strong> Some cloud CI services like TravisCI and DroneIO provide support for ARM/ARM64. I have experience only with TravisCI and it is not faster than QEMU. I’ve had some small issues with it but it was easy to work them around. Hopefully it will become even better in the near future!</p><p>2) you need to find a good base Docker image for the CPU architecture you need to support. There are many images at DockerHub but depending on how exotic your needs are it may be harder to find one.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Martin Grigorov&lt;/p&gt;
&lt;p&gt;原文链接: &lt;a href=&quot;https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5&quot;&gt;https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Many Linux open source projects provide only source code releases. To be able to use them the users need to download the source code and to build it, usually by executing steps like: &lt;code&gt;./configure&lt;/code&gt;,&lt;code&gt;make&lt;/code&gt; and &lt;code&gt;make install&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Some users prefer this way because they have the chance to configure the software by passing specific arguments to the &lt;code&gt;./configure&lt;/code&gt; script. It is also the preferred way from security point of view — the person responsible for managing the system is certain that this is the original version of the source code and no one added anything on top.&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="Packaging" scheme="https://kunpengcompute.github.io/tags/Packaging/"/>
    
  </entry>
  
  <entry>
    <title>ARM上跑Mysql, 能行吗？</title>
    <link href="https://kunpengcompute.github.io/2020/04/22/arm-shang-pao-mysql-neng-xing-ma/"/>
    <id>https://kunpengcompute.github.io/2020/04/22/arm-shang-pao-mysql-neng-xing-ma/</id>
    <published>2020-04-22T05:00:46.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>作者: Krunal Bauskar </p><p>原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-ARM/">https://mysqlonarm.github.io/Running-MySQL-on-ARM/</a></p><p>我相信在座的大多数都会有这个问题。事实上，在我开始主动开始#mysqlonarm 相关工作之前，我也有过同样的经历。在ARM上运行MySQL需要什么？真的有用吗？包依赖关系呢？它有什么样的表现？支持什么功能呢？社区支持这么玩吗？还有很多悬而未决的问题。。</p><p>让我们试着用简单的问答形式来回答这些问题。</p><a id="more"></a><h4 id="Q-Mysql支持ARM架构吗"><a href="#Q-Mysql支持ARM架构吗" class="headerlink" title="Q: Mysql支持ARM架构吗?"></a>Q: Mysql支持ARM架构吗?</h4><p>A:支持，Mysql官方支持ARM。可以从mysql.com站点下载可用的软件包。</p><h4 id="Q-支持啥OS"><a href="#Q-支持啥OS" class="headerlink" title="Q: 支持啥OS?"></a>Q: 支持啥OS?</h4><p>A: 目前支持RHEL-7 &amp; 8/Oracle-Linux- 7 &amp; 8，还没有发现发布其他OS的包。</p><h4 id="Q-我们能在ARM架构下从源代码在OS上-如ubuntu-构建Mysql吗"><a href="#Q-我们能在ARM架构下从源代码在OS上-如ubuntu-构建Mysql吗" class="headerlink" title="Q: 我们能在ARM架构下从源代码在OS上(如ubuntu)构建Mysql吗?"></a>Q: 我们能在ARM架构下从源代码在OS上(如ubuntu)构建Mysql吗?</h4><p>A: 可以，能玩。我一直在源码构建二进制包，使用的是当前mysql的release tag mysql-8.0.19。同样也可以在CentOS上玩。这也意味着所有需要的包依赖性问题都得到了解决，或已经可用。</p><h4 id="Q-ARM上的工具链可用吗"><a href="#Q-ARM上的工具链可用吗" class="headerlink" title="Q: ARM上的工具链可用吗?"></a>Q: ARM上的工具链可用吗?</h4><p>A: 因为软件包是可用的，而且我能够从源代码构建它，所以默认的应用程序工具，如mysql shell/mysqladmin/mysqlslap/mysqldump/etc…，以及大量其他的默认程序都随二进制文件一起发布了。如果你关心某个特定的工具，告诉我，我会检查它们。现在，我尝试了一些我认为比较重要的工具，它们工作正常。</p><h4 id="Q-MariaDB和Percona在ARM上是否支持各自的服务器规格"><a href="#Q-MariaDB和Percona在ARM上是否支持各自的服务器规格" class="headerlink" title="Q: MariaDB和Percona在ARM上是否支持各自的服务器规格?"></a>Q: MariaDB和Percona在ARM上是否支持各自的服务器规格?</h4><p>A: MariaDB Community Server软件包（来自MariaDB公司）在ARM (CentOS7/Ubuntu-16.04/18.04)上可用，MariaDB服务器工具在ARM上暂未正式发布。 Percona尚未正式支持ARM ，但我能够从源代码构建它（ MyRocks / TokuDB不可用）。</p><h4 id="Q-如果工具不可用。这能阻止我在ARM上尝试MySQL-或其衍生软件-吗？"><a href="#Q-如果工具不可用。这能阻止我在ARM上尝试MySQL-或其衍生软件-吗？" class="headerlink" title="Q: 如果工具不可用。这能阻止我在ARM上尝试MySQL(或其衍生软件)吗？"></a>Q: 如果工具不可用。这能阻止我在ARM上尝试MySQL(或其衍生软件)吗？</h4><p>A: 不会，因为大多数工具都是符合mysql标准的，不是所谓的强绑定特定平台。所以你当然既可以把它们安装在X86，同时安装在ARM上(除非工具还没有移植到ARM上)。</p><h4 id="Q-社区支持这么玩吗"><a href="#Q-社区支持这么玩吗" class="headerlink" title="Q: 社区支持这么玩吗?"></a>Q: 社区支持这么玩吗?</h4><p>A: MySQL on ARM 已经有一段时间了。ARM、高通、华为等多家厂商积极贡献，Mysql社区发展迅速。在对Mysql ARM优化方面，社区呈现很大的兴趣，非常多的开发者想参与其中。挑战极具吸引力，然而最麻烦的是ARM硬件资源短缺。如果你有兴趣参与，和我聊聊（给我发一封电子邮件）。</p><h4 id="Q-看起来都还行。性能咋样"><a href="#Q-看起来都还行。性能咋样" class="headerlink" title="Q: 看起来都还行。性能咋样?"></a>Q: 看起来都还行。性能咋样?</h4><p>A: 这是一个大话题，所以我会在未来几天内发布多个关于这个话题的博文，但把它在一定范围内的表现是可比的。另一方面，ARM实例提供更好的价格比。</p><h4 id="Q-可以从哪里获取帮助"><a href="#Q-可以从哪里获取帮助" class="headerlink" title="Q:可以从哪里获取帮助?"></a>Q:可以从哪里获取帮助?</h4><p>由于软件包可以从MySQL官方下载，我假定他们的服务提供也应涵盖ARM。和MariaDB一样。当然，除了官方支持之外，还有普通团体和独立开发者。</p><h4 id="Command-to-build-MySQL-on-ARM"><a href="#Command-to-build-MySQL-on-ARM" class="headerlink" title="Command to build MySQL on ARM"></a>Command to build MySQL on ARM</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake .. -DWITH_NUMA&#x3D;1 -DDOWNLOAD_BOOST&#x3D;1 -DWITH_BOOST&#x3D;&lt;boost-dir&gt; -DCMAKE_INSTALL_PREFIX&#x3D;&lt;dir-to-install&gt;</span><br><span class="line">make -j &lt;num-of-cores&gt;</span><br></pre></td></tr></table></figure><p>因此，在ARM上构建Mysql不需要什么特殊的flag。(假设你已经安装了标准依赖). 它会默认使用”CMAKE_BUILD_TYPE=RelWithDebInfo”来编译。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>MySQL on ARM已为事实，并且得到了日益增长的生态系统/社区来支持。可以来试试。当你不考虑性能或功能时，它可以成为你节约成本这个大目标的必选项。</p><p><em>如果你还有问题/疑问，请告诉我。我会尝试回答</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar &lt;/p&gt;
&lt;p&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我相信在座的大多数都会有这个问题。事实上，在我开始主动开始#mysqlonarm 相关工作之前，我也有过同样的经历。在ARM上运行MySQL需要什么？真的有用吗？包依赖关系呢？它有什么样的表现？支持什么功能呢？社区支持这么玩吗？还有很多悬而未决的问题。。&lt;/p&gt;
&lt;p&gt;让我们试着用简单的问答形式来回答这些问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM vs X86 Mysql性能大比拼</title>
    <link href="https://kunpengcompute.github.io/2020/04/22/arm-vs-x86-mysql-xing-neng-da-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/04/22/arm-vs-x86-mysql-xing-neng-da-bi-pin/</id>
    <published>2020-04-22T04:25:50.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>原文链接：<a href="https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/">https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/</a></p><p>作者: Krunal Bauskar</p><p>在开始探索这一领域之前，我相信大家对这个话题都会非常感兴趣，包括本人在内。另外，在深入讨论具体的数据前，让我们先来了解一下两种架构之间的基本差异。除了CISC和RISC之外，让我们从Mysql的角度来看待这些重要的差异。</p><ul><li>强 vs  弱内存模型 (弱内存模型在无锁写场景下需要适当的内存屏障).</li><li>底层硬件特定专用指令。例如：两者现在都支持crc32c硬件指令，但是底层调用它们的方式不同。更多指令差异请参看x86-SSE/ARM-ACLE。</li><li>Cache Line 差异。 大多数ARM处理器倾向于使用更大的cacheline size (所有缓存以128 bytes为单位或者 64/128字节混合的方式)。</li><li>其他系统调用级别的差异，如：ARM的PAUSE指令缺失和具有非常低延迟的替代指令无法引导硬件达到所需的延迟，sched_getcpu在ARM上引入了使用无锁构造的挑战，内存操作似乎带来更高的延迟等。</li></ul><p>Mysql社区已经为这个领域的问题贡献了多个补丁（另一个博客的主题）。自从MySQL刚刚宣称开始支持ARM架构以来，几乎没有什么优化，大部分工作仍然没有完成。</p><a id="more"></a><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>现在让我们来看看最重要的方面：性能</p><p>我们在x86和ARM上测试了MySQL（当前版本8.0.19）的性能。试验步骤和机器详情如下。</p><h3 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h3><ul><li>24核/48GB Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz，用于在x86上运行MySQL数据库软件。</li><li>24核/48GB ARM @ 2.60GHz，用于在ARM上运行MySQL</li><li>sysbench运行在一台专用的机器，且位于相同的数据中心。</li><li>sysbench步骤:<ul><li>加载测试表. (相同的数据库被多次运行重用，所以需要预热).</li><li>Checksum预热。对所有表执行checksum。对于checksum,流程中需要获取bufferpool中的行记录。</li><li>Query 预热。 如果您使用的是自适应哈希索引，可以跳过此节，但它确实有些效果。</li><li>执行测试套件 (oltp-read-write/oltp-update-index/oltp-update-non-index/oltp-read-only/oltp-point-select)</li><li>每个测试套件以多种不同的扩展性来执行。如对于24 vCPU，尝试多threads 1/2/4/8/16/32/128/256.</li><li>在切换测试套件中间，引入一些睡眠操作确保之前的测试套件完全运行完毕。虽然这不能保证所有的数据库刷新都完成了，但是X秒的睡眠可以保证对后续测试套件的影响最小。</li><li>MySQL-Server Configuration:<ul><li>足够大的BP(bufferpool) ，以保证可以存储完整的数据</li><li>了解更多配置详情，参看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">以下配置</a></li></ul></li></ul></li></ul><p>运行脚本和调用sysbench的自动化测试脚本细节在 <a href="https://github.com/mysqlonarm/benchmark-suites">这里</a></p><h3 id="测试运行具体细节"><a href="#测试运行具体细节" class="headerlink" title="测试运行具体细节:"></a>测试运行具体细节:</h3><ul><li>Table: 96-tables * 150万数据 (data-size= 34GB)</li><li>Buffer Pool: 36GB</li><li>Redo-Log: 4GB*2</li><li>TC-run-time: 300 secs</li><li>TC-warmup: 60 (sysbench –warmup-time)</li><li>workload-query-based warmup: 600</li><li>change-over-sleep: 180</li><li>checksum-based-warmup: enabled</li><li>data-storage: 300GB (支持16500 IOPS(对Burst IOPS无效果).)</li></ul><p><em>注: Frequency Scaling (FS)频率缩放. 所述ARM 主频规格2.6 GHz vs x86主频规格3.0 GHz。直接比较它们的数据是不公平的。为了补偿频率差，下面的图还为ARM添加了频率调整 tps/qps（ARM-fscaled简单地按(3/2.6）的算法基于原始数据算出ARM tps/qps数据)。在现实生活中，考虑到CPU频率的增加会影响争用曲线图和等待周期，这个影响因素可能会稍微高一些。</em></p><hr><h3 id="1-Point-Select"><a href="#1-Point-Select" class="headerlink" title="1. Point Select:"></a>1. Point Select:</h3><p><a href="https://camo.githubusercontent.com/929493df33b60738e09eee164e57181f3945e935/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d70732e706e67"><img src="https://camo.githubusercontent.com/929493df33b60738e09eee164e57181f3945e935/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d70732e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>6696</td><td>6439</td><td>7726</td><td>4</td><td>20</td></tr><tr><td>2</td><td>12482</td><td>11774</td><td>14402</td><td>6</td><td>22</td></tr><tr><td>4</td><td>23881</td><td>21308</td><td>27555</td><td>12</td><td>29</td></tr><tr><td>8</td><td>45993</td><td>42110</td><td>53069</td><td>9</td><td>26</td></tr><tr><td>16</td><td>88517</td><td>81239</td><td>102135</td><td>9</td><td>26</td></tr><tr><td>32</td><td>142974</td><td>136724</td><td>164970</td><td>5</td><td>21</td></tr><tr><td>64</td><td>198839</td><td>212484</td><td>229430</td><td>-6</td><td>8</td></tr><tr><td>128</td><td>217778</td><td>241555</td><td>251282</td><td>-10</td><td>4</td></tr><tr><td>256</td><td>209797</td><td>224009</td><td>242073</td><td>-6</td><td>8</td></tr></tbody></table><h4 id="分析"><a href="#分析" class="headerlink" title="分析:"></a>分析:</h4><ul><li>ARM在较低扩展性上性能比X86要好，但是在扩展到与cpu数目相近的threads时，性能就开始下降了</li><li>在频率缩放下，尽管存在扩展性问题，ARM仍然击败X86</li></ul><hr><h3 id="2-Read-Only"><a href="#2-Read-Only" class="headerlink" title="2. Read Only:"></a>2. Read Only:</h3><p><a href="https://camo.githubusercontent.com/e5b5e8d60dd3a1e636246b3d11f9e19602c755ce/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d726f2e706e67"><img src="https://camo.githubusercontent.com/e5b5e8d60dd3a1e636246b3d11f9e19602c755ce/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d726f2e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>5222</td><td>5259</td><td>6025</td><td>-1</td><td>15</td></tr><tr><td>2</td><td>10333</td><td>10200</td><td>11923</td><td>1</td><td>17</td></tr><tr><td>4</td><td>19176</td><td>19349</td><td>22126</td><td>-1</td><td>14</td></tr><tr><td>8</td><td>36881</td><td>37035</td><td>42555</td><td>0</td><td>15</td></tr><tr><td>16</td><td>70337</td><td>67065</td><td>81158</td><td>5</td><td>21</td></tr><tr><td>32</td><td>109207</td><td>113210</td><td>126008</td><td>-4</td><td>11</td></tr><tr><td>64</td><td>139294</td><td>164148</td><td>160724</td><td>-15</td><td>-2</td></tr><tr><td>128</td><td>151382</td><td>175872</td><td>174672</td><td>-14</td><td>-1</td></tr><tr><td>256</td><td>149136</td><td>164382</td><td>172080</td><td>-9</td><td>5</td></tr></tbody></table><h4 id="分析-1"><a href="#分析-1" class="headerlink" title="分析:"></a>分析:</h4><ul><li>在低扩展性上，ARM与X86基本持平，在较高扩展性败于X86</li><li>应用频率缩放，ARM在大多数场景下继续击败X86</li></ul><hr><h3 id="3-Read-Write"><a href="#3-Read-Write" class="headerlink" title="3. Read Write:"></a>3. Read Write:</h3><p><a href="https://camo.githubusercontent.com/252ecf14b5b65667b2cfece150fdf9b7e42d36ca/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d72772e706e67"><img src="https://camo.githubusercontent.com/252ecf14b5b65667b2cfece150fdf9b7e42d36ca/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d72772e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>137</td><td>149</td><td>158</td><td>-8</td><td>6</td></tr><tr><td>2</td><td>251</td><td>273</td><td>290</td><td>-8</td><td>6</td></tr><tr><td>4</td><td>462</td><td>502</td><td>533</td><td>-8</td><td>6</td></tr><tr><td>8</td><td>852</td><td>920</td><td>983</td><td>-7</td><td>7</td></tr><tr><td>16</td><td>1539</td><td>1678</td><td>1776</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>2556</td><td>2906</td><td>2949</td><td>-12</td><td>1</td></tr><tr><td>64</td><td>3770</td><td>5158</td><td>4350</td><td>-27</td><td>-16</td></tr><tr><td>128</td><td>5015</td><td>8131</td><td>5787</td><td>-38</td><td>-29</td></tr><tr><td>256</td><td>5676</td><td>8562</td><td>6549</td><td>-34</td><td>-24</td></tr></tbody></table><h4 id="分析-2"><a href="#分析-2" class="headerlink" title="分析:"></a>分析:</h4><ul><li>在read-write测试套件中情况有些不同。ARM开始落后了。在低扩展性情况下，频率缩放会减缓这种落后，在高扩展性则持续增大了差距。</li></ul><hr><h3 id="4-Update-Index"><a href="#4-Update-Index" class="headerlink" title="4. Update Index:"></a>4. Update Index:</h3><p><a href="https://camo.githubusercontent.com/114ccd28ff574e5becf7723a62f24a0e2e599ef4/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d75692e706e67"><img src="https://camo.githubusercontent.com/114ccd28ff574e5becf7723a62f24a0e2e599ef4/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d75692e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>623</td><td>768</td><td>719</td><td>-19</td><td>-6</td></tr><tr><td>4</td><td>1060</td><td>1148</td><td>1223</td><td>-8</td><td>7</td></tr><tr><td>8</td><td>1905</td><td>2028</td><td>2198</td><td>-6</td><td>8</td></tr><tr><td>16</td><td>3284</td><td>3590</td><td>3789</td><td>-9</td><td>6</td></tr><tr><td>32</td><td>5543</td><td>6275</td><td>6396</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9138</td><td>10381</td><td>10544</td><td>-12</td><td>2</td></tr><tr><td>128</td><td>13879</td><td>16868</td><td>16014</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>19954</td><td>25459</td><td>23024</td><td>-22</td><td>-10</td></tr></tbody></table><h4 id="分析-3"><a href="#分析-3" class="headerlink" title="分析:"></a>分析:</h4><ul><li>频率缩放下，ARM继续与X86处于同等/更好的状态 (除了高竞争用例)</li></ul><hr><h3 id="5-Update-Non-Index"><a href="#5-Update-Non-Index" class="headerlink" title="5. Update Non-Index:"></a>5. Update Non-Index:</h3><p><a href="https://camo.githubusercontent.com/6d578230e925ce5afdae131d46ce484cfb647e1b/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d756e692e706e67"><img src="https://camo.githubusercontent.com/6d578230e925ce5afdae131d46ce484cfb647e1b/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d756e692e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>588</td><td>686</td><td>678</td><td>-14</td><td>-1</td></tr><tr><td>4</td><td>1075</td><td>1118</td><td>1240</td><td>-4</td><td>11</td></tr><tr><td>8</td><td>1941</td><td>2043</td><td>2240</td><td>-5</td><td>10</td></tr><tr><td>16</td><td>3367</td><td>3662</td><td>3885</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>5681</td><td>6438</td><td>6555</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9328</td><td>10631</td><td>10763</td><td>-12</td><td>1</td></tr><tr><td>128</td><td>14158</td><td>17245</td><td>16336</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>20377</td><td>26367</td><td>23512</td><td>-23</td><td>-11</td></tr></tbody></table><h4 id="分析-4"><a href="#分析-4" class="headerlink" title="分析:"></a>分析:</h4><ul><li>频率缩放下，ARM继续与X86处于同等/更好的状态 (除了高竞争用例)</li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>基于以上的数据，我们能够得到如下结论:</p><ul><li>在只读场景中测试Mysql, ARM和X86性能几乎一致。</li><li>在涉及写场景中测试Mysql，ARM开始有些落后，但是如果我们考虑频率缩放，情况可能会好一点。</li><li>在生活里，我们通常不会考虑频率缩放，而会考虑他们的性价比。这是一个话题，但通常是事实：ARM实例比X86实例便宜34%（我们在测试中用相同的flavor 24U48G）。</li><li>有一点我们需要持续观察，就是， ARM工作负载在达到CPU限制之前具有很好的可伸缩性。随着可伸缩性的增强，争用增加，ARM开始滞后。这是因为互斥/竞争热点都是针对x86调优的（例如：自旋锁 spin-lock）。现在MySQL正式支持ARM，并且ARM的社区和来自各地开发者的兴趣也不断增长，所以Mysql社区也会针对ARM进行调优，让我们拭目以待把。</li></ul><p>总之，Mysql on ARM是一个值得从成本和性能角度来尝试的选择。</p><p><em>如果你还有问题/疑问，请告诉我。我会试着去回答。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/&quot;&gt;https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;在开始探索这一领域之前，我相信大家对这个话题都会非常感兴趣，包括本人在内。另外，在深入讨论具体的数据前，让我们先来了解一下两种架构之间的基本差异。除了CISC和RISC之外，让我们从Mysql的角度来看待这些重要的差异。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强 vs  弱内存模型 (弱内存模型在无锁写场景下需要适当的内存屏障).&lt;/li&gt;
&lt;li&gt;底层硬件特定专用指令。例如：两者现在都支持crc32c硬件指令，但是底层调用它们的方式不同。更多指令差异请参看x86-SSE/ARM-ACLE。&lt;/li&gt;
&lt;li&gt;Cache Line 差异。 大多数ARM处理器倾向于使用更大的cacheline size (所有缓存以128 bytes为单位或者 64/128字节混合的方式)。&lt;/li&gt;
&lt;li&gt;其他系统调用级别的差异，如：ARM的PAUSE指令缺失和具有非常低延迟的替代指令无法引导硬件达到所需的延迟，sched_getcpu在ARM上引入了使用无锁构造的挑战，内存操作似乎带来更高的延迟等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mysql社区已经为这个领域的问题贡献了多个补丁（另一个博客的主题）。自从MySQL刚刚宣称开始支持ARM架构以来，几乎没有什么优化，大部分工作仍然没有完成。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>源于鲲鹏，回归社区：GNU Glibc的ARM优化小记</title>
    <link href="https://kunpengcompute.github.io/2020/04/17/yuan-yu-kun-peng-hui-gui-she-qu-gnu-glibc-de-arm-you-hua-xiao-ji/"/>
    <id>https://kunpengcompute.github.io/2020/04/17/yuan-yu-kun-peng-hui-gui-she-qu-gnu-glibc-de-arm-you-hua-xiao-ji/</id>
    <published>2020-04-17T03:44:54.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者：姜逸坤 张学磊</p><p>从2019年10月初开始，我们团队开始着手Glibc在aarch64(64)架构下的优化工作，并且在2019年年底，将我们的全部优化贡献给上游开源社区。本文分享我们在Glibc的版本完成的优化以及性能测试结果，同时我们也尝试着将优化的思路进行总结，希望对其他项目的优化提供一些思路。</p><a id="more"></a><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><h3 id="1-1-什么是Glibc"><a href="#1-1-什么是Glibc" class="headerlink" title="1.1 什么是Glibc?"></a>1.1 什么是Glibc?</h3><p>我们先看看官方的解释：</p><blockquote><p>The GNU C Library project provides the core libraries for the GNU system and GNU/Linux systems, as well as many other systems that use Linux as the kernel.</p></blockquote><p>Glibc的全名是The GNU C Library，它为GNU系统、GNU/Linux系统以及提供了核心的底层库。比如，我们平常使用的<code>memset</code>，<code>strlen</code>等等这些非常常用的接口都由这个库提供。</p><h3 id="1-2-为什么要优化？"><a href="#1-2-为什么要优化？" class="headerlink" title="1.2 为什么要优化？"></a>1.2 为什么要优化？</h3><p>在计算领域的水平场景，例如大数据、数据库、Web等领域都直接或者间接地依赖着Glibc，举个简单的例子，在数据库的代码中，我们经常使用memcpy接口，对变量进行复制，调用频次也异常的高。如果在数据复制的过程中，性能能够有所提升，那么对上层软件的性能提升也是显而易见的。</p><h3 id="1-3-做了什么优化？"><a href="#1-3-做了什么优化？" class="headerlink" title="1.3 做了什么优化？"></a>1.3 做了什么优化？</h3><p>根据我们的分析，字符、内存和锁操作是最基础也是最重要的基本接口，因此，我们选择了对这三种类型的接口优先进行优化。在实现优化中，我们利用了Glibc的<a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">indirect function</a>这一机制，即会根据CPU、CPU arch去自动选择匹配的函数。这一机制让我们的实现，更加灵活，也对现有系统影响最小。</p><p>下图为我们这次优化主要接口：</p><p><img src="https://user-images.githubusercontent.com/1736354/82533405-c5c93c00-9b75-11ea-8c71-f37f557febee.png" alt="image"></p><p>在上游社区的推进过程中，我们始终坚持<strong>Upstream First</strong>的原则，希望能够将鲲鹏优化的收益共享给整个生态，真正做到<strong>源于鲲鹏，回归社区</strong>。</p><p>所以，可以看到我们的优化大部分（橙色部分）都贡献到了上游社区的AArch64的generic实现中，从而使得整个生态都能够受益，而小部分（绿色部分）针对于Kunpeng CPU的特殊优化则保持了单独实现。</p><h2 id="2-优化"><a href="#2-优化" class="headerlink" title="2. 优化"></a>2. 优化</h2><p>我们知道在一般的开发中，小字节数据操作的使用频率，是远远的大于大字节数据操作的使用频率，而对于大数据和数据库的场景，则有可能会出现很多大字节数据操作的使用。因此，其实我们的一个优化原则是：<strong>在保证中小字节没有负优化的前提下，提升大字节数据操作的性能</strong>。</p><p>本节我们将一一解析在我们贡献的过程中，每个接口优化的关键点，并且尽可能的写的通俗易懂，希望能通过这些干货，给大家在其他的优化中带来启发。</p><h3 id="2-1-memcmp，每次做更多，总时间更少"><a href="#2-1-memcmp，每次做更多，总时间更少" class="headerlink" title="2.1 memcmp，每次做更多，总时间更少"></a>2.1 memcmp，每次做更多，总时间更少</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1182191/">aarch64: Optimized implementation of memcmp</a></p><h4 id="2-1-1-优化思路"><a href="#2-1-1-优化思路" class="headerlink" title="2.1.1 优化思路"></a>2.1.1 优化思路</h4><p>对于memcmp的优化，我们的核心思路是通过<strong>循环展开</strong>让每个周期内做的事情更多，从而减少循环本身的开销。下图可以直观的看出，循环展开带来的性能提升：<br><img src="https://user-images.githubusercontent.com/1736354/82417438-212ff700-9aae-11ea-941b-ee98d33a9df6.png" alt="image"><br>具体如下：</p><ol><li>扩展循环间隔长度<br>memcmp的aarch64原实现是以16bit的长度作为循环的周期长度，在无形中增加了很多次循环的消耗，尤其是在进行大字节数据比较中，有较大的性能损失。因此，我们这次优化的核心思路是：<strong>将16bit的循环扩展的64bit的循环</strong>，简单的说就是现在一次循环会比较64bit的数据。</li><li>寻址方式优化<br>除此之外，我们还改变了LDP的寻址方式，从原来的后变址寻址（Post Index Addressing）变成了偏移寻址（Base Plus index）。</li></ol><h4 id="2-1-2-性能测试"><a href="#2-1-2-性能测试" class="headerlink" title="2.1.2 性能测试"></a>2.1.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/79631884-7f577a80-818e-11ea-8dba-b5d4d92718e1.png" alt="image"></p><p>可以从我们实际的测试结果看到整体在中大字节的性能有不错的提升，尤其是在128字节以上的场景，性能提升更是达到了18%。</p><h3 id="2-2-memcpy，他山之石，可以为玉"><a href="#2-2-memcpy，他山之石，可以为玉" class="headerlink" title="2.2 memcpy，他山之石，可以为玉"></a>2.2 memcpy，他山之石，可以为玉</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1215732/">add default memcpy version for kunpeng920</a><br>memcpy优化，因为社区的falkor版本在大、小字节的性能表现，已经很完善，因此最终，我们直接使用了Flakor版本作为优化版本。</p><p>Falkor版本的将字符分为3种场景：</p><ol><li>对于small(&lt; 32)的场景，优先处理，避免过多判断，影响性能。</li><li>对于medium(33-128)的场景，做展开，避免多次循环带来的性能损失。</li><li>对于large(&gt;128)的场景，4字节对齐处理，并做循环展开每次循环处理64字节。</li></ol><p>有兴趣的可以看看源码的实现<a href="http://patchwork.ozlabs.org/project/glibc/patch/1502134812-31816-1-git-send-email-siddhesh@sourceware.org/">链接</a>，整体性能提升13-18%。</p><h3 id="2-3-memrchr，站在巨人的肩上"><a href="#2-3-memrchr，站在巨人的肩上" class="headerlink" title="2.3 memrchr，站在巨人的肩上"></a>2.3 memrchr，站在巨人的肩上</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1178706/">aarch64: Optimized implementation of memrchr</a></p><h4 id="2-3-1-优化思路"><a href="#2-3-1-优化思路" class="headerlink" title="2.3.1 优化思路"></a>2.3.1 优化思路</h4><p>memrchr整体的优化思路是，参考memchr设计的魔鬼数字算法，通过汇编实现逻辑适配，实现对特定字符逆向查找的功能，替代原有的C语言实现方案达到优化，具体实现见上链接。</p><h4 id="2-3-2-性能测试"><a href="#2-3-2-性能测试" class="headerlink" title="2.3.2 性能测试"></a>2.3.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/82403228-ed92a400-9a90-11ea-9a35-9ec4899cc9ea.png" alt="image"><br>最终，我们获得了58%的性能提升，最终在大字节的场景，比generic版本提升了4倍左右。</p><h3 id="2-4-memset，定向优化，更懂硬件"><a href="#2-4-memset，定向优化，更懂硬件" class="headerlink" title="2.4 memset，定向优化，更懂硬件"></a>2.4 memset，定向优化，更懂硬件</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1188834/">aarch64: Optimized memset for Kunpeng processor. </a></p><h4 id="2-4-1-优化思路"><a href="#2-4-1-优化思路" class="headerlink" title="2.4.1 优化思路"></a>2.4.1 优化思路</h4><p>我们进行了通过循环展开和特殊的定制优化来更好的适配硬件分支预测的特性，从而达到优化的效果。</p><p>特别说明的是，对于memset来说，置零场景是非常常用的场景，我们发现原有的实现使用DZ_ZVA指令并未在置零场景有显著效果，反而增加了许多条件分支，因此我们使用set_long代替了置零，由于set_long本身有更少的分支及更少的预测，所以性能与原实现比也有所提升。</p><h4 id="2-4-2-性能测试"><a href="#2-4-2-性能测试" class="headerlink" title="2.4.2 性能测试"></a>2.4.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/82403298-19158e80-9a91-11ea-8b50-9542edcc15ca.png" alt="image"></p><h3 id="2-5-strcpy，加速的武器，vector-loads"><a href="#2-5-strcpy，加速的武器，vector-loads" class="headerlink" title="2.5 strcpy，加速的武器，vector loads"></a>2.5 strcpy，加速的武器，vector loads</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1181183/">aarch64: Optimized implementation of strcpy</a><br>strlen使用了neon寄存器，通过vector operations对函数进行了优化，对比原有的汇编实现，在64字节以上的场景，获得了5%-18%的提升：</p><p><img src="https://user-images.githubusercontent.com/1736354/82403315-2763aa80-9a91-11ea-82f4-441c90b52962.png" alt="image"></p><h3 id="2-6-strlen-strnlen-循环展开，判断更少，性能更优"><a href="#2-6-strlen-strnlen-循环展开，判断更少，性能更优" class="headerlink" title="2.6 strlen/strnlen 循环展开，判断更少，性能更优"></a>2.6 strlen/strnlen 循环展开，判断更少，性能更优</h3><p>strlen Patch链接：<a href="https://patches-gcc.linaro.org/patch/25209/">aarch64: Optimized strlen for strlen_asimd</a><br>strnlen Patch链接：<a href="http://patchwork.ozlabs.org/patch/1181184/">aarch64: Optimized implementation of strnlen</a></p><p>strlen和strnlen同样使用了vector operations和循环展开，对主循环仅行了改造</p><p>strlen有7%-18%的提升:<br><img src="https://user-images.githubusercontent.com/1736354/82403331-321e3f80-9a91-11ea-9703-ab92470b2178.png" alt="image"></p><p>strnlen有11%-24%的提升:<br><img src="https://user-images.githubusercontent.com/1736354/82403342-39dde400-9a91-11ea-9fc8-5f68df97887b.png" alt="image"></p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>经过上面的介绍，相信大家已经了解了我们是怎么去优化这些函数的版本的，虽说大部分的优化都是比较晦涩的汇编语言，但是其实实际原理还是非常易懂的。</p><p>最后，我们再总结下我们应该从哪些方面考虑，去完成优化：</p><ul><li>使用Neon汇编指令提高指令速度</li><li>使用Prefetch机制充分利用cache</li><li>避免非对齐的内存访问</li><li>指令重排，减少数据依赖</li><li>循环展开，减少高频判断</li><li>结合硬件特性，用软件补齐硬件缺陷</li></ul><h2 id="4-写在最后"><a href="#4-写在最后" class="headerlink" title="4. 写在最后"></a>4. 写在最后</h2><p>本书所提及的所有代码，均已贡献到Glibc上游社区，并且随着<a href="https://sourceware.org/legacy-ml/libc-announce/2020/msg00001.html">Glibc 2.31</a>已经在社区完成发布，有需要的可以直接从社区上游获取使用，有任何问题也可以在本文留言。</p><p>另外，Glibc优化，也全部<a href="https://gitee.com/src-openeuler/glibc/pulls/17">合入到</a>集成在当前版本的openeuler中，有兴趣的，也可以直接使用openEuler最新版本进行体验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：姜逸坤 张学磊&lt;/p&gt;
&lt;p&gt;从2019年10月初开始，我们团队开始着手Glibc在aarch64(64)架构下的优化工作，并且在2019年年底，将我们的全部优化贡献给上游开源社区。本文分享我们在Glibc的版本完成的优化以及性能测试结果，同时我们也尝试着将优化的思路进行总结，希望对其他项目的优化提供一些思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让Github Action在你的ARM机器上跑起来</title>
    <link href="https://kunpengcompute.github.io/2020/04/15/rang-github-action-zai-ni-de-arm-ji-qi-shang-pao-qi-lai/"/>
    <id>https://kunpengcompute.github.io/2020/04/15/rang-github-action-zai-ni-de-arm-ji-qi-shang-pao-qi-lai/</id>
    <published>2020-04-15T08:22:00.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://github.com/Yikun">姜逸坤</a></p><p>Github在2019年8月，宣布推出了一项新的功能——<a href="https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/">Github Action</a>，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供<a href="https://github.blog/2019-11-05-self-hosted-runners-for-github-actions-is-now-in-beta/">Self Hosted Runners</a>功能，让开发者们可以将自己的机器接入到Github中来。</p><p>最近，我们利用这一功能，将搭载着<a href="https://openeuler.org/zh/">openEuler 20.03 (LTS) 操作系统</a>，跑在<a href="http://www.hisilicon.com/en/Products/ProductList/Kunpeng">Kunpeng 920 处理器</a>的ARM环境接入进来，在近期华为与阿里合作的<a href="https://github.com/kunpengcompute/kunpeng-mpam">MPAM</a>项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。</p><p>本篇文章将接入方法分享给大家，希望能够帮助更多同学们把<strong>自己的ARM环境</strong>也在Github上用起来。</p><a id="more"></a><h2 id="1-接入资源"><a href="#1-接入资源" class="headerlink" title="1. 接入资源"></a>1. 接入资源</h2><p><img src="https://user-images.githubusercontent.com/1736354/79320175-f21ce780-7f3b-11ea-8802-d0be06455e70.png" alt="image"><br>资源的接入流程比较简单：</p><ol><li><p>依次点击项目的<code>Settings</code>–<code>Actions</code>进入资源接入页面，点击<code>Add Runner</code>。</p></li><li><p>根据弹出的提示，下载和运行脚本<br><img src="https://user-images.githubusercontent.com/1736354/79411221-e8938e00-7fd4-11ea-8e8e-7a1b576c2fa2.png" alt="image"></p></li><li><p>完成后我们可以看到接入的资源：<br><img src="https://user-images.githubusercontent.com/1736354/79411190-d4e82780-7fd4-11ea-952c-06f43dbfa5e2.png" alt="image"></p></li></ol><h2 id="2-使用资源"><a href="#2-使用资源" class="headerlink" title="2. 使用资源"></a>2. 使用资源</h2><p><img src="https://user-images.githubusercontent.com/1736354/79320076-c4d03980-7f3b-11ea-8822-7e724fd8743a.png" alt="image"><br>我们为接入的项目增加一个Action：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Run</span> <span class="string">some</span> <span class="string">script</span> <span class="string">in</span> <span class="string">Kunpeng</span> <span class="string">env</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span> <span class="string">[</span> <span class="string">master</span> <span class="string">]</span></span><br><span class="line">  <span class="attr">pull_request:</span></span><br><span class="line">    <span class="attr">branches:</span> <span class="string">[</span> <span class="string">master</span> <span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">self-hosted</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Run</span> <span class="string">`uname</span> <span class="string">-a`</span> <span class="string">in</span> <span class="string">Kunpeng</span> <span class="string">env</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">uname</span> <span class="string">-a</span></span><br><span class="line">        <span class="string">cat</span> <span class="string">/etc/os-release</span></span><br><span class="line">        <span class="string">lscpu</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-E</span> <span class="string">"Architecture|Model name|CPU\(s\):"</span></span><br></pre></td></tr></table></figure><p>这样，这个workflow是展示所接入的环境上内核、操作系统、处理器信息，我们可以从结果看到job的结果：<br><img src="https://user-images.githubusercontent.com/1736354/79411452-71122e80-7fd5-11ea-83db-e4f001c5f796.png" alt="image"></p><p>点击<code>Details</code>可以进入详情页面：</p><p><img src="https://user-images.githubusercontent.com/1736354/79414159-7aeb6000-7fdc-11ea-86d5-60dce06abe43.png" alt="image"></p><p>可以看到，我们在资源上执行的指令，已经运行成功，可以看到这台资源的系统为<code>openEuler 20.03 (LTS)</code>，CPU为aarch64 128核的<code>Kunpeng 920</code>。</p><h2 id="3-结语"><a href="#3-结语" class="headerlink" title="3.结语"></a>3.结语</h2><p>本文介绍了我们是如何将搭载着鲲鹏920处理器、openEuler操作系统的计算资源接入到Github Action的。可以看到Github Action的自定义资源接入，在ARM64下还是很顺滑的。</p><p>希望这篇文章能够帮助到大家，大家也可以尝试着将你们自己ARM资源接入进来，有问题可以留言一起讨论，玩的开心！：）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://github.com/Yikun&quot;&gt;姜逸坤&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Github在2019年8月，宣布推出了一项新的功能——&lt;a href=&quot;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&quot;&gt;Github Action&lt;/a&gt;，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供&lt;a href=&quot;https://github.blog/2019-11-05-self-hosted-runners-for-github-actions-is-now-in-beta/&quot;&gt;Self Hosted Runners&lt;/a&gt;功能，让开发者们可以将自己的机器接入到Github中来。&lt;/p&gt;
&lt;p&gt;最近，我们利用这一功能，将搭载着&lt;a href=&quot;https://openeuler.org/zh/&quot;&gt;openEuler 20.03 (LTS) 操作系统&lt;/a&gt;，跑在&lt;a href=&quot;http://www.hisilicon.com/en/Products/ProductList/Kunpeng&quot;&gt;Kunpeng 920 处理器&lt;/a&gt;的ARM环境接入进来，在近期华为与阿里合作的&lt;a href=&quot;https://github.com/kunpengcompute/kunpeng-mpam&quot;&gt;MPAM&lt;/a&gt;项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。&lt;/p&gt;
&lt;p&gt;本篇文章将接入方法分享给大家，希望能够帮助更多同学们把&lt;strong&gt;自己的ARM环境&lt;/strong&gt;也在Github上用起来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>理解InnoDB rw-lock的统计数据</title>
    <link href="https://kunpengcompute.github.io/2020/04/15/li-jie-innodb-rw-lock-de-tong-ji-shu-ju/"/>
    <id>https://kunpengcompute.github.io/2020/04/15/li-jie-innodb-rw-lock-de-tong-ji-shu-ju/</id>
    <published>2020-04-15T03:46:30.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>原文链接：<a href="https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/">https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/</a></p><p>作者: Krunal Bauskar</p><p>InnoDB使用互斥锁进行独占访问，使用rw-locks进行共享访问。rw-locks用于控制对缓冲池页、表空间、自适应搜索系统、数据字典、informaton_schema等公共共享资源的访问。总之，rw-locks在InnoDB系统中扮演着非常重要的角色，因此跟踪和监视它们也很重要。</p><a id="more"></a><p>InnoDB 提供了一种简单的方式来跟踪它们， “SHOW ENGINE INNODB STATUS”.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 38667, rounds 54868, OS waits 16539</span><br><span class="line">RW-excl spins 6353, rounds 126218, OS waits 3936</span><br><span class="line">RW-sx spins 1896, rounds 43888, OS waits 966</span><br><span class="line">Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx</span><br></pre></td></tr></table></figure><p>在本文中，我们将了解这些统计数据是如何计算的，以及每个数据的意义。我们还将尝试使用不同的用例来描述一些推论，并且接触一下基础又重要的统计数据， 这个<a href="https://bugs.mysql.com/bug.php?id=99171">bug</a>使当前状态的统计几乎无法进行调优。</p><h2 id="rw-lock-自旋算法"><a href="#rw-lock-自旋算法" class="headerlink" title="rw-lock 自旋算法"></a>rw-lock 自旋算法</h2><p>rw-locks有三种类型:</p><ul><li>Shared: 提供资源的共享访问。允许多个共享锁。</li><li>Exclusive: 提供对资源的独占访问。共享锁等待排他锁。</li><li>Shared-Exclusive (SX): 对读不一致的资源提供写访问（relaxed exclusive）。</li></ul><p>首先我们先尝试理解流程，然后讨论一些调优步骤。</p><p>(为了便于讨论，假设 spins=0, rounds=0, os-waits=0).</p><h3 id="Locking-步骤"><a href="#Locking-步骤" class="headerlink" title="Locking 步骤"></a>Locking 步骤</h3><ul><li>Step-1:</li></ul><p>  尝试获取所需的锁</p><ul><li>If SUCCESS then return immediately. (spins=0, rounds=0, os-waits=0)</li><li>If FAILURE then enter spin-loop.</li></ul><ul><li><strong>Step-2:</strong> 开始自旋回环(Spin-loop). 增加自旋计数 (spin-count). <em>(为什么需要自旋循环？如果我们的线程进入等待状态，那么操作系统将把CPU从给定的线程中带走，暂时不让它使用CPU，然后线程将不得不按照操作系统调度的次序等待CPU资源，从而进行下面的任务。更好的方法是在繁忙等待中（busy-wait）使用自旋循环(带有条件的检查确认锁是否被释放）以便保留CPU。由于这些锁大多数都是短时间使用，因此重新获得的机会可能性非常高。).</em></li></ul><ul><li><strong>Step-3:</strong> 开始N轮自旋。Start spinning for N rounds. (这里N的定义由<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_sync_spin_loops">innodb_sync_spin_loops</a>来控制)。默认为30轮。<ul><li><strong>Step-3a:</strong> 每一轮将调用一个PAUSE逻辑（见下面关于PAUSE逻辑的单独一节），这将导致CPU进入PAUSE的X个周期。</li><li><strong>Step-3b:</strong> 每轮检查（软实现）是否对应的锁已经可用(busy-wait)。<ul><li>如果它可用，那么自旋循环退出。（可能还有一些其他同样正在自旋的检查。我们将使用下面的信息）。</li></ul></li><li><strong>Step-3c:</strong> 再次尝试获取所需的锁。<ul><li>If SUCCESS then return. (spins=1, rounds=M (M &lt;= N), os-waits=0)</li><li>If FAILURE，并且此时仍有其他正在自旋，且悬而未决任务 (max=innodb_sync_spin_loops)还是继续自旋。（为什么循环被中断，锁失败。注：该锁被多个线程并行查看。而当多个线程试图获取锁时，它们收到了锁可用的信号。被其他线程取走，所以该线程现在仍在重新尝试）。</li></ul></li><li><strong>Step-3d:</strong> 当这个线程现在完成了它设置的spin-wait轮循次数，到现在它还没有获得锁。那么它会被认为浪费CPU周期，没有必要继续自旋。最好的选择是放弃挂起的CPU周期并交还给操作系统，让操作系统调度做其他有用的事情。此外，由于所述线程现在将要进入睡眠，它应该向一些公共基础设施注册自身，这些基础设施将帮助它在所述锁可用时发出恢复活动的信号。</li><li><strong>Step-3e:</strong> 这个将线程从唤醒的基础设施正式InnoDB中的同步阵列（sync-array）基础设施。<ul><li>所述线程通过在同步阵列中预留插槽来注册自身。</li><li>在开始等待之前，再试一次看锁是否可用。（因为预留可能很费时间，同时锁这个时候是可用的状态）。</li><li>如果仍然没有获得锁，则将等待同步阵列向该线程送回信号。</li><li>这种等待称为OS-wait，进入这个循环现在会导致OS-waits计数增加。</li></ul></li><li><strong>Step-3f:</strong> 如果该线程收到由同步阵列发送回的wait-event信号。它会重新尝试获取锁。<ul><li>If SUCCESS then return. (spins=1, rounds=N, os-waits=1)</li><li>If FAILURE ，则整个循环从旋转逻辑重新启动(返回Step-3，rounds-count重新初始化为0)。注意：自旋计数(spins count)不会重新递增。</li></ul></li></ul></li></ul><p>所以现在我们来给这些计数赋予意义</p><ul><li><strong>spins:</strong> 代表在第一次尝试中多少轮数而未能得到一个锁，并不得不进入自旋循环。</li><li><strong>rounds:</strong> 表示执行多少轮PAUSE逻辑。</li><li><strong>os-waits:</strong>自旋循环在多少轮自旋时仍未得到锁而导致os-waits。</li></ul><p>在获取所述锁流程中的自旋循环期间中，可能需要超过30轮(innodb_sync_spin_loops)PAUSE逻辑，并且还可能多次进入os-waits。这可能会导致os-waits &gt; spins-count。</p><h3 id="PAUSE-逻辑"><a href="#PAUSE-逻辑" class="headerlink" title="PAUSE 逻辑"></a>PAUSE 逻辑</h3><p>K = {取 (0 - <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_delay">innodb_spin_wait_delay</a>)之间的随机数 * <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_pause_multiplier">innodb_spin_wait_pause_multiplier</a>}</p><p>调用底层 PAUSE 指令 K 次.</p><p>并不是所有的架构都提供底层的PAUSE指令。x86有提供，但ARM没有。即使x86深藏了这个PAUSE指令，它也会随着处理器的不同系列而继续变化。老一代处理器的周期约为10-15次（pre-skylake）,Skylake系列的周期约为140次，然后CascadeLake系列的周期数又降下来了 (我看到在属于CascadeLake系列的Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz芯片上是13次). <em>(除了Cascadelake系列以外，我个人并没有在其他平台上对它进行基准测试) 但这些信息是可供参考的。</em>这意味着延迟引入PAUSE指令（按照周期计算）会持续不断的变化，所以针对每一代/类型的处理器调整PAUSE逻辑是非常重要的。 这里有两个可配置的变量可以解决这个问题，innodb_spin_wait_delay 和 innodb_spin_wait_pause_multiplier。</p><h2 id="统计数据解读"><a href="#统计数据解读" class="headerlink" title="统计数据解读"></a>统计数据解读</h2><p>现在我们已经了解了统计数据，让我们看看这个数字，并试着做出一些推断。</p><p>不过，在谈及进一步的细节之前，让我先看看这个<a href="https://bugs.mysql.com/bug.php?id=99171">bug</a>， 它描述了导致这些统计数据不一致和不正确的原因和修复方法。</p><p>为了得到一个公正的结论，我们将使用mysql对应版本，并打上补丁。 (正如bug中指出的，不使用修复补丁统计数据不能产生正确的数据，因此各种解释和调优都毫无用处).</p><h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case 1"></a>Use Case 1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br><span class="line">RW-excl spins 50582, rounds 1502625, OS waits 56124</span><br><span class="line">RW-sx spins 12583, rounds 360973, OS waits 10484</span><br><span class="line">Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx</span><br></pre></td></tr></table></figure><p>让我们分析一下共享自旋的情况:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br></pre></td></tr></table></figure><ul><li>在头一次尝试中，用了338K 次仍未获取到锁，迫使线程进去自旋锁状态(spin-lock)。</li><li>在每个自旋周期内，执行了60轮PAUSE周期（因此，所述自旋周期执行了2次）。</li><li>OS-waits/spins = 592/338 = 1.75表明大部分被分流进入了OS-wait（PAUSE的延迟不够）。</li><li>表明对于大多数自旋周期，单一的操OS-wait是不够的，因此这种操作是在重复进行的。</li></ul><p><strong>Conclusion:</strong> 该Use-case是高竞争情况。而且，诸如PAUSE循环无法产生所需的延迟来获得锁，导致每个自旋周期产生如此之多的PAUSE循环。</p><p><em>256 thread oltp-read-write workload on 24 vCPU ARM machine</em></p><h3 id="Use-Case-2"><a href="#Use-Case-2" class="headerlink" title="Use Case 2"></a>Use Case 2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 35943, rounds 777178, OS waits 19051</span><br><span class="line">RW-excl spins 4269, rounds 121796, OS waits 4164</span><br><span class="line">RW-sx spins 13407, rounds 321954, OS waits 7346</span><br><span class="line">Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx</span><br></pre></td></tr></table></figure><p>让我们分析一下共享自旋的情况:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 35943, rounds 777178, OS waits 19051</span><br></pre></td></tr></table></figure><ul><li>流程中，自旋循环35K次。</li><li>只有19K次(大约是自旋循环的一半)引起了OS-waits。</li><li>平均每个自旋周期也限制在21.62，这表明，对于每个自旋周期，平均有22轮PAUSE循环。</li></ul><p><strong>Conclusion:</strong> 该Use-case表示中度竞争情况。</p><p><em>16 thread oltp-read-write workload on 24 vCPU ARM machine</em></p><h3 id="Use-Case-3"><a href="#Use-Case-3" class="headerlink" title="Use Case 3"></a>Use Case 3</h3><p>让我举一个常见的例子，以供参考。这是16个线程的oltp-read-write工作负载在基于x86_64的16CPU虚拟机上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 39578, rounds 424553, OS waits 7329</span><br><span class="line">RW-excl spins 5828, rounds 78225, OS waits 1341</span><br><span class="line">RW-sx spins 11666, rounds 67297, OS waits 449</span><br><span class="line">Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx</span><br></pre></td></tr></table></figure><ul><li>流程中自旋循环39K次。</li><li>只有7K(约占自旋循环的20%) 导致OS-waits。</li><li>每自旋周期平均数也限制为10。</li></ul><p><strong>Conclusion:</strong> 该Use-case表示低竞争情况。</p><p><em>16 thread oltp-read-write workload on 24 vCPU x86_64 machine</em></p><h3 id="调优注意事项"><a href="#调优注意事项" class="headerlink" title="调优注意事项"></a>调优注意事项</h3><p>记得我们在上面看到的高竞争案例。通过优化一些代码，可以显著减少共享自旋的争用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 318800, rounds 13856732, OS waits 374634</span><br><span class="line">RW-excl spins 35759, rounds 656955, OS waits 22310</span><br><span class="line">RW-sx spins 10750, rounds 226315, OS waits 5598</span><br><span class="line">Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx</span><br></pre></td></tr></table></figure><p>每个自旋周期的轮数: 平均数从 60 降到 43</p><p>每个自旋周期的OS-wait次数: 从1.75 降到1.17</p><p>这性能好起来了吗？不是太好。有许多因素需要考虑。</p><ul><li>你看到TPS有改善吗？</li><li>有时，它可能会建议简单地增加PAUSE循环。但是，增加PAUSE循环超过某个点将会导致延长自旋周期，最终浪费宝贵的CPU周期，尤其是这会导致线程返回到OS-wait状态。(这种方式可能对HT案例和多核案例更有效)。</li><li>同样，如上所述，不同处理器的系列和类型会影响PAUSE循环延迟。</li></ul><p>有许多因素需要考虑。甚至我正在研究这个问题，看看我们如何为所有类型的CPU来优化它。一旦我在这个研究中发掘到一些非常好的通用的算法(或者我们可以开发一些自动的、自调整的或自适应的算法)，我会发布更多关于这个问题的博客，用户无需担心。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>正如我们在上文看到的，rw-locks统计数据可以帮助我们更好地理解系统中锁的争用。当然，它不是有关InnoDB争用的唯一说明，因为互斥锁没在这些统计数据里面。调优可能具有挑战性，因为以错误的方式过度调优也会影响性能。</p><p><em>如果你还有问题/疑问，请告诉我。会试着去回答他们。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/&quot;&gt;https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;InnoDB使用互斥锁进行独占访问，使用rw-locks进行共享访问。rw-locks用于控制对缓冲池页、表空间、自适应搜索系统、数据字典、informaton_schema等公共共享资源的访问。总之，rw-locks在InnoDB系统中扮演着非常重要的角色，因此跟踪和监视它们也很重要。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Understanding InnoDB rw-lock stats</title>
    <link href="https://kunpengcompute.github.io/2020/04/14/understanding-innodb-rw-lock-stats/"/>
    <id>https://kunpengcompute.github.io/2020/04/14/understanding-innodb-rw-lock-stats/</id>
    <published>2020-04-14T07:21:05.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar</p><p>InnoDB uses mutexes for exclusive access and rw-locks for the shared access of the resources. rw-locks are used to control access to the common shared resources like buffer pool pages, tablespaces, adaptive search systems, data-dictionary, informaton_schema, etc… In short, rw-locks play a very important role in the InnoDB system and so tracking and monitoring them is important too.</p><a id="more"></a><p>InnoDB provides an easy way to track them using “SHOW ENGINE INNODB STATUS”.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 38667, rounds 54868, OS waits 16539</span><br><span class="line">RW-excl spins 6353, rounds 126218, OS waits 3936</span><br><span class="line">RW-sx spins 1896, rounds 43888, OS waits 966</span><br><span class="line">Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx</span><br></pre></td></tr></table></figure><p>In this article we will try to understand how these stats are calculated and what is the significance of each of these numbers. We will also try to draw inferences using different use-cases and touch base important stats <a href="https://bugs.mysql.com/bug.php?id=99171">bug</a> that makes the current state of the stats almost ineffective for tuning.</p><h2 id="rw-lock-spin-algorithm"><a href="#rw-lock-spin-algorithm" class="headerlink" title="rw-lock spin algorithm"></a><span style="color:#4885ed">rw-lock spin algorithm</span></h2><p>There are 3 types of rw-locks:</p><ul><li>Shared: offers shared access to the resource. Multiple shared locks are allowed.</li><li>Exclusive: offers exclusive access to the resource. Shared locks wait for exclusive locks.</li><li>Shared-Exclusive (SX): offer write access to the resource with inconsistent read. (relaxed exclusive).</li></ul><p>We will first try to understand the flow and then discuss some tuning steps. <br><br>(For sake of discussion, to start with, let’s assume <span style="background-color: #fff2e6; color: red">spins=0, rounds=0, os-waits=0</span>).</p><h3 id="Locking-Steps"><a href="#Locking-Steps" class="headerlink" title="Locking Steps"></a><span style="color:#1aa260">Locking Steps</span></h3><ul><li><p><strong>Step-1:</strong> Try to obtain the needed lock</p><ul><li>If <span style="color: green">SUCCESS</span> then return immediately. <span style="background-color: #fff2e6; color: red"> (spins=0, rounds=0, os-waits=0)</span></li><li>If <span style="color: red">FAILURE</span> then enter spin-loop.<p></p> </li></ul></li><li><p><strong>Step-2:</strong> Start Spin-loop. Increment spin-count. <em>(Why is spin-loop needed? If we enter wait then the OS will take away CPU from the given thread and then the thread will have to wait for its turns as per OS-scheduling. Better approach is to busy-wait using spin-loop (with condition check) so that the CPU is kept. Since most of these locks are short-duration likely chance of re-obtaining it very high).</em></p><p></p> </li><li><p><strong>Step-3:</strong> Start spinning for N rounds. (Here N is defined and controlled by <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_sync_spin_loops">innodb_sync_spin_loops</a>). Default is 30 rounds. </p><ul><li><p><strong>Step-3a:</strong> Each round will invoke a PAUSE logic (see a separate section below about PAUSE logic) that will cause the CPU to go to PAUSE for X cycles.</p></li><li><p><strong>Step-3b:</strong> Post each round a soft check is done if the said lock is available (busy-wait).</p><ul><li>If it is available then spin-cycle exits. (There could still be some rounds pending. We will use this info below).</li></ul></li><li><p><strong>Step-3c:</strong> Again try to obtain the needed lock.</p><ul><li>If <span style="color: green">SUCCESS</span> then return.<span style="background-color: #fff2e6; color: red"> (spins=1, rounds=M (M &lt;= N), os-waits=0)</span></li><li>If <span style="color: red">FAILURE</span> and there are some pending rounds (max=innodb_sync_spin_loops) then resume spinning. <em>(How come the loop was interrupted and locking failed. Note: the said lock is being looked upon by multiple threads in parallel. While multiple threads got signals about lock availability by the time said thread tried to obtain the lock, some other thread took it. So the said thread is now back re-trying)</em>.</li></ul></li><li><p><strong>Step-3d:</strong> Say a thread now completes its set rounds of spin-wait and even now it failed to obtain the lock. There is no point in  spinning further and wasting CPU cycles. Better give up pending CPU cycles back to OS and let OS scheduling do the needful. Also, since the said thread is now going to go to sleep it should register itself with some common infrastructure that will help signal it back to active whenever the said lock is available.</p></li><li><p><strong>Step-3e:</strong> This infrastructure to signal it back to active is sync-array infrastructure in InnoDB.</p><ul><li>Said thread registers itself by reserving a slot in the said array.</li><li>Before starting the wait, give another try to see if the lock is available. (since reserving could be time consuming and lock could be available in meantime).</li><li>If still lock is not available then wait for sync-array infrastructure to signal back the said thread.</li><li>This wait is called OS-wait and entering this loop will now cause OS-waits count to increase.</li></ul></li><li><p><strong>Step-3f:</strong> Say the said thread is signaled by sync-array infrastructure for the wait-event. It retries to obtain the needed lock.</p><ul><li>If <span style="color: green">SUCCESS</span> then return. <span style="background-color: #fff2e6; color: red"> (spins=1, rounds=N, os-waits=1)</span></li><li>If <span style="color: red">FAILURE</span> then the whole loop restarts from spinning logic (Back to Step-3 with rounds-count re-intialize to 0). Note: spins count is not re-incremented.</li></ul></li></ul></li></ul><p>So let’s now assign the meaning to these counts</p><ul><li><strong>spins:</strong> represent how many times flow failed to get a lock in first go and had to enter spin-loop.</li><li><strong>rounds:</strong> represent how many rounds of PAUSE logic executed.</li><li><strong>os-waits:</strong> how many times spin-loop failed to grant a lock that resulted in os-waits.</li></ul><p>It is possible that during a given spin loop for acquiring said lock flow may need more than 30 (innodb_sync_spin_loops) rounds of PAUSE logic and have to multiple time enter os-waits. This can cause os-waits &gt; spins-count.</p><h3 id="PAUSE-logic"><a href="#PAUSE-logic" class="headerlink" title="PAUSE logic"></a><span style="color:#1aa260">PAUSE logic</span></h3><p>K = {random value from between  (0 - <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_delay">innodb_spin_wait_delay</a>) * <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_pause_multiplier">innodb_spin_wait_pause_multiplier</a>}</p><p>Invoke low-level PAUSE instruction K times.</p><p>Not all architectures provide low-level pause instruction. x86 does provide it but ARM doesn’t. Even with x86 latency of this pause instruction continues to change with different families of processors. It was around 10-15 cycles for old generation processors (pre-skylake). Went upto 140 cycles with Skylake and again came down with CascadeLake (I see 13 cycles with Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz which belongs to CascadeLake family). <em>(I have not personally benchmarked it on all platforms (except Cascadelake) but this is based on the references.</em> This means the delay (in terms of cycle) introduced by PAUSE continues to change and so tuning PAUSE logic for each generation/type of processor is important. 2 configurable variables viz. innodb_spin_wait_delay, innodb_spin_wait_pause_multiplier exactly help achieve this.</p><h2 id="Interpreting-stats"><a href="#Interpreting-stats" class="headerlink" title="Interpreting stats"></a><span style="color:#4885ed">Interpreting stats</span></h2><p>Now that we understand the stats let’s look at the number and try to draw some inferences.</p><p>But before we get into further details let me point out a <a href="https://bugs.mysql.com/bug.php?id=99171">bug</a> that makes these stats inconsistent and incorrect.</p><p>To get a fair idea we will use the version of mysql with the patch applied (as pointed in bug, stats w/o patch doesn’t help yield correct picture and so all kinds of interpretation and tuning is bound to fail).</p><h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case 1"></a><span style="color:#1aa260">Use Case 1</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br><span class="line">RW-excl spins 50582, rounds 1502625, OS waits 56124</span><br><span class="line">RW-sx spins 12583, rounds 360973, OS waits 10484</span><br><span class="line">Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx</span><br></pre></td></tr></table></figure><p>Let’s analyze the shared spins case:<br></p><figure class="highlight plain"><figcaption><span>spins 338969, rounds 20447615, OS waits 592941```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">* 338K times flow couldn’t find the lock in first go, forcing thread to enter spin-lock.</span><br><span class="line">* During each spin-cycle there were 60 rounds of PAUSE cycle executed (so the said spin-cycles were done 2 times).</span><br><span class="line">* OS-waits&#x2F;spins &#x3D; 592&#x2F;338 &#x3D; 1.75 suggest that majority of the flow entered OS-wait (delay from PAUSE was not sufficient).</span><br><span class="line">* It also suggests that for the majority of spin-cycles, single OS -wait was not enough so it was repeated.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents heavy contention. Also, it sounds like PAUSE loop is unable to introduce the needed delay that is causing so many ROUNDS of PAUSE loop per spin-cycle.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;256 thread oltp-read-write workload on 24 vCPU ARM machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Use Case 2&lt;&#x2F;span&gt;</span><br></pre></td></tr></table></figure><p>RW-shared spins 35943, rounds 777178, OS waits 19051<br>RW-excl spins 4269, rounds 121796, OS waits 4164<br>RW-sx spins 13407, rounds 321954, OS waits 7346<br>Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">Let’s analyze the shared spins case:&lt;br&gt;</span><br><span class="line">&#96;RW-shared spins 35943, rounds 777178, OS waits 19051&#96;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* Flow invokes spin-loop 35K times.</span><br><span class="line">* Only 19K (that is approximately half of the spin-loop) caused OS-waits.</span><br><span class="line">* Per spin-cycle average too is limited to 21.62 that suggests that for each spin-cycle on average 22 rounds of PAUSE loop was </span><br><span class="line">invoked.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents medium contention.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU ARM machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Use Case 3&lt;&#x2F;span&gt;</span><br><span class="line"></span><br><span class="line">Just for reference, let me put an example of very less contention. This is with 16 threads oltp-read-write workload on x86_64 based VM with 16 CPU.</span><br></pre></td></tr></table></figure><p>RW-shared spins 39578, rounds 424553, OS waits 7329<br>RW-excl spins 5828, rounds 78225, OS waits 1341<br>RW-sx spins 11666, rounds 67297, OS waits 449<br>Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* Flow invokes spin-loop 39K times.</span><br><span class="line">* Only 7K (that is approximately 20% of the spin-loop) caused OS-waits.</span><br><span class="line">* Per spin-cycle average too is limited to 10.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents low contention.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU x86_64 machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Note on tuning&lt;&#x2F;span&gt;</span><br><span class="line"></span><br><span class="line">Remember the high contention case we saw above. By tuning some things + code changes I could reduce the contention for shared-spins significantly.</span><br></pre></td></tr></table></figure><p>RW-shared spins 318800, rounds 13856732, OS waits 374634<br>RW-excl spins 35759, rounds 656955, OS waits 22310<br>RW-sx spins 10750, rounds 226315, OS waits 5598<br>Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx</p><p>```</p><p>Rounds per spins-cycles: average of 60 -&gt; 43 <br><br>OS-wait per spin-cycle: 1.75 -&gt; 1.17</p><p>Is this good.? Not really. There are multiple factors to consider.</p><ul><li>Do you see improvement in TPS?</li><li>Sometime it may be suggested to simply increase the PAUSE loop. But increasing PAUSE loop beyond some point can cause extended spin-cycle wasting precious CPU cycles especially if that causes it to land back in OS-wait. (Does this help more in HT cases vs multi-core case).</li><li>Also, as pointed above, processor generation and type affect the PAUSE loop latency.</li></ul><p>There are multiple factors to consider. Even I am exploring this to see how we can tune this for all kinds of CPUs. I will blog more about it once I get some good solid algorithms on this front (or maybe we can develop some automated, self-adjustable or adaptive algorithms) so users don’t need to worry about it.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>As we saw above rw-locks stats can help us get good insight on understanding the contention of the system. Of-course it is not the only in-sight about InnoDB contention as mutexes are not covered as part of these stats. Tuning could be challenging but over-tuning can affect performance in the wrong way too.</p><br><em>If you have more questions/queries do let me know. Will try to answer them.</em>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;InnoDB uses mutexes for exclusive access and rw-locks for the shared access of the resources. rw-locks are used to control access to the common shared resources like buffer pool pages, tablespaces, adaptive search systems, data-dictionary, informaton_schema, etc… In short, rw-locks play a very important role in the InnoDB system and so tracking and monitoring them is important too.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Web开源服务之ARM64现状</title>
    <link href="https://kunpengcompute.github.io/2020/04/10/web-kai-yuan-fu-wu-zhi-arm64-xian-zhuang/"/>
    <id>https://kunpengcompute.github.io/2020/04/10/web-kai-yuan-fu-wu-zhi-arm64-xian-zhuang/</id>
    <published>2020-04-10T03:03:00.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/wangxiyuan">王玺源</a></p><p>社区核心参与者：<a href="https://github.com/martin-g">Martin Grigorov</a>、<a href="https://www.linkedin.com/in/mikerumph/">Michael Rumph</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>开源界中Web服务众多,但其中很多软件对ARM64的支持并不理想。或是没有官方CI测试保证代码质量，或是在ARM64上的性能明显差于X86_64，甚至有的服务根本无法在ARM上运行。为了完善Web领域的ARM64生态，我们参与了主流的几个开源社区，旨在推动Web on ARM64。以下是我们近期的一些进展，以供大家参考。</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>我们目前参与了主流的共9个Web相关项目。如下所示</p><table><thead><tr><th>项目</th><th>主要语言</th></tr></thead><tbody><tr><td>Apache Httpd Server</td><td>C</td></tr><tr><td>Apache Tomcat</td><td>Java</td></tr><tr><td>Memcached</td><td>C</td></tr><tr><td>Nginx</td><td>C</td></tr><tr><td>Lighttpd</td><td>C</td></tr><tr><td>JBoss/WildFly</td><td>Java</td></tr><tr><td>HAProxy</td><td>C</td></tr><tr><td>Squid</td><td>C++</td></tr><tr><td>Varnish Cache</td><td>C</td></tr></tbody></table><p>针对这些项目，我们按照以下三个方面循序渐进的推动中:</p><ol><li>能不能在ARM上运行</li><li>如何稳定在ARM上运行</li><li>怎么更好的在ARM上运行</li></ol><h1 id="能不能在ARM上运行"><a href="#能不能在ARM上运行" class="headerlink" title="能不能在ARM上运行"></a>能不能在ARM上运行</h1><p>我们可以看到这9大项目主要由Java和C/C++编写。</p><p>首先，像Python、Java这种自带runtime的语言天生就是跨平台的。这样的项目在ARM64平台上至少可以保证程序的可运行。</p><p>而C/C++项目则需要先编译成ARM64平台的目标可执行文件。这样的项目则需要先进行编译测试。</p><p>经过我们的测试，这9个Web项目都可以在ARM64上成功编译并运行。</p><h1 id="如何稳定在ARM上运行"><a href="#如何稳定在ARM上运行" class="headerlink" title="如何稳定在ARM上运行"></a>如何稳定在ARM上运行</h1><p>所谓稳定，包含两个方面：</p><ol><li>软件在ARM64上是否和在X86_64上行为一致？</li><li>随着代码更新迭代，软件在ARM64上是否持续可用？</li></ol><h2 id="行为一致"><a href="#行为一致" class="headerlink" title="行为一致"></a>行为一致</h2><p>我们常遇到两类行为一致的问题：</p><ol><li>同样的代码，不同的结果</li><li>同样的功能，不同的支持</li></ol><p>很遗憾，由于架构不同、底层实现不同等原因，很多软件的某些行为在X86_64和ARM64上的行为并不一致。</p><p>例如，之前的<a href="https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/">文章</a>提到的Java中Math计算结果的差异。</p><p>又或者某些功能依赖独有的平台特性或者特殊的第三方库，导致在X86_64上可以运行的功能，在ARM64上却执行失败。</p><p>例如我们发现WildFly官方发布的源码包中缺少了个别ARM64平台的<code>.so</code>文件，这就导致个别调用<code>.so</code>的功能不可用。</p><p>针对这种问题，我们需要打开代码逐个分析、逐个修复。保证所有测试在ARM64上全部通过。</p><h2 id="持续可用"><a href="#持续可用" class="headerlink" title="持续可用"></a>持续可用</h2><p>CI/CD是保证软件持续可用的重要方法。主流软件的CI系统都有X86_64平台的测试。而ARM64平台的少之又少。</p><p>针对这个问题，我们推动了这9个项目的ARM CI支持。除Lighttpd还在推动中以外，其他8个项目目前都已支持了ARM CI。甚至其中4个项目已经官方声明了ARM64的支持（详见附录）。</p><p>其中Httpd、Tomcat、Memcached、HAProxy和Varnish Cache通过Travis CI支持了ARM64测试。Nginx使用内部CI，对外不可见。Squid使用自己的树莓派。而JBOSS使用了我们捐献的基于Kunpeng 920的ARM虚拟机。同时我们也计划捐献同样的测试机到Lighttpd社区中。</p><p>随着ARM CI的落地，我们将持续保证ARM CI的稳定。我们相信在不久的将来，这9大核心Web项目都会官方声明ARM64的支持，并满足用户在ARM64上稳定、高效使用Web服务的需求。</p><h1 id="怎么更好的在ARM上运行"><a href="#怎么更好的在ARM上运行" class="headerlink" title="怎么更好的在ARM上运行"></a>怎么更好的在ARM上运行</h1><p>我们不仅希望软件在ARM64上能用，还在不断探索如何让软件在ARM64上用的好。其中<strong>性能优化</strong>是重中之重，也是我们未来一段时间的主要投入点。</p><p>例如，有些软件只实现了X86_64的汇编实现，但缺少ARM64的汇编代码。</p><p>又或者有些在X86_64上纯软实现的功能，可以在ARM64上通过下沉至硬编码的方式提高性能。</p><p>甚至还可以考虑如何最大化利用ARM64的多核优势，或规避ARM64的锁劣势等等。</p><p>关于性能优化的内容，我们将在以后的文章中针对不同的软件一一细说。敬请期待。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>最后附上我们参与Web社区的总览表格及相关链接，感兴趣的同学可以进一步详读，有任何问题，欢迎留言。</p><table><thead><tr><th></th><th>Official arm64 CI</th><th>CI tool</th><th>Package in Downloads</th><th>Official ARM support</th></tr></thead><tbody><tr><td><a href="https://tomcat.apache.org/">Apache Tomcat</a></td><td><a href="https://github.com/apache/tomcat/commit/f386fbb4abaa3fe8f3b3df1da7d14f756c729e2e">YES</a></td><td><a href="https://github.com/apache/tomcat/blob/master/.travis.yml">TravisCI</a></td><td><a href="https://tomcat.apache.org/download-90.cgi">Binary</a></td><td><a href="https://tomcat.apache.org/ci.html#TravisCI">YES</a></td></tr><tr><td><a href="https://memcached.org/">Memcached</a></td><td><a href="https://github.com/memcached/memcached/pull/593">YES</a></td><td>1. <a href="http://build.memcached.org:8010/">BuildBot</a> 2. <a href="https://github.com/memcached/memcached/blob/master/.travis.yml">TravisCI</a></td><td><a href="https://memcached.org/downloads">Source Code</a></td><td><a href="https://github.com/memcached/memcached/wiki/Hardware">YES</a></td></tr><tr><td><a href="https://httpd.apache.org/">Apache httpd</a></td><td><a href="https://markmail.org/message/ajm3eouaqfhm22ox">YES</a></td><td><a href="https://github.com/apache/httpd/blob/trunk/.travis.yml">TravisCI</a></td><td><a href="http://httpd.apache.org/download.cgi">Source Code</a></td><td><a href="https://github.com/apache/httpd/blob/2.4.x/CHANGES#L17-L20">YES</a></td></tr><tr><td><a href="https://nginx.org/">NGINX</a></td><td><a href="https://mailman.nginx.org/pipermail/nginx-devel/2020-January/012943.html">YES</a></td><td>Internal</td><td>Only for <a href="https://nginx.org/en/linux_packages.html">Ubuntu</a> LTSs</td><td><a href="https://nginx.org/en/linux_packages.html#Ubuntu">YES</a></td></tr><tr><td><a href="https://www.lighttpd.net/">Lighttpd</a></td><td>NO</td><td><a href="https://ci.lighttpd.net/view/lighttpd1.4/job/lighttpd1.4/">Jenkins</a></td><td><a href="https://www.lighttpd.net/download/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://www.wildfly.org/">JBoss/Wildfly</a></td><td><a href="https://ci.wildfly.org/viewType.html?buildTypeId=WF_MasterLinuxArm64OpenJ911">YES</a></td><td><a href="https://ci.wildfly.org/">TeamCity</a></td><td><a href="https://wildfly.org/downloads/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://www.haproxy.org/">HAProxy</a></td><td><a href="https://github.com/haproxy/haproxy/commit/9bf2a1be89a6eaddb00f07b9d069a9a16c24c037">YES</a></td><td>1. <a href="https://cirrus-ci.com/github/haproxy/haproxy">CirrusCI</a> <br> 2. <a href="https://github.com/haproxy/haproxy/blob/master/.travis.yml">TravisCI</a></td><td><a href="http://www.haproxy.org/">Source Code</a></td><td><a href="https://www.haproxy.org/#plat">YES</a></td></tr><tr><td><a href="http://www.squid-cache.org/">Squid</a></td><td><a href="http://build.squid-cache.org/computer/arm64-rpi/">YES</a></td><td><a href="http://build.squid-cache.org/">Jenkins</a></td><td><a href="http://squid-cache.org/Versions/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://github.com/varnishcache/varnish-cache/">Varnish Cache</a></td><td><a href="https://github.com/varnishcache/varnish-cache/pull/3195">YES</a></td><td><a href="https://github.com/varnishcache/varnish-cache/blob/master/.travis.yml">Travis</a></td><td>1. <a href="https://varnish-cache.org/releases/index.html">Source Code</a> <br> 2. <a href="https://packagecloud.io/varnishcache">Package</a></td><td>NO</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;王玺源&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;社区核心参与者：&lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;、&lt;a href=&quot;https://www.linkedin.com/in/mikerumph/&quot;&gt;Michael Rumph&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;开源界中Web服务众多,但其中很多软件对ARM64的支持并不理想。或是没有官方CI测试保证代码质量，或是在ARM64上的性能明显差于X86_64，甚至有的服务根本无法在ARM上运行。为了完善Web领域的ARM64生态，我们参与了主流的几个开源社区，旨在推动Web on ARM64。以下是我们近期的一些进展，以供大家参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>MySQL on x86 vs ARM</title>
    <link href="https://kunpengcompute.github.io/2020/04/08/mysql-on-x86-vs-arm/"/>
    <id>https://kunpengcompute.github.io/2020/04/08/mysql-on-x86-vs-arm/</id>
    <published>2020-04-08T10:16:43.000Z</published>
    <updated>2020-09-11T01:53:43.717Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar</p><p>By and large this would be a topic of interest for most of us including me when I started to explore this space. Before we dwell into the numbers let’s first understand some basic differences between 2 architectures. Beyond being CISC and RISC let’s look at the important differences from MySQL perspective.</p><a id="more"></a><ul><li>Strong vs Weak memory model (weak memory model needs proper memory barrier while writing lock-free code).</li><li>Underlying hardware specific specialized instructions. For example: both now support crc32c hardware instructions but being low-level they are different ways to invoke them. For more differences checkout for x86-SSE/ARM-ACLE.</li><li>Cache Line differences. Most of the ARM processors tend to use bigger cache lines (128 bytes for all caches or a mix of 64/128 bytes).</li><li>Other sys-call level differences like: absence of PAUSE instructions with ARM and substitute instruction with very low latency failing to induce needed delay, sched_getcpu is costlier on ARM introducing challenges with use of lock-free construct, memory operations seems to show higher latency, etc…</li></ul><p>Community has contributed multiple patches around this space (Topic for another blog). Since MySQL just started supporting MySQL on ARM there are  few optimizations but most of the work is yet to be done.</p><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a><span style="color:#4885ed">Performance</span></h2><p>Now let’s look at the most important aspect: Performance</p><p>We tested the performance of MySQL (current release 8.0.19) on x86 and ARM. Details of the test and machine are given below.</p><h3 id="Test-Setup"><a href="#Test-Setup" class="headerlink" title="Test Setup"></a><span style="color:#1aa260">Test Setup</span></h3><ul><li>24 vCPU/48 GB Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz for running MySQL on x86.</li><li>24 vCPU/48 GB ARM @ 2.60GHz for running MySQL on ARM</li><li>sysbench is running on a dedicated machine located in the same data-center.</li><li>sysbench steps:<ul><li>Load Tables. (Same seed db is reused for multiple runs so warmup is needed).</li><li>Checksum based warmup. Run checksum on all tables. For checksum, flow needs to fetch the rows in the buffer pool there-by causing it to warm up.</li><li>Query based warm up. Can skip but helpful if you are using adaptive hash indexes.</li><li>Execute TC (oltp-read-write/oltp-update-index/oltp-update-non-index/oltp-read-only/oltp-point-select)</li><li>Each TC is executed for N different scalability. Given 24 vCPU tried it for 1/2/4/8/16/32/128/256.</li><li>Before switching TC, an intermediate sleep is introduced to help flush changes from previous TC. This can’t ensure all changes are flushed but sleep of X secs ensures least impact on followup TC.</li><li>MySQL-Server Configuration:<ul><li>BP is large enough to accomodate complete data in-memory</li><li>For more details please check the <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">following configuration details</a></li></ul></li></ul></li></ul><p></p><br> Details of running the scripts and automated test-script to invoke sysbench are also [available here](https://github.com/mysqlonarm/benchmark-suites)<h3 id="Run-specific-details"><a href="#Run-specific-details" class="headerlink" title="Run specific details:"></a><span style="color: #1aa260">Run specific details:</span></h3><ul><li>Table: 96-tables * 1.5 million (data-size= 34GB)</li><li>Buffer Pool: 36GB</li><li>Redo-Log: 4GB*2</li><li>TC-run-time: 300 secs</li><li>TC-warmup: 60 (sysbench –warmup-time)</li><li>workload-query-based warmup: 600</li><li>change-over-sleep: 180</li><li>checksum-based-warmup: enabled</li><li>data-storage: 300GB (support for 16500 IOPS (nullify effect of Burst IOPS)).</li></ul><p><font size="3"><em>Note: Frequency Scaling (FS). Given ARM is running @ 2.6 GHz vs x86 is running @ 3.0 GHz. Comparing them directly is not fair. In order to compensate for the frequency difference, graphs also add frequency-scaled tps/qps for ARM (ARM-fscaled simply extrapolate original ARM tps/qps number by (3/2.6) factor). In real life, the factor could be a bit on the higher side given increasing CPU frequency can affect contention graphs and wait cycles.</em></font></p><br><hr><h3 id="1-Point-Select"><a href="#1-Point-Select" class="headerlink" title="1. Point Select:"></a><span style="color: #de5246"><ins>1. Point Select:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ps.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>6696</td><td>6439</td><td>7726</td><td>4</td><td>20</td></tr><tr><td>2</td><td>12482</td><td>11774</td><td>14402</td><td>6</td><td>22</td></tr><tr><td>4</td><td>23881</td><td>21308</td><td>27555</td><td>12</td><td>29</td></tr><tr><td>8</td><td>45993</td><td>42110</td><td>53069</td><td>9</td><td>26</td></tr><tr><td>16</td><td>88517</td><td>81239</td><td>102135</td><td>9</td><td>26</td></tr><tr><td>32</td><td>142974</td><td>136724</td><td>164970</td><td>5</td><td>21</td></tr><tr><td>64</td><td>198839</td><td>212484</td><td>229430</td><td>-6</td><td>8</td></tr><tr><td>128</td><td>217778</td><td>241555</td><td>251282</td><td>-10</td><td>4</td></tr><tr><td>256</td><td>209797</td><td>224009</td><td>242073</td><td>-6</td><td>8</td></tr></tbody></table><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>ARM performs better than x86 for lower scalability but fails to scale at same rate with increasing scalability.</li><li>With frequency scaling applied ARM continues to beat x86 despite of the scalability issues.</li></ul><br><hr><h3 id="2-Read-Only"><a href="#2-Read-Only" class="headerlink" title="2. Read Only:"></a><span style="color: #de5246"><ins>2. Read Only:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ro.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>5222</td><td>5259</td><td>6025</td><td>-1</td><td>15</td></tr><tr><td>2</td><td>10333</td><td>10200</td><td>11923</td><td>1</td><td>17</td></tr><tr><td>4</td><td>19176</td><td>19349</td><td>22126</td><td>-1</td><td>14</td></tr><tr><td>8</td><td>36881</td><td>37035</td><td>42555</td><td>0</td><td>15</td></tr><tr><td>16</td><td>70337</td><td>67065</td><td>81158</td><td>5</td><td>21</td></tr><tr><td>32</td><td>109207</td><td>113210</td><td>126008</td><td>-4</td><td>11</td></tr><tr><td>64</td><td>139294</td><td>164148</td><td>160724</td><td>-15</td><td>-2</td></tr><tr><td>128</td><td>151382</td><td>175872</td><td>174672</td><td>-14</td><td>-1</td></tr><tr><td>256</td><td>149136</td><td>164382</td><td>172080</td><td>-9</td><td>5</td></tr></tbody></table><h4 id="Analysis-1"><a href="#Analysis-1" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>ARM is almost on par with x86 for lower scalability but again fails to scale for higher scalability.</li><li>With frequency scaling applied ARM continues to beat x86 (in most cases).</li></ul><br><hr><h3 id="3-Read-Write"><a href="#3-Read-Write" class="headerlink" title="3. Read Write:"></a><span style="color: #de5246"><ins>3. Read Write:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-rw.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>137</td><td>149</td><td>158</td><td>-8</td><td>6</td></tr><tr><td>2</td><td>251</td><td>273</td><td>290</td><td>-8</td><td>6</td></tr><tr><td>4</td><td>462</td><td>502</td><td>533</td><td>-8</td><td>6</td></tr><tr><td>8</td><td>852</td><td>920</td><td>983</td><td>-7</td><td>7</td></tr><tr><td>16</td><td>1539</td><td>1678</td><td>1776</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>2556</td><td>2906</td><td>2949</td><td>-12</td><td>1</td></tr><tr><td>64</td><td>3770</td><td>5158</td><td>4350</td><td>-27</td><td>-16</td></tr><tr><td>128</td><td>5015</td><td>8131</td><td>5787</td><td>-38</td><td>-29</td></tr><tr><td>256</td><td>5676</td><td>8562</td><td>6549</td><td>-34</td><td>-24</td></tr></tbody></table><h4 id="Analysis-2"><a href="#Analysis-2" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Pattern is different with read-write workload. ARM starts lagging. Frequency scaling helps ease this lag for lower scalability but increasing scalability continues to increase the gap.</li></ul><br><hr><h3 id="4-Update-Index"><a href="#4-Update-Index" class="headerlink" title="4. Update Index:"></a><span style="color: #de5246"><ins>4. Update Index:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ui.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>623</td><td>768</td><td>719</td><td>-19</td><td>-6</td></tr><tr><td>4</td><td>1060</td><td>1148</td><td>1223</td><td>-8</td><td>7</td></tr><tr><td>8</td><td>1905</td><td>2028</td><td>2198</td><td>-6</td><td>8</td></tr><tr><td>16</td><td>3284</td><td>3590</td><td>3789</td><td>-9</td><td>6</td></tr><tr><td>32</td><td>5543</td><td>6275</td><td>6396</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9138</td><td>10381</td><td>10544</td><td>-12</td><td>2</td></tr><tr><td>128</td><td>13879</td><td>16868</td><td>16014</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>19954</td><td>25459</td><td>23024</td><td>-22</td><td>-10</td></tr></tbody></table><h4 id="Analysis-3"><a href="#Analysis-3" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases).</li></ul><br><hr><h3 id="5-Update-Non-Index"><a href="#5-Update-Non-Index" class="headerlink" title="5. Update Non-Index:"></a><span style="color: #de5246"><ins>5. Update Non-Index:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-uni.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>588</td><td>686</td><td>678</td><td>-14</td><td>-1</td></tr><tr><td>4</td><td>1075</td><td>1118</td><td>1240</td><td>-4</td><td>11</td></tr><tr><td>8</td><td>1941</td><td>2043</td><td>2240</td><td>-5</td><td>10</td></tr><tr><td>16</td><td>3367</td><td>3662</td><td>3885</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>5681</td><td>6438</td><td>6555</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9328</td><td>10631</td><td>10763</td><td>-12</td><td>1</td></tr><tr><td>128</td><td>14158</td><td>17245</td><td>16336</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>20377</td><td>26367</td><td>23512</td><td>-23</td><td>-11</td></tr></tbody></table><h4 id="Analysis-4"><a href="#Analysis-4" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases).</li></ul><br><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>There are some important observations we can make:</p><ul><li>For read only workload MySQL on ARM continues to perform on-par with MySQL on x86. </li><li>For write involving workload MySQL on ARM starts lagging a bit but if we consider frequency scaling things start getting  better.</li><li>Frequency scaling is not a real life parameter so we should consider the price-per-performance ratio. This could be a topic in itself but just a quick fact: ARM instance is 66% cheaper than x86 (24U48G same one we used).</li><li>There is a pattern that we can observe. ARM workloads are very well scalable till it hits the CPU limits. With increasing scalability, contention increases and ARM starts lagging. This is expected since mutexes/contention hot-spots were all tuned for x86 (for example: spin-lock). But now that MySQL officially supports ARM and the growing ARM community and interest, it would be tuned for ARM too.</li></ul><p>To summarize, MySQL on ARM is a worth exploring option from a cost and performance perspective.</p><br><em>If you have more questions/queries do let me know. Will try to answer them.</em>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;By and large this would be a topic of interest for most of us including me when I started to explore this space. Before we dwell into the numbers let’s first understand some basic differences between 2 architectures. Beyond being CISC and RISC let’s look at the important differences from MySQL perspective.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM优化和Java Math库有关的那些坑</title>
    <link href="https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/"/>
    <id>https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/</id>
    <published>2020-04-08T03:44:29.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://github.com/Yikun">姜逸坤</a></p><h2 id="1-起初"><a href="#1-起初" class="headerlink" title="1. 起初"></a>1. 起初</h2><p>最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Math.log(3): "</span> + Math.log(<span class="number">3</span>));</span><br><span class="line">        System.out.println(<span class="string">"StrictMath.log(3): "</span> + StrictMath.log(<span class="number">3</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>我们发现，在x86下，Math的结果为<code>1.0986122886681098</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> on x86</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Math.log(3): 1.0986122886681098</span><br><span class="line">StrictMath.log(3): 1.0986122886681096</span><br></pre></td></tr></table></figure><p>而aarch64的结果为<code>1.0986122886681096</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> on aarch64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Math.log(3): 1.0986122886681096</span><br><span class="line">StrictMath.log(3): 1.0986122886681096</span><br></pre></td></tr></table></figure><p>而在Java 8的<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html">官方文档</a>中，对此有明确说明：</p><blockquote><p>Unlike some of the numeric methods of class StrictMath, all implementations of the equivalent functions of class Math are not defined to return the bit-for-bit same results. This relaxation permits better-performing implementations where strict reproducibility is not required.</p></blockquote><p>因此，结论是：<strong>Math的结果有可能是不精确的，如果结果对精度有苛求，那么请使用StrictMath</strong>。</p><p>在此，我们留下2个疑问：</p><ol><li>为什么说Math的实现不是<code>the bit-for-bit same results</code>？</li><li>Math是怎么实现在各个架构下<code>better-performing implementations</code>的？</li></ol><h2 id="2-深度探索一下Math的实现"><a href="#2-深度探索一下Math的实现" class="headerlink" title="2. 深度探索一下Math的实现"></a>2. 深度探索一下Math的实现</h2><p>为了能够更清晰的看到StrictMath的实现，我们深入的看了下JDK的实现。</p><h3 id="2-1-Math和StrictMath的基本实现"><a href="#2-1-Math和StrictMath的基本实现" class="headerlink" title="2.1 Math和StrictMath的基本实现"></a>2.1 Math和StrictMath的基本实现</h3><p>我们从Math.log和StrictMath.log的实现为例，进行深入学习：</p><ol><li><a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/classes/java/lang/Math.java#l293">Math.log的代码</a>表面上很简单，就是直接调用StrictMath.log。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">log</span><span class="params">(<span class="keyword">double</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StrictMath.log(a); <span class="comment">// default impl. delegates to StrictMath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/classes/java/lang/StrictMath.java#l231">StrictMath的代码</a>，会调用<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/StrictMath.c#l76">StrictMath.c</a>中的方法，最终会调用fdlibm的<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/fdlibm/src/e_log.c">e_log.c</a>的实现。</li></ol><p>总体的实现和下图类似：<br><img src="https://user-images.githubusercontent.com/1736354/78893132-569ff880-7a9d-11ea-85dc-4652c9bf85f8.png" alt="image"></p><p>对于StrictMath来说，没有什么黑科技，最终的实现就是e_log.c的ieee754标准实现，是通过C语言实现的，所以在各个平台的表现是一样的，整个流程如图中蓝色部分。感兴趣的同学可以看<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/fdlibm/src/e_log.c">e_log.c</a>的源码实现即可。</p><h3 id="2-2-Math的黑科技"><a href="#2-2-Math的黑科技" class="headerlink" title="2.2 Math的黑科技"></a>2.2 Math的黑科技</h3><p>回到我们最初的起点，再加上一个问题：</p><ol><li>为什么说Math的实现不是<code>the bit-for-bit same results</code>？</li><li>Math是怎么实现在各个架构下<code>better-performing implementations</code>的？</li><li>既然Math的实现，也是直接调用StrictMath，为什么结果确不一样呢？</li></ol><p>原来，JVM为了让各个arch的CPU能够充分的发挥自己CPU的优势，会根据架构不同，会通过Hotspot intrinsics替换掉Math函数的实现，我们可以从代码<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/dae2d83e0ec2/src/share/vm/classfile/vmSymbols.hpp#l598">vmSymbols.hpp</a>看到，Math的很多实现都被替换掉了。log的替换类似于：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">do_intrinsic(_dlog, java_lang_Math, log_name, double_double_signature, F_S)</span><br></pre></td></tr></table></figure><p>最终，Math的调用为下图红色部分：</p><p><img src="https://user-images.githubusercontent.com/1736354/78893234-8f3fd200-7a9d-11ea-903a-311c8c3cc836.png" alt="image"></p><p>log的实现:</p><ul><li>在x86下，最终其实调用的是<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/dae2d83e0ec2/src/cpu/x86/vm/assembler_x86.cpp#l4140">assembler_x86.cpp</a>中的<code>flog</code>实现:<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Assembler::flog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  fldln2();</span><br><span class="line">  fxch();</span><br><span class="line">  fyl2x();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>而在aarch64下，我们可以从<a href="http://hg.openjdk.java.net/jdk/jdk/file/e53ec3b362f4/src/hotspot/cpu/">src/hotspot/cpu/</a>目录下看到，aarch64并未实现优化版本。因此，实际aarch64调用的就是标准的StrictMath。</li></ul><p>正因如此，x86汇编的计算结果的差异导致了x86和aarch64结果在Math.log差异。</p><p>当然，aarch64也在JDK 11中，对部分的Math接口做了加速实现，有兴趣可以看看<a href="https://bugs.openjdk.java.net/browse/JDK-8189104">JEP 315: Improve Aarch64 Intrinsics</a>的实现。</p><h2 id="3-toRadians的小插曲"><a href="#3-toRadians的小插曲" class="headerlink" title="3. toRadians的小插曲"></a>3. toRadians的小插曲</h2><p>在ARM优化过程中，有的是因为Math库和StrictMath不同的实现造成结果不同，所以我们如果对精度要求非常高，直接切到StrictMath即可。</p><p>但有的函数，由于在Java大版本升级的过程中，出现了一些实现的差异，先看一个简单的Java程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"Math.toRadians(0.33): "</span> + Math.toRadians(<span class="number">0.33</span>));</span><br><span class="line">System.out.println(<span class="string">"StrictMath.toRadians(0.33): "</span> + StrictMath.toRadians(<span class="number">0.33</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们分别看看在Java11和Java8的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-11-openjdk-amd64&#x2F;bin&#x2F;java Hello</span><br><span class="line">Math.toRadians(0.33): 0.005759586531581287</span><br><span class="line">StrictMath.toRadians(0.33): 0.005759586531581287</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-amd64&#x2F;bin&#x2F;java Hello</span><br><span class="line">Math.toRadians(0.33): 0.005759586531581288</span><br><span class="line">StrictMath.toRadians(0.33): 0.005759586531581288</span><br></pre></td></tr></table></figure><p>最后一位很奇怪的差了1，我们继续深入进去看到toRadians的实现：</p><ul><li><a href="https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/lang/Math.java#l236">Java8的实现</a>为：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java 8 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">toDegrees</span><span class="params">(<span class="keyword">double</span> angrad)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> angrad * <span class="number">180.0</span> / PI;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><a href="http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/Math.java#l253">Java11的实现</a>为：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEGREES_TO_RADIANS = <span class="number">0.017453292519943295</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">toRadians</span><span class="params">(<span class="keyword">double</span> angdeg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> angdeg * DEGREES_TO_RADIANS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>原来在Java11的实现中，为了优化性能，将<code>* 180.0 / PI</code>提前算好了，这样每次只用乘以乘数即可，从而化简了计算。这也最终导致了，Java8和Java11在精度上有一些差别。</li></ul><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>Math在各个arch下的实现不同，精度也不同，如果对精度要求很高，可以使用StrictMath。</li><li>Java不同版本的优化，也有可能导致Math库的精度不同</li><li>Math库在实现时，利用intrinsics机制，把各个arch下Math的实现换掉了，从而充分的发挥各个CPU自身的优势。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://github.com/Yikun&quot;&gt;姜逸坤&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-起初&quot;&gt;&lt;a href=&quot;#1-起初&quot; class=&quot;headerlink&quot; title=&quot;1. 起初&quot;&gt;&lt;/a&gt;1. 起初&lt;/h2&gt;&lt;p&gt;最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Hello&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;Math.log(3): &quot;&lt;/span&gt; + Math.log(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;StrictMath.log(3): &quot;&lt;/span&gt; + StrictMath.log(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="Java" scheme="https://kunpengcompute.github.io/tags/Java/"/>
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM CPU Vendor 及 Part ID 映射关系（持续更新）</title>
    <link href="https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/"/>
    <id>https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/</id>
    <published>2020-04-03T04:51:40.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>根据<a href="https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1">ARM CPU官方技术手册</a>，ARM CPU的CPU型号、Vendor、版本等信息存于<code>MIDR_EL1</code>寄存器中:<br><img width="492" alt="MIDR" src="https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG"><br>其中从低至高第0-3 bit表示<code>revision</code>，代表固件版本的小版本号，如r1p3中的p3；<br>第4-15 bit表示<code>part number(id)</code>，代表这款CPU在所在<code>vendor</code>产品中定义的产品代码，如在<code>HiSilicon</code>产品中，<code>part_id=0xd01</code>代表<code>Kunpeng-920</code>芯片；<br>第16-19 bit表示<code>architecture</code>，即架构版本，<code>0x8</code>即ARMv8；<br>第20-23 bit表示<code>variant</code>，即固件版本的大版本号，如r1p3中的r1；<br>第24-31 bit表示<code>implementer</code>，即<code>vendor id</code>，如<code>vendor_id=0x48</code>表示<code>HiSilicon</code>。</p><a id="more"></a><p>想要知道一款ARM CPU的具体型号，则需要首先解析<code>vendor_id(implementer)</code> 然后再在该Vendor的所有型号中匹配<code>part_id</code>，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系，以及主流厂商的主要型号映射关系：</p><h1 id="Vendor映射关系："><a href="#Vendor映射关系：" class="headerlink" title="Vendor映射关系："></a>Vendor映射关系：</h1><table><thead><tr><th>Vendor Name</th><th>Vendor ID</th></tr></thead><tbody><tr><td>ARM</td><td>0x41</td></tr><tr><td>Broadcom</td><td>0x42</td></tr><tr><td>Cavium</td><td>0x43</td></tr><tr><td>DigitalEquipment</td><td>0x44</td></tr><tr><td>HiSilicon</td><td>0x48</td></tr><tr><td>Infineon</td><td>0x49</td></tr><tr><td>Freescale</td><td>0x4D</td></tr><tr><td>NVIDIA</td><td>0x4E</td></tr><tr><td>APM</td><td>0x50</td></tr><tr><td>Qualcomm</td><td>0x51</td></tr><tr><td>Marvell</td><td>0x56</td></tr><tr><td>Intel</td><td>0x69</td></tr></tbody></table><h1 id="型号映射关系"><a href="#型号映射关系" class="headerlink" title="型号映射关系"></a>型号映射关系</h1><h2 id="ARM"><a href="#ARM" class="headerlink" title="ARM"></a>ARM</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xd03</td><td>Cortex-a53</td></tr><tr><td>0xd07</td><td>Cortex-a57</td></tr><tr><td>0xd08</td><td>Cortex-a72</td></tr></tbody></table><h2 id="Broadcom"><a href="#Broadcom" class="headerlink" title="Broadcom"></a>Broadcom</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0x0f</td><td>Brahma B15</td></tr><tr><td>0x100</td><td>Brahma B53</td></tr></tbody></table><h2 id="Cavium"><a href="#Cavium" class="headerlink" title="Cavium"></a>Cavium</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0x0af</td><td>Thunder X2 29xx</td></tr></tbody></table><h2 id="Qualcomm"><a href="#Qualcomm" class="headerlink" title="Qualcomm"></a>Qualcomm</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xc00</td><td>Falkor</td></tr></tbody></table><h2 id="HiSilicon"><a href="#HiSilicon" class="headerlink" title="HiSilicon"></a>HiSilicon</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xd01</td><td>Kunpeng-920</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&quot;https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1&quot;&gt;ARM CPU官方技术手册&lt;/a&gt;，ARM CPU的CPU型号、Vendor、版本等信息存于&lt;code&gt;MIDR_EL1&lt;/code&gt;寄存器中:&lt;br&gt;&lt;img width=&quot;492&quot; alt=&quot;MIDR&quot; src=&quot;https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG&quot;&gt;&lt;br&gt;其中从低至高第0-3 bit表示&lt;code&gt;revision&lt;/code&gt;，代表固件版本的小版本号，如r1p3中的p3；&lt;br&gt;第4-15 bit表示&lt;code&gt;part number(id)&lt;/code&gt;，代表这款CPU在所在&lt;code&gt;vendor&lt;/code&gt;产品中定义的产品代码，如在&lt;code&gt;HiSilicon&lt;/code&gt;产品中，&lt;code&gt;part_id=0xd01&lt;/code&gt;代表&lt;code&gt;Kunpeng-920&lt;/code&gt;芯片；&lt;br&gt;第16-19 bit表示&lt;code&gt;architecture&lt;/code&gt;，即架构版本，&lt;code&gt;0x8&lt;/code&gt;即ARMv8；&lt;br&gt;第20-23 bit表示&lt;code&gt;variant&lt;/code&gt;，即固件版本的大版本号，如r1p3中的r1；&lt;br&gt;第24-31 bit表示&lt;code&gt;implementer&lt;/code&gt;，即&lt;code&gt;vendor id&lt;/code&gt;，如&lt;code&gt;vendor_id=0x48&lt;/code&gt;表示&lt;code&gt;HiSilicon&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Linux下获取ARMv8-A CPU详情的3种方法</title>
    <link href="https://kunpengcompute.github.io/2020/04/03/linux-xia-huo-qu-armv8-a-cpu-xiang-qing-de-3-chong-fang-fa/"/>
    <id>https://kunpengcompute.github.io/2020/04/03/linux-xia-huo-qu-armv8-a-cpu-xiang-qing-de-3-chong-fang-fa/</id>
    <published>2020-04-03T01:49:34.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>在ARM平台上进行软件适配时，经常遇到需要根据不同CPU的具体型号、额外属性等信息进行分支处理的需求，因而需要获取CPU的详情信息；ARM架构CPU与X86架构芯片在CPU详情信息的呈现上有很大不同。本文将简述ARM CPU与CPU详情相关的知识及在Linux下获取ARMv8-A CPU详情的三种方法。</p><a id="more"></a><h1 id="ARM-CPU中有关CPU详情的寄存器"><a href="#ARM-CPU中有关CPU详情的寄存器" class="headerlink" title="ARM CPU中有关CPU详情的寄存器"></a>ARM CPU中有关CPU详情的寄存器</h1><p>根据<a href="https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1">ARM CPU官方技术手册</a>，ARM CPU的CPU型号、Vendor、版本等信息存于<code>MIDR_EL1</code>寄存器中:<br><img width="492" alt="MIDR" src="https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG"><br>其中从低至高第0-3 bit表示<code>revision</code>，代表固件版本的小版本号，如r1p3中的p3；<br>第4-15 bit表示<code>part number(id)</code>，代表这款CPU在所在<code>vendor</code>产品中定义的产品代码，如在<code>HiSilicon</code>产品中，<code>part_id=0xd01</code>代表<code>Kunpeng-920</code>芯片；<br>第16-19 bit表示<code>architecture</code>，即架构版本，<code>0x8</code>即ARMv8；<br>第20-23 bit表示<code>variant</code>，即固件版本的大版本号，如r1p3中的r1；<br>第24-31 bit表示<code>implementer</code>，即<code>vendor id</code>，如<code>vendor_id=0x48</code>表示<code>HiSilicon</code>。</p><p>想要知道一款ARM CPU的具体型号，则需要首先解析<code>vendor_id(implementer)</code> 然后再在该Vendor的所有型号中匹配<code>part_id</code>，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系</p><table><thead><tr><th align="center">Vendor Name</th><th align="center">Vendor ID</th></tr></thead><tbody><tr><td align="center">ARM</td><td align="center">0x41</td></tr><tr><td align="center">Broadcom</td><td align="center">0x42</td></tr><tr><td align="center">Cavium</td><td align="center">0x43</td></tr><tr><td align="center">DigitalEquipment</td><td align="center">0x44</td></tr><tr><td align="center">HiSilicon</td><td align="center">0x48</td></tr><tr><td align="center">Infineon</td><td align="center">0x49</td></tr><tr><td align="center">Freescale</td><td align="center">0x4D</td></tr><tr><td align="center">NVIDIA</td><td align="center">0x4E</td></tr><tr><td align="center">APM</td><td align="center">0x50</td></tr><tr><td align="center">Qualcomm</td><td align="center">0x51</td></tr><tr><td align="center">Marvell</td><td align="center">0x56</td></tr><tr><td align="center">Intel</td><td align="center">0x69</td></tr></tbody></table><p>而对于具体型号来说，对应关系则更为复杂，这里就不一一列举，可以参考<a href="https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/">本站文章</a>或<a href="https://github.com/karelzak/util-linux/blob/master/sys-utils/lscpu-arm.c">util-linux/lscpu</a>工具中的相关具体实现来获取完整的映射关系，<code>lscpu</code>工具我们则将在后面的部分中进行介绍。</p><p>上面介绍过，除了CPU型号之外，我们通常还会关注CPU是否支持我们需要的特性(扩展指令集，CPU Flags, CPU features)；与X86相差较大(CPU features定义集中在EBX,ECX和EDX寄存器中)，ARM架构的这些特性分散于<a href="https://www.kernel.org/doc/Documentation/arm64/cpu-feature-registers.txt"><code>ID_PFR0_EL1</code>, <code>ID_PFR1_EL1</code>, <code>ID_DFR0_EL1</code>, <code>ID_ISAR0_EL1</code></a> 等等若干个专用寄存器中，解析起来难度较高，后面我们会详细讨论如何获取这些内容。</p><h1 id="在Linux下如何获取CPU详情信息"><a href="#在Linux下如何获取CPU详情信息" class="headerlink" title="在Linux下如何获取CPU详情信息"></a>在Linux下如何获取CPU详情信息</h1><p>在介绍具体的方法前，首先需要介绍一下ARMv8架构下的安全分层机制(Exception Level):<br><img src="https://user-images.githubusercontent.com/10849016/78317406-c9d4d680-7594-11ea-9a35-14158c262d1d.png" alt="image"><br>如上图所示，ARMv8架构是专为数据中心场景而设计的架构，相比较早的ARM架构，新增了<code>EL2</code>层用于实现硬件虚拟化；较高层的用户是无权直接限访问下一层的数据内容的，对于我们的场景来说，由上面介绍的内容中可以看到，前面所有介绍的寄存器都存在于<code>EL1</code>层，而我们通常使用的应用程序都处于<code>EL0</code>层，因此是无法直接访问到这些寄存器的。那么该如何读取这些内容呢？</p><h2 id="1-从文件节点获取"><a href="#1-从文件节点获取" class="headerlink" title="1. 从文件节点获取"></a>1. 从文件节点获取</h2><p>OS在启动时，会将底层硬件信息载入到相应的文件节点中，这样，位于<code>EL0</code>层的用户就可以通过读取这些文件节点来获取这些信息，比较常用的有两个：</p><ol><li><strong>/sys/devices/system/cpu:</strong><br>该文件夹下保存了较全的CPU信息文件，并按单个CPU进行区分，读取其中某一个的<code>regs</code>文件目录就可以获得相应的CPU详情信息，如我们尝试获取CPU0的相关信息：<br><img src="https://user-images.githubusercontent.com/10849016/78318946-9c8a2780-7598-11ea-8a31-3a76d422a055.png" alt="image"><br>可以看到，我们读取的仍然是<code>MIDR_EL1</code>寄存器相对应的信息，并且是未解析的数据，需要对应上文介绍的方法进行解析。并且目前没有在这个文件夹下找到<code>CPU Flags</code>的相关的信息，如果后续找到其所在位置，会刷新。</li><li><strong>/proc/cpuinfo:</strong><br><img src="https://user-images.githubusercontent.com/10849016/78318499-731ccc00-7597-11ea-8229-de07a2e0db92.png" alt="image"><br>通过读取cpuinfo可以看到，通过这种方法获取的CPU详情，是进行过解析的，对原始数据进行了拆分，并且是包含了<code>CPU Flag</code>信息的，但仍与X86下的结果有较大不同，各个key所对应的信息仍然需要根据表单进行解析才能转变为人为可读的信息。</li></ol><h2 id="2-使用LSCPU命令读取"><a href="#2-使用LSCPU命令读取" class="headerlink" title="2.使用LSCPU命令读取"></a>2.使用LSCPU命令读取</h2><p>Linux内核的开发者显然也发现了上文介绍的两种方法获取信息不够全面且需要二次解析的问题，因此在Linux外围工具组<code>util-linux/lscpu</code>(<a href="https://en.wikipedia.org/wiki/Util-linux">wiki</a>)中进行了改进，从<a href="https://lwn.net/Articles/749882/">2.32</a>版本开始增加了对ARM平台CPU信息的解析，从而提供人为可读的内容(由于2019年11月才合入相关Patch，HiSilicon芯片的解析需要手动编译最新主干代码才能实现)。<br><img src="https://user-images.githubusercontent.com/10849016/78319822-95fcaf80-759a-11ea-92b3-41367a2ed5da.png" alt="image"><br>从上图可以看到，<code>lscpu</code>提供了非常丰富且直观的内容。</p><h2 id="3-使用内联汇编和辅助向量直接解析"><a href="#3-使用内联汇编和辅助向量直接解析" class="headerlink" title="3.使用内联汇编和辅助向量直接解析"></a>3.使用内联汇编和辅助向量直接解析</h2><p>上文介绍的两种方法相对来说比较简单，但需要进行读取文件、运行外部命令等操作；当想在自己的程序中引用上述信息时，速度会相对较慢且会引入新的依赖(lscpu)。因此最快速的方法是通过内联汇编直接读取并解析相应的寄存器；上文中已经提到，用户在<code>EL0</code>无法直接读取到位于<code>EL1</code>的寄存器内的内容，那么该如何去做呢？</p><p>ARM已经为我们准备好了一切，用户可以通过<code>MRS</code>指令将程序状态寄存器的内容传送到通用寄存器中，再进行进一步的解析，因此我们可以这样做：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* read the cpuid data from MIDR_EL1 register */</span></span><br><span class="line"><span class="keyword">asm</span>(<span class="string">"mrs %0, MIDR_EL1"</span> : <span class="string">"=r"</span> (cpuid));</span><br><span class="line">VIR_DEBUG(<span class="string">"CPUID read from register:  0x%016lx"</span>, cpuid);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* parse the coresponding part_id bits */</span></span><br><span class="line">data-&gt;pvr = cpuid&gt;&gt;<span class="number">4</span>&amp;<span class="number">0xFFF</span>;</span><br><span class="line"><span class="comment">/* parse the coresponding vendor_id bits */</span></span><br><span class="line">data-&gt;vendor_id = cpuid&gt;&gt;<span class="number">24</span>&amp;<span class="number">0xFF</span>;</span><br></pre></td></tr></table></figure><p>这样就可以快速的获取CPUID相关的具体内容，在根据表格进行映射即可获得Vendor和Model信息；</p><p>对于CPU Flags，由于牵扯到的寄存器众多，读者可以根据<a href="https://www.kernel.org/doc/Documentation/arm64/cpu-feature-registers.txt">ARM64 CPU Feature Registers</a>中的示例程序进行依次进行寄存器读取，再根据相应的映射关系进行解析，也可以使用下面将要介绍的可读性更高的另一种方法。</p><p>Linux内核提供了<a href="http://man7.org/linux/man-pages/man3/getauxval.3.html"><strong>getauxval()</strong></a>方法，用于读取辅助向量(auxiliary vector, 一个从内核到用户空间的信息交流机制)，通过读取相应的辅助向量，我们就能获取相应的硬件信息；辅助向量有很多，感兴趣的读者可以查看上面的链接，对于我们读取CPU Flags来说，关心的是<strong>AT_HWCAP</strong>这个辅助向量，通过<code>getauxval()</code>读取这个向量的值，可以获得整合过的CPU Flags信息，其bit定位规则则在<a href="https://github.com/torvalds/linux/blob/master/arch/arm64/include/uapi/asm/hwcap.h">hwcap.h</a>，当然，每种架构下的对应关系不相同，需要根据需要进行查找。</p><p>那么，我们就可以采用下面的方法进行解析：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/auxv.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 通过移位Bit Mask来读取相应的标志位 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BIT_SHIFTS(n)(UL(1) &lt;&lt; (n))</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> hwcaps = getauxval(AT_HWCAP);</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="comment">/* 目前ARMv8架构只有32种CPU Flags */</span></span><br><span class="line">    <span class="keyword">char</span> *<span class="built_in">list</span>[<span class="number">32</span>] = &#123;<span class="string">"fp\n"</span>, <span class="string">"asimd\n"</span>, <span class="string">"evtstrm\n"</span>, <span class="string">"aes\n"</span>, <span class="string">"pmull\n"</span>, <span class="string">"sha1"</span>,</span><br><span class="line">                              <span class="string">"sha2\n"</span>, <span class="string">"crc32\n"</span>, <span class="string">"atomics\n"</span>, <span class="string">"fphp\n"</span>, <span class="string">"asimdhp\n"</span>,</span><br><span class="line">                              <span class="string">"cpuid\n"</span>, <span class="string">"asimdrdm\n"</span>,<span class="string">"jscvt\n"</span>, <span class="string">"fcma\n"</span>, <span class="string">"lrcpc\n"</span>,</span><br><span class="line">                              <span class="string">"dcpop\n"</span>, <span class="string">"sha3\n"</span>, <span class="string">"sm3\n"</span>, <span class="string">"sm4\n"</span>, <span class="string">"asimddp\n"</span> ,</span><br><span class="line">                              <span class="string">"sha512\n"</span>, <span class="string">"sve\n"</span>, <span class="string">"asimdfhm\n"</span>, <span class="string">"dit\n"</span>, <span class="string">"uscat\n"</span>,</span><br><span class="line">                              <span class="string">"ilrcpc\n"</span>, <span class="string">"flagm\n"</span>, <span class="string">"ssbs\n"</span>, <span class="string">"sb\n"</span>, <span class="string">"paca\n"</span>,<span class="string">"pacg\n"</span>,&#125;;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i&lt; <span class="number">32</span>; i++)&#123;</span><br><span class="line"><span class="keyword">if</span> (hwcaps &amp; BIT_SHIFTS(i)) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s\n"</span>,<span class="built_in">list</span>[i]);</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者，可以直接通过<code>hwcap.h</code>中预先定义好的宏来做bit mask:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/auxv.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;asm/hwcap.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> hwcaps= getauxval(AT_HWCAP);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_AES)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"AES instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_CRC32)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"CRC32 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_PMULL)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"PMULL/PMULL2 instructions that operate on 64-bit data are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_SHA1)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"SHA1 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_SHA2)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"SHA2 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;在ARM平台上进行软件适配时，经常遇到需要根据不同CPU的具体型号、额外属性等信息进行分支处理的需求，因而需要获取CPU的详情信息；ARM架构CPU与X86架构芯片在CPU详情信息的呈现上有很大不同。本文将简述ARM CPU与CPU详情相关的知识及在Linux下获取ARMv8-A CPU详情的三种方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Running MySQL on ARM. Does it work?</title>
    <link href="https://kunpengcompute.github.io/2020/03/31/running-mysql-on-arm-does-it-work/"/>
    <id>https://kunpengcompute.github.io/2020/03/31/running-mysql-on-arm-does-it-work/</id>
    <published>2020-03-31T07:11:02.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar 原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-ARM/">https://mysqlonarm.github.io/Running-MySQL-on-ARM/</a></p><p>I am sure most of you may have this question. In fact, I too had it before I started working on #mysqlonarm initiative. What does it take to run MySQL on ARM? Does it really work? What about dependencies? What kind of performance does it have? What about support? Is there enough community support? This could go on…..</p><p>Let’s try to answer these questions in simple question answer format.</p><a id="more"></a><h4 id="Q-Is-MySQL-supported-on-ARM"><a href="#Q-Is-MySQL-supported-on-ARM" class="headerlink" title="Q: Is MySQL supported on ARM?"></a>Q: Is MySQL supported on ARM?</h4><p>A: Yes, MySQL is officially supported on ARM. There are packages available that you can download from mysql.com site.</p><h4 id="Q-Which-OS-are-supported"><a href="#Q-Which-OS-are-supported" class="headerlink" title="Q: Which OS are supported?"></a>Q: Which OS are supported?</h4><p>A: Currently support is enabled for RHEL-7 &amp; 8/Oracle-Linux- 7 &amp; 8. I don’t see direct package support for other OS.</p><h4 id="Q-Can-we-build-it-from-source-code-for-other-OS-like-say-Ubuntu"><a href="#Q-Can-we-build-it-from-source-code-for-other-OS-like-say-Ubuntu" class="headerlink" title="Q: Can we build it from source code for other OS (like say Ubuntu)?"></a>Q: Can we build it from source code for other OS (like say Ubuntu)?</h4><p>A: Yes. It works. I have been using binaries built from source code (using mysql-8.0.19 tag current release tag) on Ubuntu-18.04 (Bionic Beaver). (Also, build it on CentOS if you want to go the source code way). This also means all needed dependencies are taken care off or are already available.</p><h4 id="Q-Are-supporting-tools-available-on-ARM"><a href="#Q-Are-supporting-tools-available-on-ARM" class="headerlink" title="Q: Are supporting tools available on ARM?"></a>Q: Are supporting tools available on ARM?</h4><p>A: Since packages are available and I was able to build it from source too the default utilities like mysql shell/mysqladmin/mysqlslap/mysqldump/etc… and tons of other things that default ships along with binaries are available. If you care about a specific tools do let me know I will check them out. For now I have tried percona-toolkit some selective tools and they too work.</p><h4 id="Q-Does-MariaDB-and-Percona-too-support-their-respective-server-flavor-on-ARM"><a href="#Q-Does-MariaDB-and-Percona-too-support-their-respective-server-flavor-on-ARM" class="headerlink" title="Q: Does MariaDB and Percona too support their respective server flavor on ARM?"></a>Q: Does MariaDB and Percona too support their respective server flavor on ARM?</h4><p>A: MariaDB Community Server packages (from MariaDB corporation) are available for ARM (CentOS7/Ubuntu-16.04/18.04). Tools for MariaDB server are not yet officially available on ARM.<br>Percona doesn’t yet officially support ARM but I was able to build it from source (MyRocks/TokuDB are not available).</p><h4 id="Q-Non-availability-of-tools-Can-that-block-my-progress-of-trying-MySQL-or-its-variants-on-ARM"><a href="#Q-Non-availability-of-tools-Can-that-block-my-progress-of-trying-MySQL-or-its-variants-on-ARM" class="headerlink" title="Q: Non-availability of tools. Can that block my progress of trying MySQL (or its variants) on ARM?"></a>Q: Non-availability of tools. Can that block my progress of trying MySQL (or its variants) on ARM?</h4><p>A: No. Since most of these tools talk mysql protocol you can of-course install them on x86 with server running on ARM. (if tool is not yet ported to ARM)</p><h4 id="Q-Is-there-enough-community-support"><a href="#Q-Is-there-enough-community-support" class="headerlink" title="Q: Is there enough community support?"></a>Q: Is there enough community support?</h4><p>A: MySQL on ARM is there for quite some time. There are active contributions from multiple vendors including ARM, Qualcomm, Huawei etc… and the community is growing rapidly. There is a lot of interest from all sections on optimizing MySQL on ARM. Lot of developers wanted to connect with this initiative. There are few challenges, most importantly non-availability of the hardware. If you are interested in contributing please talk to me (shoot me an email).</p><h4 id="Q-All-that-looks-good-What-about-performance"><a href="#Q-All-that-looks-good-What-about-performance" class="headerlink" title="Q: All that looks good. What about performance?"></a>Q: All that looks good. What about performance?</h4><p>A: This is a wide topic so I will be publishing multiple posts on this topic in the coming days but to put it in short performance is comparable. On other hand ARM instances should provide better price performance.</p><h4 id="Q-What-about-Support"><a href="#Q-What-about-Support" class="headerlink" title="Q: What about Support?"></a>Q: What about Support?</h4><p>Since packages are available officially from MySQL I presume their service offering should also cover ARM. Same with MariaDB. And of-course beyond official support there are common groups and independent developers.</p><h4 id="Command-to-build-MySQL-on-ARM"><a href="#Command-to-build-MySQL-on-ARM" class="headerlink" title="Command to build MySQL on ARM"></a>Command to build MySQL on ARM</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake .. -DWITH_NUMA&#x3D;1 -DDOWNLOAD_BOOST&#x3D;1 -DWITH_BOOST&#x3D;&lt;boost-dir&gt; -DCMAKE_INSTALL_PREFIX&#x3D;&lt;dir-to-install&gt;</span><br><span class="line">make -j &lt;num-of-cores&gt;</span><br></pre></td></tr></table></figure><p>So no special flag is needed to build MySQL on ARM. (Assumes you have installed standard dependencies). It defaults compiles with “CMAKE_BUILD_TYPE=RelWithDebInfo”</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>MySQL on ARM is reality and it is now officially supported with ever growing eco-system/community. So give it a try. It could be your next cost-saving options without comprising performance or functionality.</p><p><em>If you have more questions/queries do let me know. Will try to answer them</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar 原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I am sure most of you may have this question. In fact, I too had it before I started working on #mysqlonarm initiative. What does it take to run MySQL on ARM? Does it really work? What about dependencies? What kind of performance does it have? What about support? Is there enough community support? This could go on…..&lt;/p&gt;
&lt;p&gt;Let’s try to answer these questions in simple question answer format.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让大数据生态在ARM架构下更顺滑</title>
    <link href="https://kunpengcompute.github.io/2020/03/30/rang-da-shu-ju-sheng-tai-zai-arm-jia-gou-xia-geng-shun-hua/"/>
    <id>https://kunpengcompute.github.io/2020/03/30/rang-da-shu-ju-sheng-tai-zai-arm-jia-gou-xia-geng-shun-hua/</id>
    <published>2020-03-30T12:23:45.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>受疫情影响Linaro Connect 2020改为线上直播的Linaro Tech Days，笔者所在团队在该活动上介绍了自19年Q4以来笔者团队在各主流开源社区推广ARM生态所做的工作以及所取得的成果。直播活动约有120+与会者。</p><p>视频回放：<a href="https://static.linaro.org/connect/ltd20/videos/ltd20-104.mp4">视频连接</a><br>PPT：<a href="https://kunpengcompute.github.io/presentations/Linaro-tech-days-2020-03.pdf">LTD20-104 Make life easier for Big Data users on ARM - Our efforts and future plans</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;受疫情影响Linaro Connect 2020改为线上直播的Linaro Tech Days，笔者所在团队在该活动上介绍了自19年Q4以来笔者团队在各主流开源社区推广ARM生态所做的工作以及所取得的成果。直播活动约有120+与会者。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="会议" scheme="https://kunpengcompute.github.io/tags/%E4%BC%9A%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>Why ARM?</title>
    <link href="https://kunpengcompute.github.io/2020/03/30/why-arm/"/>
    <id>https://kunpengcompute.github.io/2020/03/30/why-arm/</id>
    <published>2020-03-30T10:56:54.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar 原文链接: <a href="https://mysqlonarm.github.io/Why-ARM/">https://mysqlonarm.github.io/Why-ARM/</a></p><p>ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.</p><a id="more"></a><p>But this has changed in the past few years. More and more ARM processors are being used for high-end applications like database server, web server, application server, big data use-cases. They have already made their way to the data-centers as a server class machines. They are being looked upon as a cost effective option while running applications in cloud.</p><h2 id="ARM-ecosystem-evolution"><a href="#ARM-ecosystem-evolution" class="headerlink" title="ARM ecosystem evolution"></a><span style="color:#4885ed">ARM ecosystem evolution</span></h2><p>Few years back it was difficult to imagine that ARM would be used for running some high-end server class applications. There were 2 major reasons that I could think off:</p><ul><li>ARM were best suited for small handheld devices.</li><li>ARM ecosystem was limited around the specific product it supported.</li></ul><p>ARM ecosystem has really picked up well after some major OS providers added support for it including RedHat (CentOS), Ubuntu, Debian, Windows. This eased out porting of the major softwares to ARM. ARM community gave it a push to make sure most of the standard softwares are available on ARM viz. IDE, DB-server, Hadoop and all its variants from Apache Foundation, CI/CD software, Container, Virtualization, etc…</p><p>The ARM model that allows other vendors to license and develop their own ARM processors further helped fueled its popularity with more chip designers joining, collaborating and innovating.</p><p>Break-through came with major cloud providers like Amazon started providing ec2 instances (currently invitation only) based on ARM processors this means now everyone can boot an ARM instance and start developing/porting their software on ARM. This helped further grow the ecosystem.</p><h2 id="What-was-missing"><a href="#What-was-missing" class="headerlink" title="What was missing?"></a><span style="color:#4885ed">What was missing?</span></h2><p>Though most of these software have been ported to ARM they were not yet optimized for ARM. ARM has a weak memory model, can fit more cores in smaller space, difference in low-level instruction (for software that uses them), etc..</p><p>This was the start of the 2nd phase of ARM where the community/developer/user started moving from “running software on arm” -&gt; “optimizing software on arm”. I think this was a major win for the arm community when users started to think ARM seriously and started spending efforts on optimizing their software on ARM.</p><p>This (especially optimization) is a never ending process but I see first goal is to at-least be on par with x86. I purposely say “onpar” because each of architecture has its own USP so say if you port an enterprise class application to ARM and you can offer it to customer @ 50% of the cost (operating cost + initial investment) for 75% of the performance of x86 I think that would be still be attractive fit for most of the customers (especially given application are horizontally scalable). Of-course that doesn’t mean all applications run on ARM at reduced speed, in fact there are applications that run on ARM faster than x86 and since the optimization phase has just started in next few years a lot of applications would be running on ARM faster than other architectures.</p><h2 id="Go-Green"><a href="#Go-Green" class="headerlink" title="Go Green"></a><span style="color:#4885ed">Go Green</span></h2><p>It is everywhere and especially a matter of major concern for data-center operators (small or big). ARM being power efficient can save approximately 50% of the power compared to other architecture. This makes it help support Go-Green initiative.</p><h2 id="ARM-is-Next-Gen-processor"><a href="#ARM-is-Next-Gen-processor" class="headerlink" title="ARM is Next-Gen processor"></a><span style="color:#4885ed">ARM is Next-Gen processor</span></h2><p>It is interesting why I referred to it this way. Next generation kids are actively using kits like Andrino, Raspberry Pi, Odroid, Banana Pi, Asus tinker board, etc…. to build some of the next-gen system. These kids will be defining the next generation of computing. Given they started with ARM their social community has grown around ARM in the next few years there would be an army of ARM users/developers with a very active community.</p><p>All the groundwork and good things that are being built at this stage around ARM will be pushed to the next level once this workforce becomes active.</p><h2 id="ARM-in-Desktop-Laptop"><a href="#ARM-in-Desktop-Laptop" class="headerlink" title="ARM in Desktop/Laptop"></a><span style="color:#4885ed">ARM in Desktop/Laptop</span></h2><p>This is catching up fast and no wonder if we start seeing ARM based Desktop/PC workstation/Laptop (there are already few) commonly being used.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>The ARM Ecosystem looks a lot more fascinating and full of new challenges and opportunities. Current decade will be ruled by ARM based processors and it will be everywhere from tiny wearable devices to high-end movie experience, from auto-driving cycle/car to jumbo jet/space-craft. It is estimated that there would be 35 active ARM power devices per person. That’s Ocean of Opportunity.</p><p><em>If you have any comments feel free to drop an email (check about section)</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar 原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Why-ARM/&quot;&gt;https://mysqlonarm.github.io/Why-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>鲲鹏计算团队博客开张啦，欢迎投稿！</title>
    <link href="https://kunpengcompute.github.io/2020/03/27/kun-peng-ji-suan-tuan-dui-bo-ke-kai-zhang-la-huan-ying-tou-gao/"/>
    <id>https://kunpengcompute.github.io/2020/03/27/kun-peng-ji-suan-tuan-dui-bo-ke-kai-zhang-la-huan-ying-tou-gao/</id>
    <published>2020-03-27T03:37:41.000Z</published>
    <updated>2020-09-11T01:53:43.897Z</updated>
    
    <content type="html"><![CDATA[<div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>我们将在这里分享关于鲲鹏计算相关的技术、开源、生态的点滴。</p><p>欢迎关注！欢迎转发！欢迎投稿！</p></div><div id="English" class="tab-content"><p>We are share something about technology, opensource and ecosystem of Kunpeng.</p><p>Welcome to join us!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script><h2 id="如何投稿？"><a href="#如何投稿？" class="headerlink" title="如何投稿？"></a>如何投稿？</h2><p>非常简单，仅需要两步：</p><ol><li>点击<a href="https://github.com/kunpengcompute/kunpengcompute.github.io/issues">这里</a>，进入博客提交页面，我们使用Issue对博客进行管理，点击<code>New</code>进行投稿。</li><li>填写标题和内容，issue标题即为文章标题，issue内容即为文章内容，并发布请求。</li></ol><p>好了，至此你的投稿已经完成，你可以在<a href="https://github.com/kunpengcompute/kunpengcompute.github.io/issues">这里</a>看到你的投稿，并进行迭代修改。</p><h2 id="如何发布？"><a href="#如何发布？" class="headerlink" title="如何发布？"></a>如何发布？</h2><p>等到管理员审核通过后，会将你issue打上<code>publish</code>标签，之后，你内容就会自动同步在<a href="https://kunpengcompute.github.io/">博客</a>中啦！</p><p>来吧，还等什么？把你的干货分享起来！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;tabs is-toggle&quot;&gt;&lt;ul&gt;
&lt;li class=&quot;is-active&quot;&gt;&lt;a onclick=&quot;onTabClick(event)&quot;&gt;
&lt;span&gt;中文&lt;/span&gt;
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a onclick=&quot;onTabClick(
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
